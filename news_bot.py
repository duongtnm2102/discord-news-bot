import discord
from discord.ext import commands
import feedparser
import requests
import asyncio
import os
import re
from datetime import datetime, timedelta
import time
import calendar
from urllib.parse import urljoin, urlparse, quote
import html
import chardet
import pytz
import json
import aiohttp
from keep_alive import keep_alive
from enum import Enum
from typing import List, Dict, Tuple, Optional
import random
import hashlib

# üöÄ OPTIMIZED LIBRARIES - Enhanced for Yahoo Finance
try:
    import trafilatura
    TRAFILATURA_AVAILABLE = True
except ImportError:
    TRAFILATURA_AVAILABLE = False

try:
    import newspaper
    from newspaper import Article
    NEWSPAPER_AVAILABLE = True
except ImportError:
    NEWSPAPER_AVAILABLE = False

try:
    from bs4 import BeautifulSoup
    BEAUTIFULSOUP_AVAILABLE = True
except ImportError:
    BEAUTIFULSOUP_AVAILABLE = False

try:
    import wikipedia
    WIKIPEDIA_AVAILABLE = True
except ImportError:
    WIKIPEDIA_AVAILABLE = False

# üÜï GEMINI ONLY - Enhanced AI System with Direct Content Access
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

# Bot configuration
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix='!', intents=intents)

# üîí ENVIRONMENT VARIABLES
TOKEN = os.getenv('DISCORD_TOKEN')
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
GOOGLE_CSE_ID = os.getenv('GOOGLE_CSE_ID')

# üîß TIMEZONE - Vietnam
VN_TIMEZONE = pytz.timezone('Asia/Ho_Chi_Minh')
UTC_TIMEZONE = pytz.UTC

# üîß DISCORD LIMITS
DISCORD_EMBED_FIELD_VALUE_LIMIT = 1000
DISCORD_EMBED_DESCRIPTION_LIMIT = 4000
DISCORD_EMBED_TITLE_LIMIT = 250
DISCORD_EMBED_TOTAL_EMBED_LIMIT = 5800

# User cache with deduplication
user_news_cache = {}
user_last_detail_cache = {}
global_seen_articles = {}  # Global deduplication cache
scraped_news_cache = {}    # Cache for scraped news from Yahoo Finance
MAX_CACHE_ENTRIES = 25
MAX_GLOBAL_CACHE = 1000

# üîß Enhanced User Agents for Yahoo Finance
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

def get_current_vietnam_datetime():
    """Get current Vietnam date and time"""
    return datetime.now(VN_TIMEZONE)

def get_current_date_str():
    """Get current date string in Vietnam format"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%d/%m/%Y")

def get_current_time_str():
    """Get current time string in Vietnam format"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%H:%M")

def get_current_datetime_str():
    """Get current datetime string for display"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%H:%M %d/%m/%Y")

print("üöÄ NEWS BOT:")
print(f"DISCORD_TOKEN: {'‚úÖ' if TOKEN else '‚ùå'}")
print(f"GEMINI_API_KEY: {'‚úÖ' if GEMINI_API_KEY else '‚ùå'}")
print("=" * 30)

if not TOKEN:
    print("‚ùå CRITICAL: DISCORD_TOKEN not found!")
    exit(1)

# üîß MASSIVE RSS FEEDS - 20+ WORKING SOURCES from GitHub Gist 2025
RSS_FEEDS = {
    # === KINH T·∫æ TRONG N∆Ø·ªöC - CH·ªà CAFEF ===
    'domestic': {
        'cafef_chungkhoan': 'https://cafef.vn/thi-truong-chung-khoan.rss',
        'cafef_batdongsan': 'https://cafef.vn/bat-dong-san.rss',
        'cafef_taichinh': 'https://cafef.vn/tai-chinh-ngan-hang.rss',
        'cafef_vimo': 'https://cafef.vn/vi-mo-dau-tu.rss',
        'cafef_doanhnghiep': 'https://cafef.vn/doanh-nghiep.rss'
    },
    
    # === QU·ªêC T·∫æ - MASSIVE RSS COLLECTION from GitHub Gist 2025 ===
    'international': {
        # ‚úÖ YAHOO FINANCE RSS (Original working feeds)
        'yahoo_finance_main': 'https://finance.yahoo.com/news/rssindex',
        'yahoo_finance_headlines': 'https://feeds.finance.yahoo.com/rss/2.0/headline',
        'yahoo_finance_rss': 'https://www.yahoo.com/news/rss/finance',
        
        # ‚úÖ MAJOR FINANCIAL NEWS RSS FEEDS (Verified from GitHub)
        'cnn_money': 'http://rss.cnn.com/rss/money_topstories.rss',
        'reuters_topnews': 'http://feeds.reuters.com/reuters/topNews',
        'marketwatch': 'http://feeds.marketwatch.com/marketwatch/topstories/',
        'business_insider': 'http://feeds2.feedburner.com/businessinsider',
        'forbes': 'https://www.forbes.com/real-time/feed2/',
        'wsj': 'http://www.wsj.com/xml/rss/3_7031.xml',
        'cnbc': 'https://www.cnbc.com/id/100003114/device/rss/rss.html',
        'investing_com': 'https://www.investing.com/rss/news.rss',
        'seekingalpha': 'https://seekingalpha.com/market_currents.xml',
        'financial_times': 'https://www.ft.com/?format=rss',
        'fortune': 'http://fortune.com/feed/',
        'economist': 'http://www.economist.com/sections/economics/rss.xml',
        'nasdaq': 'http://articlefeeds.nasdaq.com/nasdaq/categories?category=Investing+Ideas',
        'washington_post_biz': 'http://feeds.washingtonpost.com/rss/business',
        'guardian_business': 'https://www.theguardian.com/business/economics/rss',
        'investopedia': 'https://www.investopedia.com/feedbuilder/feed/getfeed/?feedName=rss_headline',
        'nikkei_asia': 'https://asia.nikkei.com/rss/feed/nar',
        'economic_times': 'https://economictimes.indiatimes.com/markets/rssfeeds/1977021501.cms',
        'bbc_news': 'http://feeds.bbci.co.uk/news/rss.xml',
        'coindesk': 'https://feeds.feedburner.com/CoinDesk',
        
        # ‚úÖ BACKUP WORKING URLs (if primary fail)
        'yahoo_finance_crypto': 'https://finance.yahoo.com/topic/crypto/',
        'yahoo_finance_tech': 'https://finance.yahoo.com/topic/tech/',
        'yahoo_finance_stock_market': 'https://finance.yahoo.com/topic/stock-market-news/',
    }
}

def convert_utc_to_vietnam_time(utc_time_tuple):
    """Convert UTC to Vietnam time"""
    try:
        utc_timestamp = calendar.timegm(utc_time_tuple)
        utc_dt = datetime.fromtimestamp(utc_timestamp, tz=UTC_TIMEZONE)
        vn_dt = utc_dt.astimezone(VN_TIMEZONE)
        return vn_dt
    except Exception as e:
        return datetime.now(VN_TIMEZONE)

# üÜï ENHANCED DEDUPLICATION SYSTEM
def generate_article_hash(title, link, description=""):
    """Generate unique hash for article deduplication"""
    # Clean and normalize text
    clean_title = re.sub(r'[^\w\s]', '', title.lower().strip())
    clean_link = link.lower().strip()
    
    # Create content-based hash
    content = f"{clean_title}|{clean_link}"
    return hashlib.md5(content.encode('utf-8')).hexdigest()

def is_duplicate_article(news_item, source_name):
    """Check if article is duplicate using multiple methods"""
    global global_seen_articles
    
    # Method 1: Hash-based deduplication
    article_hash = generate_article_hash(news_item['title'], news_item['link'], news_item.get('description', ''))
    
    if article_hash in global_seen_articles:
        return True
    
    # Method 2: Title similarity check (for same-content different URLs)
    title_words = set(news_item['title'].lower().split())
    
    for existing_hash, existing_data in global_seen_articles.items():
        existing_title_words = set(existing_data['title'].lower().split())
        
        # Check if 80% of words are similar
        if len(title_words) > 3 and len(existing_title_words) > 3:
            similarity = len(title_words.intersection(existing_title_words)) / len(title_words.union(existing_title_words))
            if similarity > 0.8:
                return True
    
    # Method 3: URL domain check (same article, different parameters)
    for existing_hash, existing_data in global_seen_articles.items():
        if clean_url_for_comparison(news_item['link']) == clean_url_for_comparison(existing_data['link']):
            return True
    
    # Not duplicate - add to cache
    global_seen_articles[article_hash] = {
        'title': news_item['title'],
        'link': news_item['link'],
        'source': source_name,
        'timestamp': get_current_vietnam_datetime()
    }
    
    # Limit cache size
    if len(global_seen_articles) > MAX_GLOBAL_CACHE:
        # Remove oldest 100 entries
        sorted_items = sorted(global_seen_articles.items(), key=lambda x: x[1]['timestamp'])
        for old_hash, _ in sorted_items[:100]:
            del global_seen_articles[old_hash]
    
    return False

def clean_url_for_comparison(url):
    """Clean URL for comparison (remove parameters, fragments)"""
    try:
        from urllib.parse import urlparse, parse_qs
        parsed = urlparse(url)
        return f"{parsed.netloc}{parsed.path}".lower()
    except:
        return url.lower()

# üîß CONTENT VALIDATION FOR DISCORD
def validate_and_truncate_content(content: str, limit: int, suffix: str = "...") -> str:
    """Strict validation and truncation for Discord limits"""
    if not content:
        return "Kh√¥ng c√≥ n·ªôi dung."
    
    content = str(content).strip()
    safe_limit = max(limit - 50, 100)
    
    if len(content) <= safe_limit:
        return content
    
    available_space = safe_limit - len(suffix)
    if available_space <= 0:
        return suffix[:safe_limit]
    
    truncated = content[:available_space].rstrip()
    last_sentence = truncated.rfind('. ')
    if last_sentence > available_space * 0.7:
        truncated = truncated[:last_sentence + 1]
    
    return truncated + suffix

def validate_embed_field(name: str, value: str) -> Tuple[str, str]:
    """Strict embed field validation for Discord limits"""
    safe_name = validate_and_truncate_content(name, DISCORD_EMBED_TITLE_LIMIT, "...")
    safe_value = validate_and_truncate_content(value, DISCORD_EMBED_FIELD_VALUE_LIMIT, "...")
    
    if not safe_value or safe_value == "...":
        safe_value = "N·ªôi dung kh√¥ng kh·∫£ d·ª•ng."
    
    return safe_name, safe_value

def create_safe_embed(title: str, description: str = "", color: int = 0x00ff88) -> discord.Embed:
    """Create safe embed that fits Discord limits"""
    safe_title = validate_and_truncate_content(title, DISCORD_EMBED_TITLE_LIMIT, "...")
    safe_description = validate_and_truncate_content(description, DISCORD_EMBED_DESCRIPTION_LIMIT, "...")
    
    return discord.Embed(
        title=safe_title,
        description=safe_description,
        color=color,
        timestamp=get_current_vietnam_datetime()
    )

# üîß Enhanced headers with retry mechanism - OPTIMIZED for 2025
def get_enhanced_headers(url=None):
    """Enhanced headers for Yahoo Finance with anti-blocking - OPTIMIZED"""
    user_agent = random.choice(USER_AGENTS)
    
    headers = {
        'User-Agent': user_agent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9,vi;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'DNT': '1',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache',
        'Sec-Fetch-Site': 'same-origin',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-User': '?1',
        'Sec-Fetch-Dest': 'document'
    }
    
    if url and 'yahoo' in url.lower():
        headers.update({
            'Referer': 'https://finance.yahoo.com/',
            'Origin': 'https://finance.yahoo.com',
            'Host': 'finance.yahoo.com' if 'finance.yahoo.com' in url else 'feeds.finance.yahoo.com'
        })
    elif url and 'cafef.vn' in url.lower():
        headers.update({
            'Referer': 'https://cafef.vn/',
            'Origin': 'https://cafef.vn'
        })
    
    return headers

def add_random_delay():
    """Add random delay to avoid rate limiting - SHORTER for optimization"""
    delay = random.uniform(0.3, 1.5)  # Reduced from 0.5-2.0
    time.sleep(delay)

def is_international_source(source_name):
    """Check if source is international - FIXED for all RSS sources"""
    international_sources = [
        'yahoo_finance', 'cnn_money', 'reuters', 'marketwatch', 'business_insider',
        'forbes', 'wsj', 'cnbc', 'investing_com', 'seekingalpha', 'financial_times',
        'fortune', 'economist', 'nasdaq', 'washington_post', 'guardian_business',
        'investopedia', 'nikkei_asia', 'economic_times', 'bbc_news', 'coindesk'
    ]
    return any(source in source_name for source in international_sources)

def create_fallback_content(url, source_name, error_msg=""):
    """Create fallback content when extraction fails - FIXED for all sources"""
    try:
        article_id = url.split('/')[-1] if '/' in url else 'news-article'
        
        if is_international_source(source_name):
            # Get actual source display name
            source_display = "Financial News"
            if 'marketwatch' in source_name:
                source_display = "MarketWatch"
            elif 'reuters' in source_name:
                source_display = "Reuters"
            elif 'cnn' in source_name:
                source_display = "CNN Money"
            elif 'forbes' in source_name:
                source_display = "Forbes"
            elif 'wsj' in source_name:
                source_display = "Wall Street Journal"
            elif 'cnbc' in source_name:
                source_display = "CNBC"
            elif 'bbc' in source_name:
                source_display = "BBC News"
            
            return f"""**{source_display} Financial News:**

üìà **Market Analysis:** This article provides financial market insights and economic analysis.

üìä **Coverage Areas:**
‚Ä¢ Real-time market data and analysis
‚Ä¢ Economic indicators and trends
‚Ä¢ Corporate earnings and reports
‚Ä¢ Investment strategies and forecasts

**Article ID:** {article_id}
**Note:** Content extraction failed. Please visit the original link for complete article.

{f'**Technical Error:** {error_msg}' if error_msg else ''}"""
        else:
            return f"""**Tin t·ª©c kinh t·∫ø CafeF:**

üì∞ **Th√¥ng tin kinh t·∫ø:** B√†i vi·∫øt cung c·∫•p th√¥ng tin kinh t·∫ø, t√†i ch√≠nh t·ª´ CafeF.

üìä **N·ªôi dung chuy√™n s√¢u:**
‚Ä¢ Ph√¢n t√≠ch th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam
‚Ä¢ Tin t·ª©c kinh t·∫ø vƒ© m√¥ v√† ch√≠nh s√°ch
‚Ä¢ B√°o c√°o doanh nghi·ªáp v√† t√†i ch√≠nh
‚Ä¢ B·∫•t ƒë·ªông s·∫£n v√† ƒë·∫ßu t∆∞

**M√£ b√†i vi·∫øt:** {article_id}
**L∆∞u √Ω:** ƒê·ªÉ ƒë·ªçc ƒë·∫ßy ƒë·ªß, vui l√≤ng truy c·∫≠p link g·ªëc.

{f'**L·ªói:** {error_msg}' if error_msg else ''}"""
        
    except Exception as e:
        return f"N·ªôi dung t·ª´ {source_name}. Vui l√≤ng truy c·∫≠p link g·ªëc ƒë·ªÉ ƒë·ªçc ƒë·∫ßy ƒë·ªß."

async def extract_content_with_gemini(url, source_name):
    """Use Gemini to extract and translate content from international news"""
    try:
        if not GEMINI_API_KEY or not GEMINI_AVAILABLE:
            return create_fallback_content(url, source_name, "Gemini kh√¥ng kh·∫£ d·ª•ng")
        
        extraction_prompt = f"""You are a financial news content extractor and translator. Access and process this news article:

**ARTICLE URL:** {url}

**INSTRUCTIONS:**
1. Access and read the COMPLETE article content from the URL
2. Extract main content (remove ads, sidebar, footer)
3. Translate from English to Vietnamese naturally and accurately
4. Preserve all numbers, percentages, company names, financial terms
5. Use standard Vietnamese economic-financial terminology
6. Do NOT add personal commentary or explanations
7. Return translated content with clear structure
8. FOCUS ONLY on the source article content - do not reference other news sources

**IMPORTANT:** Only return the translated article content from the provided URL. Do not mention CafeF, Yahoo Finance, or other sources unless they appear in the original article.

**TRANSLATED CONTENT:**"""

        try:
            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.1,
                top_p=0.8,
                max_output_tokens=2500,
            )
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    model.generate_content,
                    extraction_prompt,
                    generation_config=generation_config
                ),
                timeout=25
            )
            
            extracted_content = response.text.strip()
            
            if len(extracted_content) > 200:
                error_indicators = [
                    'cannot access', 'unable to access', 'kh√¥ng th·ªÉ truy c·∫≠p',
                    'failed to retrieve', 'error occurred', 'sorry, i cannot'
                ]
                
                if not any(indicator in extracted_content.lower() for indicator in error_indicators):
                    return f"[ü§ñ Gemini AI tr√≠ch xu·∫•t t·ª´ {source_name}]\n\n{extracted_content}"
                else:
                    return create_fallback_content(url, source_name, "Gemini kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung")
            else:
                return create_fallback_content(url, source_name, "Gemini tr·∫£ v·ªÅ n·ªôi dung qu√° ng·∫Øn")
            
        except asyncio.TimeoutError:
            return create_fallback_content(url, source_name, "Gemini timeout")
        except Exception as e:
            return create_fallback_content(url, source_name, f"L·ªói Gemini: {str(e)}")
            
    except Exception as e:
        return create_fallback_content(url, source_name, str(e))

# üÜï OPTIMIZED YAHOO FINANCE NEWS SCRAPING - Fixed for 2025
def scrape_yahoo_finance_news(base_url, limit=20):  # Reduced limit from 30
    """OPTIMIZED scrape news directly from Yahoo Finance - Fixed URLs 2025"""
    try:
        print(f"üîÑ Optimized scraping: {base_url}")
        add_random_delay()
        
        session = requests.Session()
        headers = get_enhanced_headers(base_url)
        session.headers.update(headers)
        
        # SHORTER timeout to prevent heartbeat blocking
        response = session.get(base_url, timeout=10, allow_redirects=True)  # Reduced from 15
        
        if response.status_code != 200:
            print(f"‚ùå Failed to scrape {base_url}: {response.status_code}")
            return []
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # OPTIMIZED selectors for Yahoo Finance 2025
        news_articles = []
        
        # Enhanced selectors for different Yahoo Finance page types
        article_selectors = [
            # News page selectors
            'h3 > a[href*="/news/"]',
            'a[href*="/news/"][data-module]',
            'h3 a[href*="/news/"]',
            'h2 a[href*="/news/"]',
            
            # Sector page selectors
            'div[data-testid] a[href*="/quote/"]',
            '.js-content-viewer a',
            'article a[href*="/news/"]',
            
            # Video page selectors
            'a[href*="/video/"]',
            
            # General news selectors
            'h3 a',
            'h2 a',
            '.newsItem a',
            '.story a'
        ]
        
        for selector in article_selectors:
            try:
                elements = soup.select(selector)[:limit]  # Limit early
                for element in elements:
                    try:
                        # Extract title and link
                        if element.name == 'a':
                            title = element.get_text(strip=True)
                            link = element.get('href', '')
                        else:
                            # If it's h3 or other element, find the link inside
                            link_elem = element.find('a') if element.name != 'a' else element
                            if not link_elem:
                                continue
                            title = link_elem.get_text(strip=True)
                            link = link_elem.get('href', '')
                        
                        # Clean and validate
                        if not title or not link or len(title) < 10:
                            continue
                        
                        # Fix relative URLs
                        if link.startswith('/'):
                            link = f"https://finance.yahoo.com{link}"
                        elif not link.startswith('http'):
                            continue
                        
                        # Filter for financial/economic content - RELAXED filter
                        if is_relevant_financial_news_relaxed(title):
                            news_item = {
                                'title': html.unescape(title.strip()),
                                'link': link,
                                'source': f"yahoo_finance_scraped",
                                'published': get_current_vietnam_datetime(),
                                'published_str': get_current_vietnam_datetime().strftime("%H:%M %d/%m"),
                                'description': title[:200] + "..." if len(title) > 200 else title
                            }
                            
                            # Check for duplicates
                            if not is_duplicate_article(news_item, news_item['source']):
                                news_articles.append(news_item)
                        
                    except Exception as e:
                        continue
                
                if len(news_articles) >= limit:
                    break
                    
            except Exception as e:
                continue
        
        session.close()
        print(f"‚úÖ Scraped {len(news_articles)} unique articles from {base_url}")
        return news_articles[:limit]
        
    except Exception as e:
        print(f"‚ùå Scraping error for {base_url}: {e}")
        return []

def is_relevant_financial_news_relaxed(title):
    """RELAXED filter for relevant financial/economic news - More inclusive"""
    financial_keywords = [
        # Core financial terms
        'stock', 'market', 'trading', 'investment', 'investor', 'wall street',
        'nasdaq', 'dow', 's&p', 'earnings', 'revenue', 'profit', 'loss',
        'financial', 'finance', 'economy', 'economic', 'fed', 'federal reserve',
        'interest rate', 'inflation', 'gdp', 'unemployment', 'jobs', 'employment',
        
        # Crypto and digital assets
        'bitcoin', 'crypto', 'cryptocurrency', 'ethereum', 'digital asset',
        
        # Banking and financial services
        'bank', 'banking', 'credit', 'loan', 'mortgage', 'debt',
        'ipo', 'merger', 'acquisition', 'dividend', 'bond', 'treasury',
        
        # Commodities and currencies
        'currency', 'dollar', 'euro', 'commodity', 'oil', 'gold', 'silver',
        
        # Real estate and housing
        'real estate', 'housing', 'property', 'reit',
        
        # Business and corporate
        'retail', 'consumer', 'spending', 'sales', 'manufacturing', 'industrial',
        'tech', 'technology', 'ai', 'artificial intelligence', 'startup',
        'venture capital', 'hedge fund', 'mutual fund', 'etf', 'pension',
        
        # Economic indicators
        'tariff', 'trade', 'export', 'import', 'growth', 'recession',
        'bull market', 'bear market', 'volatility',
        
        # Company names and sectors
        'apple', 'microsoft', 'google', 'amazon', 'tesla', 'nvidia',
        'jp morgan', 'goldman sachs', 'berkshire'
    ]
    
    title_lower = title.lower()
    return any(keyword in title_lower for keyword in financial_keywords)

# üöÄ ENHANCED CONTENT EXTRACTION - USE GEMINI FOR ALL INTERNATIONAL SOURCES
async def extract_content_enhanced(url, source_name, news_item=None):
    """Enhanced content extraction - Gemini for ALL international sources"""
    
    # For ALL international sources, use Gemini (not just Yahoo Finance)
    if is_international_source(source_name):
        print(f"ü§ñ Using Gemini for international source: {source_name}")
        return await extract_content_with_gemini(url, source_name)
    
    # For domestic (CafeF) sources, use traditional methods
    try:
        print(f"üîß Using traditional methods for domestic source: {source_name}")
        add_random_delay()
        session = requests.Session()
        headers = get_enhanced_headers(url)
        session.headers.update(headers)
        
        response = session.get(url, timeout=15, allow_redirects=True)
        
        if response.status_code == 200:
            # Method 1: Trafilatura
            if TRAFILATURA_AVAILABLE:
                try:
                    result = trafilatura.bare_extraction(
                        response.content,
                        include_comments=False,
                        include_tables=True,
                        include_links=False,
                        favor_precision=True,
                        with_metadata=True
                    )
                    
                    if result and result.get('text') and len(result['text']) > 300:
                        content = result['text']
                        session.close()
                        return content.strip()
                except Exception as e:
                    print(f"‚ö†Ô∏è Trafilatura failed: {e}")
            
            # Method 2: Newspaper3k
            if NEWSPAPER_AVAILABLE:
                try:
                    session.close()
                    article = Article(url)
                    article.set_config({
                        'headers': headers,
                        'timeout': 15
                    })
                    
                    article.download()
                    article.parse()
                    
                    if article.text and len(article.text) > 300:
                        return article.text.strip()
                
                except Exception as e:
                    print(f"‚ö†Ô∏è Newspaper3k failed: {e}")
            
            # Method 3: BeautifulSoup for CafeF
            if BEAUTIFULSOUP_AVAILABLE:
                try:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # CafeF-specific selectors
                    content_selectors = [
                        'div.detail-content',
                        'div.fck_detail',
                        'div.content-detail',
                        'div.article-content',
                        'div.entry-content',
                        'div.post-content',
                        'article',
                        'main'
                    ]
                    
                    content = ""
                    for selector in content_selectors:
                        elements = soup.select(selector)
                        if elements:
                            for element in elements:
                                text = element.get_text(strip=True)
                                if len(text) > 500:
                                    content = text
                                    break
                            if content:
                                break
                    
                    if content and len(content) > 500:
                        content = clean_content_enhanced(content)
                        session.close()
                        return content.strip()
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è BeautifulSoup failed: {e}")
        
        session.close()
        print(f"‚ö†Ô∏è All traditional methods failed for {source_name}")
        return create_fallback_content(url, source_name, "Traditional extraction methods failed")
        
    except Exception as e:
        print(f"‚ùå Extract content error for {source_name}: {e}")
        return create_fallback_content(url, source_name, str(e))

def clean_content_enhanced(content):
    """Enhanced content cleaning for CafeF"""
    if not content:
        return content
    
    # Remove common patterns
    unwanted_patterns = [
        r'Theo.*?CafeF.*?',
        r'Ngu·ªìn.*?:.*?',
        r'Tags:.*?$',
        r'T·ª´ kh√≥a:.*?$',
        r'ƒêƒÉng k√Ω.*?nh·∫≠n tin.*?',
        r'Like.*?Fanpage.*?',
        r'Follow.*?us.*?'
    ]
    
    for pattern in unwanted_patterns:
        content = re.sub(pattern, '', content, flags=re.IGNORECASE | re.DOTALL)
    
    # Remove excessive whitespace
    content = re.sub(r'\s+', ' ', content)
    content = re.sub(r'\n\s*\n', '\n', content)
    
    return content.strip()

# üöÄ OPTIMIZED NEWS COLLECTION - Reduced limits to prevent timeout
async def collect_news_enhanced(sources_dict, limit_per_source=20):  # Reduced from 50
    """OPTIMIZED news collection with RSS feeds + direct scraping"""
    all_news = []
    
    for source_name, source_url in sources_dict.items():
        retry_count = 0
        max_retries = 2  # Reduced from 3
        
        while retry_count < max_retries:
            try:
                print(f"üîÑ Processing {source_name} (attempt {retry_count + 1}): {source_url}")
                
                # Determine if it's RSS or direct scraping
                if source_url.endswith('.rss') or 'rss' in source_url.lower() or 'feeds.' in source_url:
                    # RSS Feed processing
                    news_items = await process_rss_feed(source_name, source_url, limit_per_source)
                else:
                    # Direct scraping for Yahoo Finance news pages
                    news_items = scrape_yahoo_finance_news(source_url, limit_per_source)
                
                if news_items:
                    duplicates_found = 0
                    for news_item in news_items:
                        if not is_duplicate_article(news_item, source_name):
                            all_news.append(news_item)
                        else:
                            duplicates_found += 1
                    
                    entries_processed = len(news_items) - duplicates_found
                    print(f"‚úÖ Processed {entries_processed} unique entries from {source_name} (skipped {duplicates_found} duplicates)")
                    break  # Success, exit retry loop
                else:
                    if retry_count < max_retries - 1:
                        retry_count += 1
                        print(f"üîÑ Retrying {source_name}...")
                        time.sleep(1)  # Reduced sleep time
                        continue
                    else:
                        print(f"‚ùå No content from {source_name} after {max_retries} attempts")
                        break
                
            except Exception as e:
                print(f"‚ùå Error for {source_name}: {e}")
                if retry_count < max_retries - 1:
                    retry_count += 1
                    print(f"üîÑ Retrying {source_name}...")
                    time.sleep(1)  # Reduced sleep time
                else:
                    print(f"‚ùå Failed to fetch from {source_name} after {max_retries} attempts")
                    break
    
    print(f"üìä Total unique news collected: {len(all_news)}")
    
    # Sort by publish time
    all_news.sort(key=lambda x: x['published'], reverse=True)
    return all_news

async def process_rss_feed(source_name, rss_url, limit_per_source):
    """Process RSS feed with enhanced error handling"""
    try:
        add_random_delay()
        session = requests.Session()
        headers = get_enhanced_headers(rss_url)
        session.headers.update(headers)
        
        response = session.get(rss_url, timeout=10, allow_redirects=True)  # Reduced timeout
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
        elif response.status_code in [403, 429]:
            print(f"‚ö†Ô∏è Rate limited for {source_name}, waiting...")
            time.sleep(random.uniform(2.0, 4.0))  # Reduced wait time
            headers['User-Agent'] = random.choice(USER_AGENTS)
            session.headers.update(headers)
            response = session.get(rss_url, timeout=10, allow_redirects=True)
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
            else:
                feed = feedparser.parse(rss_url)
        else:
            feed = feedparser.parse(rss_url)
        
        session.close()
        
        if not feed or not hasattr(feed, 'entries') or len(feed.entries) == 0:
            return []
        
        news_items = []
        for entry in feed.entries[:limit_per_source]:
            try:
                vn_time = get_current_vietnam_datetime()
                
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    vn_time = convert_utc_to_vietnam_time(entry.published_parsed)
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    vn_time = convert_utc_to_vietnam_time(entry.updated_parsed)
                
                description = ""
                if hasattr(entry, 'summary'):
                    description = entry.summary[:400] + "..." if len(entry.summary) > 400 else entry.summary
                elif hasattr(entry, 'description'):
                    description = entry.description[:400] + "..." if len(entry.description) > 400 else entry.description
                
                if hasattr(entry, 'title') and hasattr(entry, 'link'):
                    title = entry.title.strip()
                    
                    # Filter for relevant economic/financial content
                    if is_relevant_news(title, description, source_name):
                        news_item = {
                            'title': html.unescape(title),
                            'link': entry.link,
                            'source': source_name,
                            'published': vn_time,
                            'published_str': vn_time.strftime("%H:%M %d/%m"),
                            'description': html.unescape(description) if description else ""
                        }
                        news_items.append(news_item)
                
            except Exception as entry_error:
                continue
        
        return news_items
        
    except Exception as e:
        return []

def is_relevant_news(title, description, source_name):
    """Filter for relevant economic/financial news"""
    
    # For CafeF sources, all content is relevant (already filtered by RSS category)
    if 'cafef' in source_name:
        return True
    
    # For Yahoo Finance, use relaxed filter
    if 'yahoo_finance' in source_name:
        return is_relevant_financial_news_relaxed(title)
    
    return True

def save_user_news_enhanced(user_id, news_list, command_type):
    """Enhanced user news saving"""
    global user_news_cache
    
    user_news_cache[user_id] = {
        'news': news_list,
        'command': command_type,
        'timestamp': get_current_vietnam_datetime()
    }
    
    if len(user_news_cache) > MAX_CACHE_ENTRIES:
        oldest_users = sorted(user_news_cache.items(), key=lambda x: x[1]['timestamp'])[:10]
        for user_id_to_remove, _ in oldest_users:
            del user_news_cache[user_id_to_remove]

def save_user_last_detail(user_id, news_item):
    """Save last article accessed via !chitiet"""
    global user_last_detail_cache
    
    user_last_detail_cache[user_id] = {
        'article': news_item,
        'timestamp': get_current_vietnam_datetime()
    }

# üîß DISCORD EMBED HELPERS
def split_text_for_discord(text: str, max_length: int = 950) -> List[str]:
    """Split text to fit Discord field limits"""
    if len(text) <= max_length:
        return [text]
    
    parts = []
    current_part = ""
    
    sentences = text.split('. ')
    
    for sentence in sentences:
        if len(current_part + sentence + '. ') <= max_length:
            current_part += sentence + '. '
        else:
            if current_part:
                parts.append(current_part.strip())
                current_part = sentence + '. '
            else:
                parts.append(sentence[:max_length])
                current_part = ""
    
    if current_part:
        parts.append(current_part.strip())
    
    return parts

def create_optimized_embeds(title: str, content: str, color: int = 0x9932cc) -> List[discord.Embed]:
    """Create optimized embeds for Discord limits"""
    embeds = []
    
    content_parts = split_text_for_discord(content, 950)
    
    for i, part in enumerate(content_parts):
        if i == 0:
            embed = discord.Embed(
                title=validate_and_truncate_content(title, DISCORD_EMBED_TITLE_LIMIT),
                color=color,
                timestamp=get_current_vietnam_datetime()
            )
        else:
            embed = discord.Embed(
                title=validate_and_truncate_content(f"{title[:150]}... (Ph·∫ßn {i+1})", DISCORD_EMBED_TITLE_LIMIT),
                color=color,
                timestamp=get_current_vietnam_datetime()
            )
        
        field_name = f"üìÑ N·ªôi dung {f'(Ph·∫ßn {i+1})' if len(content_parts) > 1 else ''}"
        safe_field_name, safe_field_value = validate_embed_field(field_name, part)
        
        embed.add_field(
            name=safe_field_name,
            value=safe_field_value,
            inline=False
        )
        
        embeds.append(embed)
    
    return embeds

def create_safe_embed_with_fields(title: str, description: str, fields_data: List[Tuple[str, str]], color: int = 0x00ff88) -> List[discord.Embed]:
    """Create safe embeds with multiple fields"""
    embeds = []
    
    safe_title = validate_and_truncate_content(title, DISCORD_EMBED_TITLE_LIMIT, "...")
    safe_description = validate_and_truncate_content(description, DISCORD_EMBED_DESCRIPTION_LIMIT, "...")
    
    main_embed = discord.Embed(
        title=safe_title,
        description=safe_description,
        color=color,
        timestamp=get_current_vietnam_datetime()
    )
    
    fields_added = 0
    current_embed = main_embed
    total_chars = len(safe_title) + len(safe_description)
    
    for field_name, field_value in fields_data:
        safe_name, safe_value = validate_embed_field(field_name, field_value)
        
        field_chars = len(safe_name) + len(safe_value)
        
        if fields_added >= 20 or total_chars + field_chars > DISCORD_EMBED_TOTAL_EMBED_LIMIT:
            embeds.append(current_embed)
            current_embed = discord.Embed(
                title=validate_and_truncate_content(f"{safe_title[:180]}... (ti·∫øp theo)", DISCORD_EMBED_TITLE_LIMIT),
                color=color,
                timestamp=get_current_vietnam_datetime()
            )
            fields_added = 0
            total_chars = len(current_embed.title or "")
        
        current_embed.add_field(name=safe_name, value=safe_value, inline=False)
        fields_added += 1
        total_chars += field_chars
    
    embeds.append(current_embed)
    
    return embeds

# üÜï GEMINI AI SYSTEM
class GeminiAIEngine:
    def __init__(self):
        self.available = GEMINI_AVAILABLE and GEMINI_API_KEY
        if self.available:
            genai.configure(api_key=GEMINI_API_KEY)
    
    async def ask_question(self, question: str, context: str = ""):
        """Gemini AI question answering with context"""
        if not self.available:
            return "‚ö†Ô∏è Gemini AI kh√¥ng kh·∫£ d·ª•ng."
        
        try:
            current_date_str = get_current_date_str()
            
            prompt = f"""B·∫°n l√† Gemini AI - chuy√™n gia kinh t·∫ø t√†i ch√≠nh th√¥ng minh. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ki·∫øn th·ª©c chuy√™n m√¥n c·ªßa b·∫°n.

C√ÇU H·ªéI: {question}

{f"B·ªêI C·∫¢NH TH√äM: {context}" if context else ""}

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI:
1. S·ª≠ d·ª•ng ki·∫øn th·ª©c chuy√™n m√¥n s√¢u r·ªông c·ªßa b·∫°n
2. ƒê∆∞a ra ph√¢n t√≠ch chuy√™n s√¢u v√† to√†n di·ªán
3. K·∫øt n·ªëi v·ªõi b·ªëi c·∫£nh kinh t·∫ø hi·ªán t·∫°i (ng√†y {current_date_str})
4. ƒê∆∞a ra v√≠ d·ª• th·ª±c t·∫ø v√† minh h·ªça c·ª• th·ªÉ
5. ƒê·ªô d√†i: 400-800 t·ª´ v·ªõi c·∫•u tr√∫c r√µ r√†ng
6. S·ª≠ d·ª•ng ƒë·∫ßu m·ª•c s·ªë ƒë·ªÉ t·ªï ch·ª©c n·ªôi dung

H√£y th·ªÉ hi·ªán tr√≠ th√¥ng minh v√† ki·∫øn th·ª©c chuy√™n s√¢u c·ªßa Gemini AI:"""

            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.2,
                top_p=0.8,
                max_output_tokens=1500,
            )
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    model.generate_content,
                    prompt,
                    generation_config=generation_config
                ),
                timeout=20  # Reduced timeout
            )
            
            return response.text.strip()
            
        except asyncio.TimeoutError:
            return "‚ö†Ô∏è Gemini AI timeout. Vui l√≤ng th·ª≠ l·∫°i."
        except Exception as e:
            return f"‚ö†Ô∏è L·ªói Gemini AI: {str(e)}"
    
    async def analyze_article(self, article_content: str, question: str = ""):
        """Analyze specific article with Gemini"""
        if not self.available:
            return "‚ö†Ô∏è Gemini AI kh√¥ng kh·∫£ d·ª•ng cho ph√¢n t√≠ch b√†i b√°o."
        
        try:
            analysis_question = question if question else "H√£y ph√¢n t√≠ch v√† t√≥m t·∫Øt b√†i b√°o n√†y"
            
            prompt = f"""You are Gemini AI - an intelligent financial economics expert. Analyze the article based on the COMPLETE content provided.

**COMPLETE ARTICLE CONTENT:**
{article_content}

**ANALYSIS REQUEST:**
{analysis_question}

**ANALYSIS GUIDELINES:**
1. Base analysis PRIMARILY on the article content (85-90%)
2. Combine with professional knowledge for deeper explanation (10-15%)
3. Analyze impact, causes, consequences
4. Provide insights and in-depth assessments
5. Answer questions directly with evidence from the article
6. Length: 600-1000 words with clear structure
7. Reference specific parts of the article
8. ONLY analyze the provided article - do not reference other news sources unless mentioned in the original

**IMPORTANT:** Focus solely on the content from the provided article. Do not mention CafeF, Yahoo Finance, or other sources unless they appear in the original article.

Provide INTELLIGENT and DETAILED analysis:"""

            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.2,
                top_p=0.8,
                max_output_tokens=2000,
            )
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    model.generate_content,
                    prompt,
                    generation_config=generation_config
                ),
                timeout=30  # Reduced timeout
            )
            
            return response.text.strip()
            
        except asyncio.TimeoutError:
            return "‚ö†Ô∏è Gemini AI timeout khi ph√¢n t√≠ch b√†i b√°o."
        except Exception as e:
            return f"‚ö†Ô∏è L·ªói Gemini AI: {str(e)}"
    
    async def debate_perspectives(self, topic: str):
        """Multi-perspective debate system with distinct moral characteristics"""
        if not self.available:
            return "‚ö†Ô∏è Gemini AI kh√¥ng kh·∫£ d·ª•ng cho debate."
        
        try:
            prompt = f"""B·∫°n l√† Gemini AI v·ªõi kh·∫£ nƒÉng ƒë√≥ng nhi·ªÅu vai tr√≤ kh√°c nhau. H√£y t·ªï ch·ª©c m·ªôt cu·ªôc tranh lu·∫≠n v·ªÅ ch·ªß ƒë·ªÅ sau t·ª´ 6 g√≥c nh√¨n kh√°c nhau v·ªõi t√≠nh c√°ch ƒë·∫°o ƒë·ª©c ri√™ng bi·ªát:

**CH·ª¶ ƒê·ªÄ TRANH LU·∫¨N:** {topic}

**C√ÅC TH√ÇN PH·∫¨N THAM GIA:**
1. **Nh√† Kinh T·∫ø H·ªçc Tham Nh≈©ng** - C√≥ ƒë·∫°o ƒë·ª©c ngh·ªÅ nghi·ªáp t·ªá h·∫°i, b√≥p m√©o s·ªë li·ªáu, ch·ªâ ph·ª•c v·ª• quy·ªÅn l·ª£i c√° nh√¢n
2. **Ph√≥ Gi√°o S∆∞ Ti·∫øn Sƒ© Ch√≠nh Tr·ª±c** - C√≥ ƒë·∫°o ƒë·ª©c cao, h·ªçc thu·∫≠t nghi√™m t√∫c, quan t√¢m l·ª£i √≠ch chung
3. **Nh√¢n Vi√™n VP Ham Ti·ªÅn** - Ch·ªâ quan t√¢m l∆∞∆°ng th∆∞·ªüng, s·∫µn s√†ng v·ª©t b·ªè ƒë·∫°o ƒë·ª©c v√¨ l·ª£i nhu·∫≠n
4. **Ng∆∞·ªùi Ngh√®o V√¥ H·ªçc** - T·∫ßng l·ªõp th·∫•p, kh√¥ng h·ªçc th·ª©c, ƒë·∫°o ƒë·ª©c t·ªá, hay ƒë·ªï l·ªói cho ng∆∞·ªùi kh√°c
5. **Ng∆∞·ªùi Gi√†u √çch K·ª∑** - Ch·ªâ t√¨m c√°ch b·ªè ti·ªÅn v√†o t√∫i m√¨nh, kh√¥ng quan t√¢m h·∫≠u qu·∫£ x√£ h·ªôi
6. **Ng∆∞·ªùi Gi√†u Th√¥ng Th√°i** - C√≥ t·∫ßm nh√¨n xa, hi·ªÉu bi·∫øt s√¢u r·ªông, quan t√¢m ph√°t tri·ªÉn b·ªÅn v·ªØng

**Y√äU C·∫¶U:**
- M·ªói th√¢n ph·∫≠n ƒë∆∞a ra 1 ƒëo·∫°n tranh lu·∫≠n (100-150 t·ª´)
- Th·ªÉ hi·ªán R√ï R√ÄNG t√≠nh c√°ch ƒë·∫°o ƒë·ª©c v√† ƒë·ªông c∆° c·ªßa t·ª´ng nh√¢n v·∫≠t
- T·∫°o ra m√¢u thu·∫´n v√† xung ƒë·ªôt quan ƒëi·ªÉm
- Ph·∫£n √°nh th·ª±c t·∫ø x√£ h·ªôi m·ªôt c√°ch s·∫Øc b√©n
- K·∫øt th√∫c b·∫±ng ph√¢n t√≠ch t·ªïng h·ª£p t·ª´ Gemini AI

**FORMAT:**
üí∏ **Nh√† KT H·ªçc Tham Nh≈©ng:** [quan ƒëi·ªÉm √≠ch k·ª∑, b√≥p m√©o]
üë®‚Äçüè´ **PGS.TS Ch√≠nh Tr·ª±c:** [quan ƒëi·ªÉm h·ªçc thu·∫≠t, ƒë·∫°o ƒë·ª©c cao]
üíº **Nh√¢n Vi√™n Ham Ti·ªÅn:** [ch·ªâ quan t√¢m l∆∞∆°ng th∆∞·ªüng]
üò† **Ng∆∞·ªùi Ngh√®o V√¥ H·ªçc:** [ƒë·ªï l·ªói, thi·∫øu hi·ªÉu bi·∫øt]
ü§ë **Ng∆∞·ªùi Gi√†u √çch K·ª∑:** [ch·ªâ t√¨m l·ª£i nhu·∫≠n c√° nh√¢n]
üß† **Ng∆∞·ªùi Gi√†u Th√¥ng Th√°i:** [t·∫ßm nh√¨n xa, ph√°t tri·ªÉn b·ªÅn v·ªØng]
ü§ñ **Gemini AI - T·ªïng K·∫øt:** [ph√¢n t√≠ch kh√°ch quan c√°c quan ƒëi·ªÉm]

H√£y t·∫°o ra cu·ªôc tranh lu·∫≠n gay g·∫Øt v√† ph·∫£n √°nh th·ª±c t·∫ø x√£ h·ªôi:"""

            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.4,
                top_p=0.9,
                max_output_tokens=2000,
            )
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    model.generate_content,
                    prompt,
                    generation_config=generation_config
                ),
                timeout=25  # Reduced timeout
            )
            
            return response.text.strip()
            
        except asyncio.TimeoutError:
            return "‚ö†Ô∏è Gemini AI timeout khi t·ªï ch·ª©c debate."
        except Exception as e:
            return f"‚ö†Ô∏è L·ªói Gemini AI: {str(e)}"

# Initialize Gemini Engine
gemini_engine = GeminiAIEngine()

# Bot event handlers
@bot.event
async def on_ready():
    print(f'‚úÖ {bot.user} is online!')
    
    ai_status = "‚úÖ Available" if gemini_engine.available else "‚ùå Unavailable"
    current_datetime_str = get_current_datetime_str()
    
    total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
    
    status_text = f"News Bot ‚Ä¢ {total_sources} sources"
    await bot.change_presence(
        activity=discord.Activity(
            type=discord.ActivityType.watching,
            name=status_text
        )
    )
    
    print(f"ü§ñ Gemini AI: {ai_status}")
    print(f"üìä Sources: {total_sources}")
    print(f"üï∞Ô∏è Started: {current_datetime_str}")

@bot.event
async def on_command_error(ctx, error):
    if isinstance(error, commands.CommandNotFound):
        return
    elif isinstance(error, commands.MissingRequiredArgument):
        await ctx.send("‚ùå Thi·∫øu tham s·ªë! G√µ `!menu` ƒë·ªÉ xem h∆∞·ªõng d·∫´n.")
    elif isinstance(error, commands.BadArgument):
        await ctx.send("‚ùå Tham s·ªë kh√¥ng h·ª£p l·ªá! G√µ `!menu` ƒë·ªÉ xem h∆∞·ªõng d·∫´n.")
    else:
        await ctx.send(f"‚ùå L·ªói: {str(error)}")

# üÜï ENHANCED COMMANDS

@bot.command(name='all')
async def get_all_news_enhanced(ctx, page=1):
    """Tin t·ª©c t·ª´ CafeF v√† Yahoo Finance v·ªõi Gemini-powered extraction"""
    try:
        page = max(1, int(page))
        loading_msg = await ctx.send(f"‚è≥ ƒêang t·∫£i...")
        
        domestic_news = await collect_news_enhanced(RSS_FEEDS['domestic'], 20)  # Reduced limit
        international_news = await collect_news_enhanced(RSS_FEEDS['international'], 30)  # Reduced limit
        
        await loading_msg.delete()
        
        all_news = domestic_news + international_news
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = all_news[start_index:end_index]
        
        if not page_news:
            total_pages = (len(all_news) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        # Prepare fields data
        fields_data = []
        
        domestic_count = sum(1 for news in page_news if news['source'] in RSS_FEEDS['domestic'])
        international_count = len(page_news) - domestic_count
        
        # MASSIVE source mapping for 20+ RSS feeds
        source_names = {
            # CafeF sources
            'cafef_chungkhoan': 'CafeF CK', 'cafef_batdongsan': 'CafeF BƒêS',
            'cafef_taichinh': 'CafeF TC', 'cafef_vimo': 'CafeF VM', 'cafef_doanhnghiep': 'CafeF DN',
            
            # Yahoo Finance sources
            'yahoo_finance_main': 'Yahoo RSS', 'yahoo_finance_headlines': 'Yahoo Headlines',
            'yahoo_finance_rss': 'Yahoo Finance', 'yahoo_finance_crypto': 'Yahoo Crypto',
            'yahoo_finance_tech': 'Yahoo Tech', 'yahoo_finance_stock_market': 'Yahoo Stocks',
            
            # Major financial news sources
            'cnn_money': 'CNN Money', 'reuters_topnews': 'Reuters', 'marketwatch': 'MarketWatch',
            'business_insider': 'Business Insider', 'forbes': 'Forbes', 'wsj': 'Wall Street Journal',
            'cnbc': 'CNBC', 'investing_com': 'Investing.com', 'seekingalpha': 'Seeking Alpha',
            'financial_times': 'Financial Times', 'fortune': 'Fortune', 'economist': 'The Economist',
            'nasdaq': 'Nasdaq', 'washington_post_biz': 'Washington Post', 'guardian_business': 'The Guardian',
            'investopedia': 'Investopedia', 'nikkei_asia': 'Nikkei Asia', 'economic_times': 'Economic Times',
            'bbc_news': 'BBC News', 'coindesk': 'CoinDesk',
            
            # Scraped sources
            'yahoo_finance_scraped': 'Yahoo Scraped'
        }
        
        emoji_map = {
            # CafeF sources
            'cafef_chungkhoan': 'üìà', 'cafef_batdongsan': 'üè¢', 'cafef_taichinh': 'üí∞', 
            'cafef_vimo': 'üìä', 'cafef_doanhnghiep': 'üè≠',
            
            # Yahoo Finance sources
            'yahoo_finance_main': 'üíº', 'yahoo_finance_headlines': 'üì∞', 'yahoo_finance_rss': 'üíº',
            'yahoo_finance_crypto': 'üí∞', 'yahoo_finance_tech': 'üíª', 'yahoo_finance_stock_market': 'üìà',
            
            # Major financial news sources
            'cnn_money': 'üì∫', 'reuters_topnews': 'üåç', 'marketwatch': 'üìä', 'business_insider': 'üíº',
            'forbes': 'üíé', 'wsj': 'üì∞', 'cnbc': 'üì∫', 'investing_com': 'üíπ', 'seekingalpha': 'üîç',
            'financial_times': 'üìä', 'fortune': 'üí∞', 'economist': 'üéØ', 'nasdaq': 'üìà',
            'washington_post_biz': 'üì∞', 'guardian_business': 'üõ°Ô∏è', 'investopedia': 'üìö',
            'nikkei_asia': 'üåè', 'economic_times': 'üáÆüá≥', 'bbc_news': 'üá¨üáß', 'coindesk': '‚Çø',
            
            # Scraped sources
            'yahoo_finance_scraped': 'üöÄ'
        }
        
        # Simple statistics
        stats_field = f"üáªüá≥ CafeF: {domestic_count} ‚Ä¢ üåç International: {international_count} ‚Ä¢ üìä T·ªïng: {len(all_news)}\nüî• NEW: 20+ RSS feeds t·ª´ GitHub sources!"
        fields_data.append(("üìä Th·ªëng k√™", stats_field))
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'üì∞')
            title = news['title'][:50] + "..." if len(news['title']) > 50 else news['title']
            source_display = source_names.get(news['source'], news['source'])
            
            field_name = f"{i}. {emoji} {title}"
            field_value = f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üì∞ {source_display}\nüîó [ƒê·ªçc b√†i vi·∫øt]({news['link']})"
            
            fields_data.append((field_name, field_value))
        
        # Create embeds
        embeds = create_safe_embed_with_fields(
            f"üì∞ Tin t·ª©c (Trang {page})",
            "",
            fields_data,
            0x00ff88
        )
        
        save_user_news_enhanced(ctx.author.id, page_news, f"all_page_{page}")
        
        total_pages = (len(all_news) + items_per_page - 1) // items_per_page
        for i, embed in enumerate(embeds):
            embed.set_footer(text=f"Trang {page}/{total_pages} ‚Ä¢ !chitiet [s·ªë]")
        
        for embed in embeds:
            await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='out')
async def get_international_news_enhanced(ctx, page=1):
    """Tin t·ª©c qu·ªëc t·∫ø - Fixed Yahoo Finance URLs 2025"""
    try:
        page = max(1, int(page))
        loading_msg = await ctx.send(f"‚è≥ ƒêang t·∫£i...")
        
        news_list = await collect_news_enhanced(RSS_FEEDS['international'], 30)  # Reduced limit
        await loading_msg.delete()
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        # Prepare fields data
        fields_data = []
        
        stats_field = f"üì∞ International News: {len(news_list)} tin\nüî• 20+ RSS sources: CNN, Reuters, WSJ, Forbes, BBC v√† nhi·ªÅu h∆°n!\n‚úÖ URLs from GitHub verified 2025"
        fields_data.append(("üìä Th√¥ng tin", stats_field))
        
        # MASSIVE source names for international sources
        source_names = {
            # Yahoo Finance sources
            'yahoo_finance_main': 'Yahoo RSS', 'yahoo_finance_headlines': 'Yahoo Headlines',
            'yahoo_finance_rss': 'Yahoo Finance', 'yahoo_finance_crypto': 'Yahoo Crypto',
            'yahoo_finance_tech': 'Yahoo Tech', 'yahoo_finance_stock_market': 'Yahoo Stocks',
            
            # Major financial news sources
            'cnn_money': 'CNN Money', 'reuters_topnews': 'Reuters', 'marketwatch': 'MarketWatch',
            'business_insider': 'Business Insider', 'forbes': 'Forbes', 'wsj': 'Wall Street Journal',
            'cnbc': 'CNBC', 'investing_com': 'Investing.com', 'seekingalpha': 'Seeking Alpha',
            'financial_times': 'Financial Times', 'fortune': 'Fortune', 'economist': 'The Economist',
            'nasdaq': 'Nasdaq', 'washington_post_biz': 'Washington Post', 'guardian_business': 'The Guardian',
            'investopedia': 'Investopedia', 'nikkei_asia': 'Nikkei Asia', 'economic_times': 'Economic Times',
            'bbc_news': 'BBC News', 'coindesk': 'CoinDesk',
            'yahoo_finance_scraped': 'Yahoo Scraped'
        }
        
        emoji_map = {
            # Yahoo Finance sources
            'yahoo_finance_main': 'üíº', 'yahoo_finance_headlines': 'üì∞', 'yahoo_finance_rss': 'üíº',
            'yahoo_finance_crypto': 'üí∞', 'yahoo_finance_tech': 'üíª', 'yahoo_finance_stock_market': 'üìà',
            
            # Major financial news sources
            'cnn_money': 'üì∫', 'reuters_topnews': 'üåç', 'marketwatch': 'üìä', 'business_insider': 'üíº',
            'forbes': 'üíé', 'wsj': 'üì∞', 'cnbc': 'üì∫', 'investing_com': 'üíπ', 'seekingalpha': 'üîç',
            'financial_times': 'üìä', 'fortune': 'üí∞', 'economist': 'üéØ', 'nasdaq': 'üìà',
            'washington_post_biz': 'üì∞', 'guardian_business': 'üõ°Ô∏è', 'investopedia': 'üìö',
            'nikkei_asia': 'üåè', 'economic_times': 'üáÆüá≥', 'bbc_news': 'üá¨üáß', 'coindesk': '‚Çø',
            'yahoo_finance_scraped': 'üöÄ'
        }
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'üí∞')
            title = news['title'][:50] + "..." if len(news['title']) > 50 else news['title']
            source_display = source_names.get(news['source'], 'International Finance')
            
            field_name = f"{i}. {emoji} {title}"
            field_value = f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üì∞ {source_display}\nüîó [ƒê·ªçc b√†i vi·∫øt]({news['link']})"
            
            fields_data.append((field_name, field_value))
        
        # Create embeds
        embeds = create_safe_embed_with_fields(
            f"üåç Tin n∆∞·ªõc ngo√†i (Trang {page})",
            "",
            fields_data,
            0x0066ff
        )
        
        save_user_news_enhanced(ctx.author.id, page_news, f"out_page_{page}")
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        for i, embed in enumerate(embeds):
            embed.set_footer(text=f"Trang {page}/{total_pages} ‚Ä¢ !chitiet [s·ªë]")
        
        for embed in embeds:
            await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='in')
async def get_domestic_news_enhanced(ctx, page=1):
    """Tin t·ª©c trong n∆∞·ªõc - CafeF v·ªõi traditional extraction"""
    try:
        page = max(1, int(page))
        loading_msg = await ctx.send(f"‚è≥ ƒêang t·∫£i...")
        
        news_list = await collect_news_enhanced(RSS_FEEDS['domestic'], 20)  # Reduced limit
        await loading_msg.delete()
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        # Prepare fields data
        fields_data = []
        
        stats_field = f"üì∞ T·ªïng tin CafeF: {len(news_list)} tin\nüéØ Lƒ©nh v·ª±c: CK, BƒêS, TC, VM, DN"
        fields_data.append(("üìä Th√¥ng tin", stats_field))
        
        source_names = {
            'cafef_chungkhoan': 'CafeF CK', 'cafef_batdongsan': 'CafeF BƒêS',
            'cafef_taichinh': 'CafeF TC', 'cafef_vimo': 'CafeF VM', 'cafef_doanhnghiep': 'CafeF DN'
        }
        
        emoji_map = {
            'cafef_chungkhoan': 'üìà', 'cafef_batdongsan': 'üè¢', 
            'cafef_taichinh': 'üí∞', 'cafef_vimo': 'üìä', 'cafef_doanhnghiep': 'üè≠'
        }
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'üì∞')
            title = news['title'][:55] + "..." if len(news['title']) > 55 else news['title']
            source_display = source_names.get(news['source'], news['source'])
            
            field_name = f"{i}. {emoji} {title}"
            field_value = f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üì∞ {source_display}\nüîó [ƒê·ªçc b√†i vi·∫øt]({news['link']})"
            
            fields_data.append((field_name, field_value))
        
        # Create embeds
        embeds = create_safe_embed_with_fields(
            f"üáªüá≥ Tin trong n∆∞·ªõc (Trang {page})",
            "",
            fields_data,
            0xff0000
        )
        
        save_user_news_enhanced(ctx.author.id, page_news, f"in_page_{page}")
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        for i, embed in enumerate(embeds):
            embed.set_footer(text=f"Trang {page}/{total_pages} ‚Ä¢ !chitiet [s·ªë]")
        
        for embed in embeds:
            await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='chitiet')
async def get_news_detail_enhanced(ctx, news_number: int):
    """Chi ti·∫øt b√†i vi·∫øt - Gemini cho tin n∆∞·ªõc ngo√†i, traditional cho tin trong n∆∞·ªõc"""
    try:
        user_id = ctx.author.id
        
        if user_id not in user_news_cache:
            await ctx.send("‚ùå B·∫°n ch∆∞a xem tin t·ª©c! D√πng `!all`, `!in`, ho·∫∑c `!out` tr∆∞·ªõc.")
            return
        
        user_data = user_news_cache[user_id]
        news_list = user_data['news']
        
        if news_number < 1 or news_number > len(news_list):
            await ctx.send(f"‚ùå S·ªë kh√¥ng h·ª£p l·ªá! Ch·ªçn t·ª´ 1 ƒë·∫øn {len(news_list)}")
            return
        
        news = news_list[news_number - 1]
        
        # Save as last detail for !hoi context
        save_user_last_detail(user_id, news)
        
        # Determine extraction method based on source
        if is_international_source(news['source']):
            loading_msg = await ctx.send(f"‚è≥ ƒêang t·∫£i b·∫±ng Gemini AI cho {news['source']}...")
        else:
            loading_msg = await ctx.send(f"‚è≥ ƒêang t·∫£i...")
        
        # Enhanced content extraction - NOW USES GEMINI FOR ALL INTERNATIONAL
        full_content = await extract_content_enhanced(news['link'], news['source'], news)
        
        # Enhanced source names
        source_names = {
            'cafef_chungkhoan': 'CafeF Ch·ª©ng Kho√°n', 'cafef_batdongsan': 'CafeF B·∫•t ƒê·ªông S·∫£n',
            'cafef_taichinh': 'CafeF T√†i Ch√≠nh', 'cafef_vimo': 'CafeF Vƒ© M√¥', 'cafef_doanhnghiep': 'CafeF Doanh Nghi·ªáp',
            'yahoo_finance_main': 'Yahoo Finance RSS', 'yahoo_finance_headlines': 'Yahoo Headlines',
            'yahoo_finance_scraped': 'Yahoo Finance Scraped', 'marketwatch': 'MarketWatch',
            'reuters_topnews': 'Reuters', 'cnn_money': 'CNN Money', 'forbes': 'Forbes',
            'wsj': 'Wall Street Journal', 'cnbc': 'CNBC', 'bbc_news': 'BBC News'
        }
        
        source_name = source_names.get(news['source'], news['source'])
        
        await loading_msg.delete()
        
        # Create content with metadata
        main_title = f"üìñ Chi ti·∫øt tin {news_number}"
        
        # Simple metadata
        content_with_meta = f"**üì∞ {news['title']}**\n"
        content_with_meta += f"**üï∞Ô∏è {news['published_str']}** ‚Ä¢ **üì∞ {source_name}**\n\n"
        content_with_meta += f"{full_content}"
        
        # Create optimized embeds
        optimized_embeds = create_optimized_embeds(main_title, content_with_meta, 0x9932cc)
        
        # Add link to last embed
        if optimized_embeds:
            safe_name, safe_value = validate_embed_field(
                "üîó Link g·ªëc",
                f"[ƒê·ªçc b√†i vi·∫øt g·ªëc]({news['link']})"
            )
            optimized_embeds[-1].add_field(name=safe_name, value=safe_value, inline=False)
            
            optimized_embeds[-1].set_footer(text=f"Tin s·ªë {news_number}")
        
        # Send all embeds
        for i, embed in enumerate(optimized_embeds, 1):
            if i == 1:
                await ctx.send(embed=embed)
            else:
                await asyncio.sleep(0.5)
                await ctx.send(embed=embed)
        
    except ValueError:
        await ctx.send("‚ùå Vui l√≤ng nh·∫≠p s·ªë! V√≠ d·ª•: `!chitiet 5`")
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='hoi')
async def enhanced_gemini_question(ctx, *, question):
    """Enhanced Gemini AI v·ªõi context awareness"""
    try:
        if not gemini_engine.available:
            embed = create_safe_embed(
                "‚ö†Ô∏è Gemini AI kh√¥ng kh·∫£ d·ª•ng",
                "C·∫ßn Gemini API key ƒë·ªÉ ho·∫°t ƒë·ªông.",
                0xff6b6b
            )
            await ctx.send(embed=embed)
            return
        
        current_datetime_str = get_current_datetime_str()
        
        # Check if user has recent !chitiet context
        user_id = ctx.author.id
        context = ""
        context_info = ""
        
        if user_id in user_last_detail_cache:
            last_detail = user_last_detail_cache[user_id]
            # Check if accessed within last 30 minutes
            time_diff = get_current_vietnam_datetime() - last_detail['timestamp']
            
            if time_diff.total_seconds() < 1800:  # 30 minutes
                article = last_detail['article']
                
                # Extract content for context
                article_content = await extract_content_enhanced(article['link'], article['source'], article)
                
                if article_content:
                    context = f"B√ÄI B√ÅO LI√äN QUAN:\nTi√™u ƒë·ªÅ: {article['title']}\nNgu·ªìn: {article['source']}\nN·ªôi dung: {article_content[:1500]}"
                    context_info = f"üì∞ **Context:** B√†i b√°o v·ª´a xem"
        
        progress_embed = create_safe_embed(
            "ü§ñ Gemini AI",
            f"ƒêang ph√¢n t√≠ch: {question[:100]}...",
            0x9932cc
        )
        
        progress_msg = await ctx.send(embed=progress_embed)
        
        # Get Gemini response
        if context:
            # Article analysis mode
            analysis_result = await gemini_engine.analyze_article(context, question)
        else:
            # General question mode
            analysis_result = await gemini_engine.ask_question(question, context)
        
        # Create optimized embeds
        title = f"ü§ñ Gemini AI"
        optimized_embeds = create_optimized_embeds(title, analysis_result, 0x00ff88)
        
        # Simple footer
        if optimized_embeds:
            optimized_embeds[-1].set_footer(text=f"Gemini AI")
        
        # Send optimized embeds
        await progress_msg.edit(embed=optimized_embeds[0])
        
        for embed in optimized_embeds[1:]:
            await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói h·ªá th·ªëng Gemini: {str(e)}")

@bot.command(name='debate')
async def gemini_debate_system(ctx, *, topic=""):
    """Multi-perspective debate system v·ªõi Gemini"""
    try:
        if not gemini_engine.available:
            embed = create_safe_embed(
                "‚ö†Ô∏è Gemini AI kh√¥ng kh·∫£ d·ª•ng",
                "C·∫ßn Gemini API key ƒë·ªÉ ho·∫°t ƒë·ªông.",
                0xff6b6b
            )
            await ctx.send(embed=embed)
            return
        
        # Determine debate topic
        if not topic:
            # Use last !chitiet article if available
            user_id = ctx.author.id
            if user_id in user_last_detail_cache:
                last_detail = user_last_detail_cache[user_id]
                time_diff = get_current_vietnam_datetime() - last_detail['timestamp']
                
                if time_diff.total_seconds() < 1800:  # 30 minutes
                    article = last_detail['article']
                    topic = f"B√†i b√°o: {article['title']}"
                else:
                    await ctx.send("‚ùå Vui l√≤ng nh·∫≠p ch·ªß ƒë·ªÅ debate ho·∫∑c xem b√†i b√°o b·∫±ng !chitiet tr∆∞·ªõc.")
                    return
            else:
                await ctx.send("‚ùå Vui l√≤ng nh·∫≠p ch·ªß ƒë·ªÅ debate! V√≠ d·ª•: `!debate l·∫°m ph√°t hi·ªán t·∫°i`")
                return
        
        progress_embed = create_safe_embed(
            "üé≠ Gemini Debate",
            f"Ch·ªß ƒë·ªÅ: {topic[:100]}...",
            0xff9900
        )
        
        progress_msg = await ctx.send(embed=progress_embed)
        
        # Get debate analysis
        debate_result = await gemini_engine.debate_perspectives(topic)
        
        # Create optimized embeds
        title = f"üé≠ Debate"
        optimized_embeds = create_optimized_embeds(title, debate_result, 0xff6600)
        
        # Simple footer
        if optimized_embeds:
            optimized_embeds[-1].set_footer(text=f"Gemini Debate")
        
        # Send optimized embeds
        await progress_msg.edit(embed=optimized_embeds[0])
        
        for embed in optimized_embeds[1:]:
            await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói h·ªá th·ªëng debate: {str(e)}")

@bot.command(name='menu')
async def help_command_optimized(ctx):
    """Simple menu guide"""
    
    main_embed = create_safe_embed(
        "üì∞ News Bot - 20+ RSS Sources",
        "CafeF + CNN + Reuters + WSJ + Forbes + BBC + 15 more!",
        0x00ff88
    )
    
    safe_name1, safe_value1 = validate_embed_field(
        "üì∞ L·ªánh tin t·ª©c",
        "**!all [trang]** - T·∫•t c·∫£ tin t·ª©c\n**!in [trang]** - Tin trong n∆∞·ªõc\n**!out [trang]** - Tin n∆∞·ªõc ngo√†i\n**!chitiet [s·ªë]** - Chi ti·∫øt b√†i vi·∫øt"
    )
    main_embed.add_field(name=safe_name1, value=safe_value1, inline=False)
    
    safe_name2, safe_value2 = validate_embed_field(
        "ü§ñ L·ªánh AI",
        "**!hoi [c√¢u h·ªèi]** - H·ªèi AI\n**!debate [ch·ªß ƒë·ªÅ]** - Tranh lu·∫≠n"
    )
    main_embed.add_field(name=safe_name2, value=safe_value2, inline=False)
    
    await ctx.send(embed=main_embed)

# üÜï STATUS COMMAND
@bot.command(name='status')
async def status_command(ctx):
    """Hi·ªÉn th·ªã tr·∫°ng th√°i h·ªá th·ªëng"""
    
    # System statistics
    total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
    global_cache_size = len(global_seen_articles)
    
    main_embed = create_safe_embed(
        "üìä Tr·∫°ng th√°i h·ªá th·ªëng - 20+ RSS Sources",
        "",
        0x00ff88
    )
    
    safe_name1, safe_value1 = validate_embed_field(
        "üì∞ Ngu·ªìn tin",
        f"üáªüá≥ CafeF: {len(RSS_FEEDS['domestic'])}\nüåç International: {len(RSS_FEEDS['international'])}\nüìä T·ªïng: {total_sources}\nüî• 20+ RSS feeds t·ª´ GitHub sources!\n‚úÖ CNN, Reuters, WSJ, Forbes, BBC..."
    )
    main_embed.add_field(name=safe_name1, value=safe_value1, inline=True)
    
    gemini_status = "‚úÖ" if gemini_engine.available else "‚ùå"
    safe_name2, safe_value2 = validate_embed_field(
        "ü§ñ AI System",
        f"Gemini AI: {gemini_status}\nCache: {global_cache_size}\n‚ö° Optimized timeouts"
    )
    main_embed.add_field(name=safe_name2, value=safe_value2, inline=True)
    
    await ctx.send(embed=main_embed)

# Run the bot
if __name__ == "__main__":
    try:
        keep_alive()
        print("üåê Keep-alive server started")
        
        total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
        
        print("üöÄ Starting MASSIVE RSS News Bot...")
        print(f"üîß Sources: {total_sources} (20+ RSS feeds)")
        print(f"ü§ñ Gemini: {'‚úÖ' if gemini_engine.available else '‚ùå'}")
        print("üî• MASSIVE RSS collection from GitHub sources")
        print("üì∞ CNN, Reuters, WSJ, Forbes, BBC, CNBC + more!")
        print("‚ö° Optimized timeouts and limits")
        print("=" * 40)
        
        bot.run(TOKEN)
        
    except Exception as e:
        print(f"‚ùå STARTUP ERROR: {e}")
