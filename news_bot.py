import discord
from discord.ext import commands
import feedparser
import asyncio
import os
import re
from datetime import datetime, timedelta
import calendar
from urllib.parse import urljoin, urlparse, quote
import html
import chardet
import pytz
import json
import aiohttp
from keep_alive import keep_alive
from enum import Enum
from typing import List, Dict, Tuple, Optional
import random
import hashlib

# üöÄ OPTIMIZED LIBRARIES - Enhanced for async operations
try:
    import trafilatura
    TRAFILATURA_AVAILABLE = True
except ImportError:
    TRAFILATURA_AVAILABLE = False

try:
    import newspaper
    from newspaper import Article
    NEWSPAPER_AVAILABLE = True
except ImportError:
    NEWSPAPER_AVAILABLE = False

try:
    from bs4 import BeautifulSoup
    BEAUTIFULSOUP_AVAILABLE = True
except ImportError:
    BEAUTIFULSOUP_AVAILABLE = False

# üÜï GEMINI ONLY - Enhanced AI System with Direct Content Access
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

# Bot configuration
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix='!', intents=intents)

# üîí ENVIRONMENT VARIABLES
TOKEN = os.getenv('DISCORD_TOKEN')
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

# üîß TIMEZONE - Vietnam
VN_TIMEZONE = pytz.timezone('Asia/Ho_Chi_Minh')
UTC_TIMEZONE = pytz.UTC

# üîß DISCORD LIMITS
DISCORD_EMBED_FIELD_VALUE_LIMIT = 1000
DISCORD_EMBED_DESCRIPTION_LIMIT = 4000
DISCORD_EMBED_TITLE_LIMIT = 250
DISCORD_EMBED_TOTAL_EMBED_LIMIT = 5800

# User cache with deduplication
user_news_cache = {}
user_last_detail_cache = {}
global_seen_articles = {}  # Global deduplication cache
MAX_CACHE_ENTRIES = 25
MAX_GLOBAL_CACHE = 1000

# üîß Enhanced User Agents for better compatibility
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

def get_current_vietnam_datetime():
    """Get current Vietnam date and time"""
    return datetime.now(VN_TIMEZONE)

def get_current_date_str():
    """Get current date string in Vietnam format"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%d/%m/%Y")

def get_current_time_str():
    """Get current time string in Vietnam format"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%H:%M")

def get_current_datetime_str():
    """Get current datetime string for display"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%H:%M %d/%m/%Y")

print("üöÄ News Bot:")
print(f"Token: {'‚úÖ' if TOKEN else '‚ùå'}")
print(f"Gemini: {'‚úÖ' if GEMINI_API_KEY else '‚ùå'}")
print("=" * 20)

if not TOKEN:
    print("‚ùå CRITICAL: DISCORD_TOKEN not found!")
    exit(1)

# üîß FREE RSS FEEDS ONLY - Removed ALL Paywall Sources 2025
RSS_FEEDS = {
    # === KINH T·∫æ TRONG N∆Ø·ªöC - CH·ªà CAFEF ===
    'domestic': {
        'cafef_chungkhoan': 'https://cafef.vn/thi-truong-chung-khoan.rss',
        'cafef_batdongsan': 'https://cafef.vn/bat-dong-san.rss',
        'cafef_taichinh': 'https://cafef.vn/tai-chinh-ngan-hang.rss',
        'cafef_vimo': 'https://cafef.vn/vi-mo-dau-tu.rss',
        'cafef_doanhnghiep': 'https://cafef.vn/doanh-nghiep.rss'
    },
    
    # === QU·ªêC T·∫æ - ONLY FREE RSS SOURCES (NO PAYWALL) ===
    'international': {
        # ‚úÖ YAHOO FINANCE RSS (100% Free)
        'yahoo_finance_main': 'https://finance.yahoo.com/news/rssindex',
        'yahoo_finance_headlines': 'https://feeds.finance.yahoo.com/rss/2.0/headline',
        'yahoo_finance_rss': 'https://www.yahoo.com/news/rss/finance',
        
        # ‚úÖ FREE NEWS RSS FEEDS (NO PAYWALL - Verified 2025)
        'cnn_money': 'http://rss.cnn.com/rss/money_topstories.rss',
        'marketwatch': 'http://feeds.marketwatch.com/marketwatch/topstories/',
        'business_insider': 'http://feeds2.feedburner.com/businessinsider',
        'cnbc': 'https://www.cnbc.com/id/100003114/device/rss/rss.html',
        'investing_com': 'https://www.investing.com/rss/news.rss',
        'investopedia': 'https://www.investopedia.com/feedbuilder/feed/getfeed/?feedName=rss_headline',
        'bbc_business': 'http://feeds.bbci.co.uk/news/business/rss.xml',
        'guardian_business': 'https://www.theguardian.com/business/economics/rss',
        'coindesk': 'https://feeds.feedburner.com/CoinDesk',
        'nasdaq_news': 'http://articlefeeds.nasdaq.com/nasdaq/categories?category=Investing+Ideas',
        
        # ‚úÖ FREE ALTERNATIVE SOURCES (Removed problematic ones)
        'seeking_alpha': 'https://seekingalpha.com/feed.xml',
        'benzinga': 'https://www.benzinga.com/feed',
    }
}

def convert_utc_to_vietnam_time(utc_time_tuple):
    """Convert UTC to Vietnam time"""
    try:
        utc_timestamp = calendar.timegm(utc_time_tuple)
        utc_dt = datetime.fromtimestamp(utc_timestamp, tz=UTC_TIMEZONE)
        vn_dt = utc_dt.astimezone(VN_TIMEZONE)
        return vn_dt
    except Exception as e:
        return datetime.now(VN_TIMEZONE)

# üÜï ENHANCED DEDUPLICATION SYSTEM
def generate_article_hash(title, link, description=""):
    """Generate unique hash for article deduplication"""
    clean_title = re.sub(r'[^\w\s]', '', title.lower().strip())
    clean_link = link.lower().strip()
    content = f"{clean_title}|{clean_link}"
    return hashlib.md5(content.encode('utf-8')).hexdigest()

def clean_expired_cache():
    """Clean expired articles from global cache"""
    global global_seen_articles
    current_time = get_current_vietnam_datetime()
    expired_hashes = []
    
    for article_hash, article_data in global_seen_articles.items():
        time_diff = current_time - article_data['timestamp']
        if time_diff.total_seconds() > (CACHE_EXPIRE_HOURS * 3600):
            expired_hashes.append(article_hash)
    
    for expired_hash in expired_hashes:
        del global_seen_articles[expired_hash]
    
    if expired_hashes:
        print(f"üßπ Cleaned {len(expired_hashes)} expired articles from cache")

def normalize_title(title):
    """Normalize title for exact comparison"""
    import re
    # Convert to lowercase and remove extra spaces
    normalized = re.sub(r'\s+', ' ', title.lower().strip())
    # Remove common punctuation that might vary
    normalized = re.sub(r'[.,!?;:\-\u2013\u2014]', '', normalized)
    # Remove quotes that might vary
    normalized = re.sub(r'["\'\u201c\u201d\u2018\u2019]', '', normalized)
    return normalized

def is_duplicate_article_local(news_item, existing_articles):
    """Check duplicate within current collection - EXACT TITLE MATCH ONLY"""
    current_title = normalize_title(news_item['title'])
    current_link = news_item['link'].lower().strip()
    
    for existing in existing_articles:
        existing_title = normalize_title(existing['title'])
        existing_link = existing['link'].lower().strip()
        
        # Check exact title match OR exact link match
        if current_title == existing_title or current_link == existing_link:
            return True
    
    return False

def is_duplicate_article_global(news_item, source_name):
    """Check duplicate against global cache - EXACT TITLE MATCH ONLY"""
    global global_seen_articles
    
    try:
        # Clean expired cache first
        clean_expired_cache()
        
        current_title = normalize_title(news_item['title'])
        current_link = news_item['link'].lower().strip()
        
        # Check against global cache - EXACT matches only
        for existing_data in global_seen_articles.values():
            existing_title = normalize_title(existing_data['title'])
            existing_link = existing_data['link'].lower().strip()
            
            if current_title == existing_title or current_link == existing_link:
                return True
        
        # Add to global cache using simple key
        cache_key = f"{current_title}|{current_link}"
        
        global_seen_articles[cache_key] = {
            'title': news_item['title'],
            'link': news_item['link'],
            'source': source_name,
            'timestamp': get_current_vietnam_datetime()
        }
        
        # Limit cache size
        if len(global_seen_articles) > MAX_GLOBAL_CACHE:
            sorted_items = sorted(global_seen_articles.items(), key=lambda x: x[1]['timestamp'])
            for old_key, _ in sorted_items[:100]:
                del global_seen_articles[old_key]
        
        return False
        
    except Exception as e:
        print(f"‚ö†Ô∏è Global duplicate check error: {e}")
        return False

# üîß CONTENT VALIDATION FOR DISCORD
def validate_and_truncate_content(content: str, limit: int, suffix: str = "...") -> str:
    """Strict validation and truncation for Discord limits"""
    if not content:
        return "Kh√¥ng c√≥ n·ªôi dung."
    
    content = str(content).strip()
    safe_limit = max(limit - 50, 100)
    
    if len(content) <= safe_limit:
        return content
    
    available_space = safe_limit - len(suffix)
    if available_space <= 0:
        return suffix[:safe_limit]
    
    truncated = content[:available_space].rstrip()
    last_sentence = truncated.rfind('. ')
    if last_sentence > available_space * 0.7:
        truncated = truncated[:last_sentence + 1]
    
    return truncated + suffix

def validate_embed_field(name: str, value: str) -> Tuple[str, str]:
    """Strict embed field validation for Discord limits"""
    safe_name = validate_and_truncate_content(name, DISCORD_EMBED_TITLE_LIMIT, "...")
    safe_value = validate_and_truncate_content(value, DISCORD_EMBED_FIELD_VALUE_LIMIT, "...")
    
    if not safe_value or safe_value == "...":
        safe_value = "N·ªôi dung kh√¥ng kh·∫£ d·ª•ng."
    
    return safe_name, safe_value

def create_safe_embed(title: str, description: str = "", color: int = 0x00ff88) -> discord.Embed:
    """Create safe embed that fits Discord limits"""
    safe_title = validate_and_truncate_content(title, DISCORD_EMBED_TITLE_LIMIT, "...")
    safe_description = validate_and_truncate_content(description, DISCORD_EMBED_DESCRIPTION_LIMIT, "...")
    
    return discord.Embed(
        title=safe_title,
        description=safe_description,
        color=color,
        timestamp=get_current_vietnam_datetime()
    )

# üîß ASYNC-FIRST APPROACHES - NO MORE BLOCKING
async def async_sleep_delay():
    """FIXED: Use asyncio.sleep instead of time.sleep to prevent heartbeat blocking"""
    delay = random.uniform(0.1, 0.5)  # Much shorter delay
    await asyncio.sleep(delay)

def get_enhanced_headers(url=None):
    """Enhanced headers for better compatibility - NO BROTLI"""
    user_agent = random.choice(USER_AGENTS)
    
    headers = {
        'User-Agent': user_agent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9,vi;q=0.8',
        'Accept-Encoding': 'gzip, deflate',  # Removed 'br' to avoid brotli errors
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'DNT': '1',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache',
    }
    
    if url and 'yahoo' in url.lower():
        headers.update({
            'Referer': 'https://finance.yahoo.com/',
            'Origin': 'https://finance.yahoo.com',
        })
    elif url and 'cafef.vn' in url.lower():
        headers.update({
            'Referer': 'https://cafef.vn/',
            'Origin': 'https://cafef.vn'
        })
    
    return headers

def is_international_source(source_name):
    """Check if source is international - FIXED for removed sources"""
    international_sources = [
        'yahoo_finance', 'cnn_money', 'marketwatch', 'business_insider',
        'cnbc', 'investing_com', 'investopedia', 'bbc_business',
        'guardian_business', 'coindesk', 'nasdaq_news', 'seeking_alpha', 'benzinga'
    ]
    return any(source in source_name for source in international_sources)

def create_fallback_content(url, source_name, error_msg=""):
    """Create fallback content when extraction fails"""
    try:
        article_id = url.split('/')[-1] if '/' in url else 'news-article'
        
        if is_international_source(source_name):
            source_display = "Financial News"
            if 'marketwatch' in source_name:
                source_display = "MarketWatch"
            elif 'reuters' in source_name:
                source_display = "Reuters"
            elif 'cnn' in source_name:
                source_display = "CNN Money"
            elif 'cnbc' in source_name:
                source_display = "CNBC"
            elif 'bbc' in source_name:
                source_display = "BBC Business"
            
            return f"""**{source_display} Financial News:**

üìà **Market Analysis:** This article provides financial market insights and economic analysis.

üìä **Coverage Areas:**
‚Ä¢ Real-time market data and analysis
‚Ä¢ Economic indicators and trends
‚Ä¢ Corporate earnings and reports
‚Ä¢ Investment strategies and forecasts

**Article ID:** {article_id}
**Note:** Content extraction failed. Please visit the original link for complete article.

{f'**Technical Error:** {error_msg}' if error_msg else ''}"""
        else:
            return f"""**Tin t·ª©c kinh t·∫ø CafeF:**

üì∞ **Th√¥ng tin kinh t·∫ø:** B√†i vi·∫øt cung c·∫•p th√¥ng tin kinh t·∫ø, t√†i ch√≠nh t·ª´ CafeF.

üìä **N·ªôi dung chuy√™n s√¢u:**
‚Ä¢ Ph√¢n t√≠ch th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam
‚Ä¢ Tin t·ª©c kinh t·∫ø vƒ© m√¥ v√† ch√≠nh s√°ch
‚Ä¢ B√°o c√°o doanh nghi·ªáp v√† t√†i ch√≠nh
‚Ä¢ B·∫•t ƒë·ªông s·∫£n v√† ƒë·∫ßu t∆∞

**M√£ b√†i vi·∫øt:** {article_id}
**L∆∞u √Ω:** ƒê·ªÉ ƒë·ªçc ƒë·∫ßy ƒë·ªß, vui l√≤ng truy c·∫≠p link g·ªëc.

{f'**L·ªói:** {error_msg}' if error_msg else ''}"""
        
    except Exception as e:
        return f"N·ªôi dung t·ª´ {source_name}. Vui l√≤ng truy c·∫≠p link g·ªëc ƒë·ªÉ ƒë·ªçc ƒë·∫ßy ƒë·ªß."

async def extract_content_with_gemini(url, source_name):
    """Use Gemini to extract and translate content from international news"""
    try:
        if not GEMINI_API_KEY or not GEMINI_AVAILABLE:
            return create_fallback_content(url, source_name, "Gemini kh√¥ng kh·∫£ d·ª•ng")
        
        extraction_prompt = f"""Truy c·∫≠p v√† tr√≠ch xu·∫•t TO√ÄN B·ªò n·ªôi dung b√†i b√°o t·ª´: {url}

Y√äU C·∫¶U:
1. ƒê·ªçc v√† hi·ªÉu HO√ÄN TO√ÄN b√†i b√°o
2. Tr√≠ch xu·∫•t T·∫§T C·∫¢ n·ªôi dung ch√≠nh (lo·∫°i b·ªè qu·∫£ng c√°o, sidebar)
3. D·ªãch t·ª´ ti·∫øng Anh sang ti·∫øng Vi·ªát T·ª∞ NHI√äN
4. Gi·ªØ nguy√™n s·ªë li·ªáu, t√™n c√¥ng ty, thu·∫≠t ng·ªØ t√†i ch√≠nh
5. ƒê·ªô d√†i: 500-1500 t·ª´ (to√†n b·ªô b√†i vi·∫øt)
6. CH·ªà tr·∫£ v·ªÅ n·ªôi dung b√†i b√°o ƒë√£ d·ªãch

**N·ªòI DUNG HO√ÄN CH·ªàNH:**"""

        try:
            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.1,
                top_p=0.8,
                max_output_tokens=3000,  # TƒÉng t·ª´ 2000 ƒë·ªÉ l·∫•y to√†n b·ªô n·ªôi dung
            )
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    model.generate_content,
                    extraction_prompt,
                    generation_config=generation_config
                ),
                timeout=30  # TƒÉng timeout t·ª´ 20s
            )
            
            extracted_content = response.text.strip()
            
            if len(extracted_content) > 300:
                error_indicators = [
                    'cannot access', 'unable to access', 'kh√¥ng th·ªÉ truy c·∫≠p',
                    'failed to retrieve', 'error occurred', 'sorry, i cannot'
                ]
                
                if not any(indicator in extracted_content.lower() for indicator in error_indicators):
                    return f"[ü§ñ Gemini AI - To√†n b·ªô n·ªôi dung t·ª´ {source_name}]\n\n{extracted_content}"
                else:
                    return create_fallback_content(url, source_name, "Gemini kh√¥ng th·ªÉ tr√≠ch xu·∫•t")
            else:
                return create_fallback_content(url, source_name, "N·ªôi dung qu√° ng·∫Øn")
            
        except asyncio.TimeoutError:
            return create_fallback_content(url, source_name, "Gemini timeout")
        except Exception as e:
            return create_fallback_content(url, source_name, f"L·ªói Gemini: {str(e)}")
            
    except Exception as e:
        return create_fallback_content(url, source_name, str(e))

# üöÄ ASYNC HTTP CLIENT - NO MORE BLOCKING REQUESTS
async def fetch_with_aiohttp(url, headers=None, timeout=8):
    """FIXED: Use aiohttp instead of requests to prevent blocking"""
    try:
        if headers is None:
            headers = get_enhanced_headers(url)
        
        timeout_config = aiohttp.ClientTimeout(total=timeout)
        
        async with aiohttp.ClientSession(timeout=timeout_config, headers=headers) as session:
            async with session.get(url) as response:
                if response.status == 200:
                    content = await response.read()
                    return content
                else:
                    return None
    except Exception as e:
        print(f"‚ùå aiohttp fetch error for {url}: {e}")
        return None

# üöÄ ASYNC CONTENT EXTRACTION - Non-blocking
async def extract_content_enhanced(url, source_name, news_item=None):
    """Enhanced content extraction - Gemini for international, traditional for domestic"""
    
    # For international sources, use Gemini
    if is_international_source(source_name):
        print(f"ü§ñ Using Gemini for international source: {source_name}")
        return await extract_content_with_gemini(url, source_name)
    
    # For domestic (CafeF) sources, use traditional async methods
    try:
        print(f"üîß Using async traditional methods for domestic source: {source_name}")
        await async_sleep_delay()
        
        content = await fetch_with_aiohttp(url)
        
        if content:
            # Method 1: Trafilatura with enhanced config for full content
            if TRAFILATURA_AVAILABLE:
                try:
                    result = await asyncio.to_thread(
                        trafilatura.bare_extraction,
                        content,
                        include_comments=False,
                        include_tables=True,
                        include_links=False,
                        include_images=False,
                        favor_precision=False,  # Changed to False for more content
                        favor_recall=True,      # Added for maximum content
                        with_metadata=True,
                        prune_xpath=[],         # Don't prune anything
                        only_with_metadata=False
                    )
                    
                    if result and result.get('text') and len(result['text']) > 200:
                        full_text = result['text']
                        
                        # Try to get more content with different settings
                        if len(full_text) < 1000:
                            result2 = await asyncio.to_thread(
                                trafilatura.extract,
                                content,
                                include_comments=True,
                                include_tables=True,
                                include_links=True,
                                favor_precision=False,
                                favor_recall=True
                            )
                            if result2 and len(result2) > len(full_text):
                                full_text = result2
                        
                        return full_text.strip()
                except Exception as e:
                    print(f"‚ö†Ô∏è Trafilatura failed: {e}")
            
            # Method 2: Enhanced BeautifulSoup with multiple strategies
            if BEAUTIFULSOUP_AVAILABLE:
                try:
                    soup = await asyncio.to_thread(BeautifulSoup, content, 'html.parser')
                    
                    # Strategy 1: CafeF specific selectors
                    content_selectors = [
                        'div.detail-content',
                        'div.fck_detail', 
                        'div.content-detail',
                        'div.article-content',
                        'div.entry-content',
                        'div.post-content',
                        'article',
                        'main',
                        '.article-body',
                        '.content-body',
                        '.post-body'
                    ]
                    
                    extracted_text = ""
                    for selector in content_selectors:
                        elements = soup.select(selector)
                        if elements:
                            for element in elements:
                                text = element.get_text(strip=True)
                                if len(text) > len(extracted_text):
                                    extracted_text = text
                    
                    # Strategy 2: Find all paragraphs and combine
                    if len(extracted_text) < 500:
                        all_paragraphs = soup.find_all('p')
                        paragraph_texts = []
                        for p in all_paragraphs:
                            p_text = p.get_text(strip=True)
                            if len(p_text) > 50:  # Only substantial paragraphs
                                paragraph_texts.append(p_text)
                        
                        combined_text = '\n\n'.join(paragraph_texts)
                        if len(combined_text) > len(extracted_text):
                            extracted_text = combined_text
                    
                    if extracted_text and len(extracted_text) > 300:
                        cleaned_content = clean_content_enhanced(extracted_text)
                        return cleaned_content.strip()
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è BeautifulSoup failed: {e}")
            
            # Method 3: Newspaper3k fallback
            if NEWSPAPER_AVAILABLE:
                try:
                    from newspaper import Article
                    article = Article(url)
                    article.set_config({
                        'headers': get_enhanced_headers(url),
                        'timeout': 12
                    })
                    
                    article.download()
                    article.parse()
                    
                    if article.text and len(article.text) > 300:
                        return article.text.strip()
                
                except Exception as e:
                    print(f"‚ö†Ô∏è Newspaper3k failed: {e}")
        
        print(f"‚ö†Ô∏è All traditional methods failed for {source_name}")
        return create_fallback_content(url, source_name, "Traditional extraction methods failed")
        
    except Exception as e:
        print(f"‚ùå Extract content error for {source_name}: {e}")
        return create_fallback_content(url, source_name, str(e))

def clean_content_enhanced(content):
    """Enhanced content cleaning for CafeF"""
    if not content:
        return content
    
    unwanted_patterns = [
        r'Theo.*?CafeF.*?',
        r'Ngu·ªìn.*?:.*?',
        r'Tags:.*?$',
        r'T·ª´ kh√≥a:.*?$',
        r'ƒêƒÉng k√Ω.*?nh·∫≠n tin.*?',
        r'Like.*?Fanpage.*?',
        r'Follow.*?us.*?'
    ]
    
    for pattern in unwanted_patterns:
        content = re.sub(pattern, '', content, flags=re.IGNORECASE | re.DOTALL)
    
    content = re.sub(r'\s+', ' ', content)
    content = re.sub(r'\n\s*\n', '\n', content)
    
    return content.strip()

# üöÄ ASYNC NEWS COLLECTION - Fully non-blocking
async def collect_news_enhanced(sources_dict, limit_per_source=15, use_global_dedup=False):
    """Session-based collection with EXACT TITLE duplicate detection"""
    all_news = []
    
    print(f"üîÑ Starting collection from {len(sources_dict)} sources (Global dedup: {use_global_dedup})")
    print(f"üéØ Duplicate logic: EXACT title match only")
    
    # Clean expired cache before starting
    if use_global_dedup:
        clean_expired_cache()
    
    # Create tasks for concurrent processing
    tasks = []
    for source_name, source_url in sources_dict.items():
        task = process_single_source(source_name, source_url, limit_per_source)
        tasks.append(task)
    
    # Process all sources concurrently
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Collect results with exact title duplicate detection
    total_processed = 0
    local_duplicates = 0
    global_duplicates = 0
    
    for result in results:
        if isinstance(result, Exception):
            print(f"‚ùå Source processing error: {result}")
        elif result:
            for news_item in result:
                total_processed += 1
                
                # Local duplicate check (exact title/link match within current collection)
                if is_duplicate_article_local(news_item, all_news):
                    local_duplicates += 1
                    print(f"üîÑ Local duplicate: {news_item['title'][:50]}...")
                    continue
                
                # Global duplicate check (exact title/link match cross-session) - only if enabled
                if use_global_dedup and is_duplicate_article_global(news_item, news_item['source']):
                    global_duplicates += 1
                    print(f"üåç Global duplicate: {news_item['title'][:50]}...")
                    continue
                
                # Add unique article
                all_news.append(news_item)
    
    unique_count = len(all_news)
    print(f"üìä Processed: {total_processed}, Local dups: {local_duplicates}, Global dups: {global_duplicates}, Unique: {unique_count}")
    
    # Sort by publish time
    all_news.sort(key=lambda x: x['published'], reverse=True)
    return all_news

async def process_single_source(source_name, source_url, limit_per_source):
    """Process a single RSS source asynchronously"""
    try:
        print(f"üîÑ Processing {source_name}: {source_url}")
        
        if source_url.endswith('.rss') or 'rss' in source_url.lower() or 'feeds.' in source_url:
            # RSS Feed processing
            return await process_rss_feed_async(source_name, source_url, limit_per_source)
        else:
            # For future expansion - direct scraping
            return []
            
    except Exception as e:
        print(f"‚ùå Error for {source_name}: {e}")
        return []

async def process_rss_feed_async(source_name, rss_url, limit_per_source):
    """FIXED: Async RSS feed processing to prevent blocking"""
    try:
        await async_sleep_delay()
        
        # Use aiohttp instead of requests
        content = await fetch_with_aiohttp(rss_url)
        
        if content:
            # Parse feedparser in thread to avoid blocking
            feed = await asyncio.to_thread(feedparser.parse, content)
        else:
            # Fallback to direct feedparser
            feed = await asyncio.to_thread(feedparser.parse, rss_url)
        
        if not feed or not hasattr(feed, 'entries') or len(feed.entries) == 0:
            return []
        
        news_items = []
        for entry in feed.entries[:limit_per_source]:
            try:
                vn_time = get_current_vietnam_datetime()
                
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    vn_time = convert_utc_to_vietnam_time(entry.published_parsed)
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    vn_time = convert_utc_to_vietnam_time(entry.updated_parsed)
                
                description = ""
                if hasattr(entry, 'summary'):
                    description = entry.summary[:400] + "..." if len(entry.summary) > 400 else entry.summary
                elif hasattr(entry, 'description'):
                    description = entry.description[:400] + "..." if len(entry.description) > 400 else entry.description
                
                if hasattr(entry, 'title') and hasattr(entry, 'link'):
                    title = entry.title.strip()
                    
                    # Filter for relevant economic/financial content
                    if is_relevant_news(title, description, source_name):
                        news_item = {
                            'title': html.unescape(title),
                            'link': entry.link,
                            'source': source_name,
                            'published': vn_time,
                            'published_str': vn_time.strftime("%H:%M %d/%m"),
                            'description': html.unescape(description) if description else ""
                        }
                        news_items.append(news_item)
                
            except Exception as entry_error:
                continue
        
        print(f"‚úÖ Processed {len(news_items)} articles from {source_name}")
        return news_items
        
    except Exception as e:
        print(f"‚ùå RSS processing error for {source_name}: {e}")
        return []

def is_relevant_news(title, description, source_name):
    """Filter for relevant economic/financial news - MORE RELAXED"""
    # For CafeF sources, all content is relevant
    if 'cafef' in source_name:
        return True
    
    # For international sources, use very relaxed filter
    financial_keywords = [
        'stock', 'market', 'trading', 'investment', 'economy', 'economic',
        'bitcoin', 'crypto', 'currency', 'bank', 'financial', 'finance',
        'earnings', 'revenue', 'profit', 'inflation', 'fed', 'gdp',
        'business', 'company', 'corporate', 'industry', 'sector',
        'money', 'cash', 'capital', 'fund', 'price', 'cost', 'value',
        'growth', 'analyst', 'forecast', 'report', 'data', 'sales'
    ]
    
    title_lower = title.lower()
    description_lower = description.lower() if description else ""
    
    # Check in both title and description
    for keyword in financial_keywords:
        if keyword in title_lower or keyword in description_lower:
            return True
    
    # If no keywords found, still include (very relaxed)
    return True  # Changed from False to True - accept all articles

def get_or_collect_user_news(user_id, command_type, sources_dict, limit_per_source=15):
    """Get cached news or collect new ones for consistent pagination"""
    global user_news_cache
    
    # Check if user has recent cached news for this command
    if user_id in user_news_cache:
        cached_data = user_news_cache[user_id]
        time_diff = get_current_vietnam_datetime() - cached_data['timestamp']
        
        # If cache is recent (< 10 minutes) and same command type, use it
        if time_diff.total_seconds() < 600 and cached_data['command'].startswith(command_type):
            print(f"üì± Using cached news for user {user_id}: {len(cached_data['news'])} articles")
            return cached_data['news'], True  # True = from cache
    
    print(f"üîÑ Need to collect fresh news for user {user_id}")
    return [], False  # False = need to collect

async def collect_and_cache_news(user_id, command_type, sources_dict, limit_per_source=15):
    """Collect fresh news and cache for user"""
    # Collect with minimal global deduplication for fresh session
    all_news = await collect_news_enhanced(sources_dict, limit_per_source, use_global_dedup=False)
    
    # Cache for user
    save_user_news_enhanced(user_id, all_news, command_type)
    
    return all_news

def save_user_news_enhanced(user_id, news_list, command_type, current_page=1):
    """Enhanced user news saving with pagination info"""
    global user_news_cache
    
    user_news_cache[user_id] = {
        'news': news_list,  # Full news list
        'command': command_type,
        'current_page': current_page,
        'timestamp': get_current_vietnam_datetime()
    }
    
    if len(user_news_cache) > MAX_CACHE_ENTRIES:
        oldest_users = sorted(user_news_cache.items(), key=lambda x: x[1]['timestamp'])[:10]
        for user_id_to_remove, _ in oldest_users:
            del user_news_cache[user_id_to_remove]

def save_user_page_context(user_id, page_news, page_number):
    """Save current page context for !chitiet"""
    global user_news_cache
    
    if user_id in user_news_cache:
        user_news_cache[user_id]['current_page'] = page_number
        user_news_cache[user_id]['current_page_news'] = page_news  # Current page's 12 items

def save_user_last_detail(user_id, news_item):
    """Save last article accessed via !chitiet"""
    global user_last_detail_cache
    
    user_last_detail_cache[user_id] = {
        'article': news_item,
        'timestamp': get_current_vietnam_datetime()
    }

# üîß DISCORD EMBED HELPERS
def split_text_for_discord(text: str, max_length: int = 950) -> List[str]:
    """Split text to fit Discord field limits"""
    if len(text) <= max_length:
        return [text]
    
    parts = []
    current_part = ""
    
    sentences = text.split('. ')
    
    for sentence in sentences:
        if len(current_part + sentence + '. ') <= max_length:
            current_part += sentence + '. '
        else:
            if current_part:
                parts.append(current_part.strip())
                current_part = sentence + '. '
            else:
                parts.append(sentence[:max_length])
                current_part = ""
    
    if current_part:
        parts.append(current_part.strip())
    
    return parts

def create_optimized_embeds(title: str, content: str, color: int = 0x9932cc) -> List[discord.Embed]:
    """Create optimized embeds for Discord limits"""
    embeds = []
    
    content_parts = split_text_for_discord(content, 950)
    
    for i, part in enumerate(content_parts):
        if i == 0:
            embed = discord.Embed(
                title=validate_and_truncate_content(title, DISCORD_EMBED_TITLE_LIMIT),
                color=color,
                timestamp=get_current_vietnam_datetime()
            )
        else:
            embed = discord.Embed(
                title=validate_and_truncate_content(f"{title[:150]}... (Ph·∫ßn {i+1})", DISCORD_EMBED_TITLE_LIMIT),
                color=color,
                timestamp=get_current_vietnam_datetime()
            )
        
        field_name = f"üìÑ N·ªôi dung {f'(Ph·∫ßn {i+1})' if len(content_parts) > 1 else ''}"
        safe_field_name, safe_field_value = validate_embed_field(field_name, part)
        
        embed.add_field(
            name=safe_field_name,
            value=safe_field_value,
            inline=False
        )
        
        embeds.append(embed)
    
    return embeds

def create_safe_embed_with_fields(title: str, description: str, fields_data: List[Tuple[str, str]], color: int = 0x00ff88) -> List[discord.Embed]:
    """Create safe embeds with multiple fields"""
    embeds = []
    
    safe_title = validate_and_truncate_content(title, DISCORD_EMBED_TITLE_LIMIT, "...")
    safe_description = validate_and_truncate_content(description, DISCORD_EMBED_DESCRIPTION_LIMIT, "...")
    
    main_embed = discord.Embed(
        title=safe_title,
        description=safe_description,
        color=color,
        timestamp=get_current_vietnam_datetime()
    )
    
    fields_added = 0
    current_embed = main_embed
    total_chars = len(safe_title) + len(safe_description)
    
    for field_name, field_value in fields_data:
        safe_name, safe_value = validate_embed_field(field_name, field_value)
        
        field_chars = len(safe_name) + len(safe_value)
        
        if fields_added >= 20 or total_chars + field_chars > DISCORD_EMBED_TOTAL_EMBED_LIMIT:
            embeds.append(current_embed)
            current_embed = discord.Embed(
                title=validate_and_truncate_content(f"{safe_title[:180]}... (ti·∫øp theo)", DISCORD_EMBED_TITLE_LIMIT),
                color=color,
                timestamp=get_current_vietnam_datetime()
            )
            fields_added = 0
            total_chars = len(current_embed.title or "")
        
        current_embed.add_field(name=safe_name, value=safe_value, inline=False)
        fields_added += 1
        total_chars += field_chars
    
    embeds.append(current_embed)
    
    return embeds

# üÜï GEMINI AI SYSTEM
class GeminiAIEngine:
    def __init__(self):
        self.available = GEMINI_AVAILABLE and GEMINI_API_KEY
        if self.available:
            genai.configure(api_key=GEMINI_API_KEY)
    
    async def ask_question(self, question: str, context: str = ""):
        """Gemini AI question answering with context"""
        if not self.available:
            return "‚ö†Ô∏è Gemini AI kh√¥ng kh·∫£ d·ª•ng."
        
        try:
            current_date_str = get_current_date_str()
            
            prompt = f"""B·∫°n l√† Gemini AI - chuy√™n gia kinh t·∫ø t√†i ch√≠nh th√¥ng minh. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ki·∫øn th·ª©c chuy√™n m√¥n c·ªßa b·∫°n.

C√ÇU H·ªéI: {question}

{f"B·ªêI C·∫¢NH TH√äM: {context}" if context else ""}

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI:
1. S·ª≠ d·ª•ng ki·∫øn th·ª©c chuy√™n m√¥n s√¢u r·ªông c·ªßa b·∫°n
2. ƒê∆∞a ra ph√¢n t√≠ch chuy√™n s√¢u v√† to√†n di·ªán
3. K·∫øt n·ªëi v·ªõi b·ªëi c·∫£nh kinh t·∫ø hi·ªán t·∫°i (ng√†y {current_date_str})
4. ƒê∆∞a ra v√≠ d·ª• th·ª±c t·∫ø v√† minh h·ªça c·ª• th·ªÉ
5. ƒê·ªô d√†i: 400-800 t·ª´ v·ªõi c·∫•u tr√∫c r√µ r√†ng
6. S·ª≠ d·ª•ng ƒë·∫ßu m·ª•c s·ªë ƒë·ªÉ t·ªï ch·ª©c n·ªôi dung

H√£y th·ªÉ hi·ªán tr√≠ th√¥ng minh v√† ki·∫øn th·ª©c chuy√™n s√¢u c·ªßa Gemini AI:"""

            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.2,
                top_p=0.8,
                max_output_tokens=1500,
            )
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    model.generate_content,
                    prompt,
                    generation_config=generation_config
                ),
                timeout=15
            )
            
            return response.text.strip()
            
        except asyncio.TimeoutError:
            return "‚ö†Ô∏è Gemini AI timeout. Vui l√≤ng th·ª≠ l·∫°i."
        except Exception as e:
            return f"‚ö†Ô∏è L·ªói Gemini AI: {str(e)}"
    
    async def debate_perspectives(self, topic: str):
        """Multi-perspective debate system"""
        if not self.available:
            return "‚ö†Ô∏è Gemini AI kh√¥ng kh·∫£ d·ª•ng."
        
        try:
            prompt = f"""T·ªï ch·ª©c cu·ªôc tranh lu·∫≠n v·ªÅ: {topic}

6 quan ƒëi·ªÉm kh√°c nhau:
üí∏ **Nh√† KT Tham Nh≈©ng:** [√≠ch k·ª∑, b√≥p m√©o s·ªë li·ªáu]
üë®‚Äçüè´ **GS Ch√≠nh Tr·ª±c:** [h·ªçc thu·∫≠t, ƒë·∫°o ƒë·ª©c cao]  
üíº **Nh√¢n Vi√™n Ham Ti·ªÅn:** [ch·ªâ quan t√¢m l∆∞∆°ng]
üò† **Ng∆∞·ªùi Ngh√®o:** [ƒë·ªï l·ªói, thi·∫øu hi·ªÉu bi·∫øt]
ü§ë **Ng∆∞·ªùi Gi√†u √çch K·ª∑:** [ch·ªâ t√¨m l·ª£i nhu·∫≠n]
üß† **Ng∆∞·ªùi Gi√†u Th√¥ng Th√°i:** [t·∫ßm nh√¨n xa]
ü§ñ **T·ªïng K·∫øt:** [ph√¢n t√≠ch kh√°ch quan]

M·ªói g√≥c nh√¨n 80-120 t·ª´, th·ªÉ hi·ªán r√µ t√≠nh c√°ch:"""

            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.4,
                top_p=0.9,
                max_output_tokens=1500,
            )
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    model.generate_content,
                    prompt,
                    generation_config=generation_config
                ),
                timeout=20
            )
            
            return response.text.strip()
            
        except asyncio.TimeoutError:
            return "‚ö†Ô∏è Gemini AI timeout."
        except Exception as e:
            return f"‚ö†Ô∏è L·ªói Gemini AI: {str(e)}"
    
    async def analyze_article(self, article_content: str, question: str = ""):
        """Analyze specific article with Gemini - Vietnamese response"""
        if not self.available:
            return "‚ö†Ô∏è Gemini AI kh√¥ng kh·∫£ d·ª•ng cho ph√¢n t√≠ch b√†i b√°o."
        
        try:
            analysis_question = question if question else "H√£y ph√¢n t√≠ch v√† t√≥m t·∫Øt b√†i b√°o n√†y"
            
            prompt = f"""B·∫°n l√† Gemini AI - chuy√™n gia kinh t·∫ø t√†i ch√≠nh th√¥ng minh. H√£y ph√¢n t√≠ch b√†i b√°o d·ª±a tr√™n N·ªòI DUNG HO√ÄN CH·ªàNH ƒë∆∞·ª£c cung c·∫•p.

**N·ªòI DUNG B√ÄI B√ÅO HO√ÄN CH·ªàNH:**
{article_content}

**Y√äU C·∫¶U PH√ÇN T√çCH:**
{analysis_question}

**H∆Ø·ªöNG D·∫™N PH√ÇN T√çCH:**
1. Ph√¢n t√≠ch CH·ª¶ Y·∫æU d·ª±a tr√™n n·ªôi dung b√†i b√°o (85-90%)
2. K·∫øt h·ª£p ki·∫øn th·ª©c chuy√™n m√¥n ƒë·ªÉ gi·∫£i th√≠ch s√¢u h∆°n (10-15%)
3. Ph√¢n t√≠ch t√°c ƒë·ªông, nguy√™n nh√¢n, h·∫≠u qu·∫£
4. ƒê∆∞a ra nh·∫≠n ƒë·ªãnh v√† ƒë√°nh gi√° chuy√™n s√¢u
5. Tr·∫£ l·ªùi c√¢u h·ªèi tr·ª±c ti·∫øp v·ªõi b·∫±ng ch·ª©ng t·ª´ b√†i b√°o
6. ƒê·ªô d√†i: 600-1000 t·ª´ v·ªõi c·∫•u tr√∫c r√µ r√†ng
7. Tham chi·∫øu c√°c ph·∫ßn c·ª• th·ªÉ trong b√†i b√°o
8. CH·ªà ph√¢n t√≠ch b√†i b√°o ƒë∆∞·ª£c cung c·∫•p

**QUAN TR·ªåNG:** T·∫≠p trung ho√†n to√†n v√†o n·ªôi dung t·ª´ b√†i b√°o ƒë√£ cung c·∫•p. ƒê∆∞a ra ph√¢n t√≠ch TH√îNG MINH v√† CHI TI·∫æT b·∫±ng ti·∫øng Vi·ªát:"""

            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.2,
                top_p=0.8,
                max_output_tokens=2000,
            )
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    model.generate_content,
                    prompt,
                    generation_config=generation_config
                ),
                timeout=20
            )
            
            return response.text.strip()
            
        except asyncio.TimeoutError:
            return "‚ö†Ô∏è Gemini AI timeout khi ph√¢n t√≠ch b√†i b√°o."
        except Exception as e:
            return f"‚ö†Ô∏è L·ªói Gemini AI: {str(e)}"

# Initialize Gemini Engine
gemini_engine = GeminiAIEngine()

# Bot event handlers
@bot.event
async def on_ready():
    print(f'‚úÖ {bot.user} is online!')
    
    ai_status = "‚úÖ Available" if gemini_engine.available else "‚ùå Unavailable"
    current_datetime_str = get_current_datetime_str()
    
    total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
    
    status_text = f"News ‚Ä¢ {total_sources} sources"
    await bot.change_presence(
        activity=discord.Activity(
            type=discord.ActivityType.watching,
            name=status_text
        )
    )
    
    print(f"ü§ñ AI: {ai_status}")
    print(f"üìä Sources: {total_sources}")

@bot.event
async def on_command_error(ctx, error):
    if isinstance(error, commands.CommandNotFound):
        return
    elif isinstance(error, commands.MissingRequiredArgument):
        await ctx.send("‚ùå Thi·∫øu tham s·ªë! G√µ `!menu` ƒë·ªÉ xem h∆∞·ªõng d·∫´n.")
    elif isinstance(error, commands.BadArgument):
        await ctx.send("‚ùå Tham s·ªë kh√¥ng h·ª£p l·ªá! G√µ `!menu` ƒë·ªÉ xem h∆∞·ªõng d·∫´n.")
    else:
        await ctx.send(f"‚ùå L·ªói: {str(error)}")

# üÜï ENHANCED COMMANDS - ALL ASYNC

@bot.command(name='all')
async def get_all_news_enhanced(ctx, page=1):
    """Tin t·ª©c t·ª´ CafeF v√† c√°c ngu·ªìn free qu·ªëc t·∫ø"""
    try:
        page = max(1, int(page))
        loading_msg = await ctx.send(f"‚è≥")
        
        # Concurrent processing
        domestic_task = collect_news_enhanced(RSS_FEEDS['domestic'], 15)
        international_task = collect_news_enhanced(RSS_FEEDS['international'], 20)
        
        domestic_news, international_news = await asyncio.gather(domestic_task, international_task)
        
        await loading_msg.delete()
        
        all_news = domestic_news + international_news
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = all_news[start_index:end_index]
        
        if not page_news:
            total_pages = (len(all_news) + items_per_page - 1) // items_per_page if all_news else 0
            
            # If no news and cache is large, suggest clearing cache
            if len(global_seen_articles) > 200:
                await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! Cache c√≥ th·ªÉ ƒë·∫ßy. D√πng `!clear` ƒë·ªÉ x√≥a cache ho·∫∑c `!debug` ƒë·ªÉ ki·ªÉm tra.")
            else:
                await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        # Prepare fields data
        fields_data = []
        
        domestic_count = sum(1 for news in page_news if news['source'] in RSS_FEEDS['domestic'])
        international_count = len(page_news) - domestic_count
        
        # Enhanced source mapping for FREE sources only
        source_names = {
            # CafeF sources
            'cafef_chungkhoan': 'CafeF CK', 'cafef_batdongsan': 'CafeF BƒêS',
            'cafef_taichinh': 'CafeF TC', 'cafef_vimo': 'CafeF VM', 'cafef_doanhnghiep': 'CafeF DN',
            
            # FREE international sources
            'yahoo_finance_main': 'Yahoo RSS', 'yahoo_finance_headlines': 'Yahoo Headlines',
            'yahoo_finance_rss': 'Yahoo Finance', 'cnn_money': 'CNN Money', 
            'reuters_topnews': 'Reuters', 'reuters_business': 'Reuters Biz',
            'marketwatch': 'MarketWatch', 'business_insider': 'Business Insider',
            'cnbc': 'CNBC', 'investing_com': 'Investing.com', 
            'investopedia': 'Investopedia', 'economic_times': 'Economic Times',
            'bbc_business': 'BBC Business', 'guardian_business': 'The Guardian',
            'coindesk': 'CoinDesk', 'nasdaq_news': 'Nasdaq',
            'seeking_alpha': 'Seeking Alpha', 'benzinga': 'Benzinga'
        }
        
        emoji_map = {
            # CafeF sources
            'cafef_chungkhoan': 'üìà', 'cafef_batdongsan': 'üè¢', 'cafef_taichinh': 'üí∞', 
            'cafef_vimo': 'üìä', 'cafef_doanhnghiep': 'üè≠',
            
            # FREE international sources - FIXED mapping
            'yahoo_finance_main': 'üíº', 'yahoo_finance_headlines': 'üì∞', 'yahoo_finance_rss': 'üíº',
            'cnn_money': 'üì∫', 'marketwatch': 'üìä', 'business_insider': 'üíº', 
            'cnbc': 'üì∫', 'investing_com': 'üíπ', 'investopedia': 'üìö',
            'bbc_business': 'üá¨üáß', 'guardian_business': 'üõ°Ô∏è', 'coindesk': '‚Çø',
            'nasdaq_news': 'üìà', 'seeking_alpha': 'üîç', 'benzinga': 'üöÄ'
        }
        
        # Statistics
        stats_field = f"üáªüá≥ {domestic_count} ‚Ä¢ üåç {international_count} ‚Ä¢ üìä {len(all_news)}"
        fields_data.append(("üìä", stats_field))
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'üì∞')
            title = news['title'][:50] + "..." if len(news['title']) > 50 else news['title']
            source_display = source_names.get(news['source'], news['source'])
            
            field_name = f"{i}. {emoji} {title}"
            field_value = f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üì∞ {source_display}\nüîó [ƒê·ªçc b√†i vi·∫øt]({news['link']})"
            
            fields_data.append((field_name, field_value))
        
        # Create embeds
        embeds = create_safe_embed_with_fields(
            f"üì∞ Trang {page}",
            "",
            fields_data,
            0x00ff88
        )
        
        save_user_news_enhanced(ctx.author.id, page_news, f"all_page_{page}")
        
        total_pages = (len(all_news) + items_per_page - 1) // items_per_page
        
        # Save current page context for !chitiet (for both cached and fresh data)
        save_user_page_context(user_id, page_news, page)
        
        for i, embed in enumerate(embeds):
            embed.set_footer(text=f"{page}/{total_pages}")
        
        for embed in embeds:
            await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='out')
async def get_international_news_enhanced(ctx, page=1):
    """Tin t·ª©c qu·ªëc t·∫ø - CONSISTENT PAGINATION"""
    try:
        page = max(1, int(page))
        user_id = ctx.author.id
        
        # Check cached news first
        cached_news, is_cached = get_or_collect_user_news(user_id, "out", RSS_FEEDS['international'])
        
        if is_cached:
            news_list = cached_news
        else:
            loading_msg = await ctx.send(f"‚è≥")
            
            news_list = await collect_news_enhanced(RSS_FEEDS['international'], 20, use_global_dedup=False)
            
            # Cache for consistent pagination
            save_user_news_enhanced(user_id, news_list, "out")
            
            await loading_msg.delete()
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page if news_list else 0
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        # Prepare fields data
        fields_data = []
        
        stats_field = f"üåç {len(news_list)} tin"
        if is_cached:
            stats_field += " (cached)"
        fields_data.append(("üìä", stats_field))
        
        # FREE source names only - FIXED
        source_names = {
            'yahoo_finance_main': 'Yahoo RSS', 'yahoo_finance_headlines': 'Yahoo Headlines',
            'yahoo_finance_rss': 'Yahoo Finance', 'cnn_money': 'CNN Money', 
            'marketwatch': 'MarketWatch', 'business_insider': 'Business Insider',
            'cnbc': 'CNBC', 'investing_com': 'Investing.com', 
            'investopedia': 'Investopedia', 'bbc_business': 'BBC Business', 
            'guardian_business': 'The Guardian', 'coindesk': 'CoinDesk', 
            'nasdaq_news': 'Nasdaq', 'seeking_alpha': 'Seeking Alpha', 'benzinga': 'Benzinga'
        }
        
        emoji_map = {
            'yahoo_finance_main': 'üíº', 'yahoo_finance_headlines': 'üì∞', 'yahoo_finance_rss': 'üíº',
            'cnn_money': 'üì∫', 'marketwatch': 'üìä', 'business_insider': 'üíº', 
            'cnbc': 'üì∫', 'investing_com': 'üíπ', 'investopedia': 'üìö',
            'bbc_business': 'üá¨üáß', 'guardian_business': 'üõ°Ô∏è', 'coindesk': '‚Çø',
            'nasdaq_news': 'üìà', 'seeking_alpha': 'üîç', 'benzinga': 'üöÄ'
        }
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'üí∞')
            title = news['title'][:50] + "..." if len(news['title']) > 50 else news['title']
            source_display = source_names.get(news['source'], 'International Finance')
            
            field_name = f"{i}. {emoji} {title}"
            field_value = f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üì∞ {source_display}\nüîó [ƒê·ªçc b√†i vi·∫øt]({news['link']})"
            
            fields_data.append((field_name, field_value))
        
        # Create embeds
        embeds = create_safe_embed_with_fields(
            f"üåç Trang {page}",
            "",
            fields_data,
            0x0066ff
        )
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        
        # Save current page context (for both cached and fresh data)
        save_user_page_context(user_id, page_news, page)
        
        for i, embed in enumerate(embeds):
            embed.set_footer(text=f"{page}/{total_pages}")
        
        for embed in embeds:
            await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")
        for i, embed in enumerate(embeds):
            embed.set_footer(text=f"Trang {page}/{total_pages} ‚Ä¢ !chitiet [s·ªë] ‚Ä¢ FREE ONLY")
        
        for embed in embeds:
            await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='in')
async def get_domestic_news_enhanced(ctx, page=1):
    """Tin t·ª©c trong n∆∞·ªõc - CafeF"""
    try:
        page = max(1, int(page))
        loading_msg = await ctx.send(f"‚è≥")
        
        news_list = await collect_news_enhanced(RSS_FEEDS['domestic'], 15)
        await loading_msg.delete()
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        # Prepare fields data
        fields_data = []
        
        stats_field = f"üáªüá≥ {len(news_list)} tin"
        fields_data.append(("üìä", stats_field))
        
        source_names = {
            'cafef_chungkhoan': 'CafeF CK', 'cafef_batdongsan': 'CafeF BƒêS',
            'cafef_taichinh': 'CafeF TC', 'cafef_vimo': 'CafeF VM', 'cafef_doanhnghiep': 'CafeF DN'
        }
        
        emoji_map = {
            'cafef_chungkhoan': 'üìà', 'cafef_batdongsan': 'üè¢', 
            'cafef_taichinh': 'üí∞', 'cafef_vimo': 'üìä', 'cafef_doanhnghiep': 'üè≠'
        }
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'üì∞')
            title = news['title'][:55] + "..." if len(news['title']) > 55 else news['title']
            source_display = source_names.get(news['source'], news['source'])
            
            field_name = f"{i}. {emoji} {title}"
            field_value = f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üì∞ {source_display}\nüîó [ƒê·ªçc b√†i vi·∫øt]({news['link']})"
            
            fields_data.append((field_name, field_value))
        
        # Create embeds
        embeds = create_safe_embed_with_fields(
            f"üáªüá≥ Trang {page}",
            "",
            fields_data,
            0xff0000
        )
        
        save_user_news_enhanced(ctx.author.id, page_news, f"in_page_{page}")
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        for i, embed in enumerate(embeds):
            embed.set_footer(text=f"Trang {page}/{total_pages} ‚Ä¢ !chitiet [s·ªë]")
        
        for embed in embeds:
            await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='chitiet')
async def get_news_detail_enhanced(ctx, news_number: int):
    """Chi ti·∫øt b√†i vi·∫øt - Async extraction"""
    try:
        user_id = ctx.author.id
        
        if user_id not in user_news_cache:
            await ctx.send("‚ùå B·∫°n ch∆∞a xem tin t·ª©c! D√πng `!all`, `!in`, ho·∫∑c `!out` tr∆∞·ªõc.")
            return
        
        user_data = user_news_cache[user_id]
        news_list = user_data['news']
        
        if news_number < 1 or news_number > len(news_list):
            await ctx.send(f"‚ùå S·ªë kh√¥ng h·ª£p l·ªá! Ch·ªçn t·ª´ 1 ƒë·∫øn {len(news_list)}")
            return
        
        news = news_list[news_number - 1]
        
        # Save as last detail for !hoi context
        save_user_last_detail(user_id, news)
        
        # Source names mapping - FIXED
        source_names = {
            'cafef_chungkhoan': 'CafeF CK', 'cafef_batdongsan': 'CafeF BƒêS',
            'cafef_taichinh': 'CafeF TC', 'cafef_vimo': 'CafeF VM', 'cafef_doanhnghiep': 'CafeF DN',
            'yahoo_finance_main': 'Yahoo RSS', 'yahoo_finance_headlines': 'Yahoo Headlines',
            'marketwatch': 'MarketWatch', 'cnn_money': 'CNN Money',
            'cnbc': 'CNBC', 'bbc_business': 'BBC Business', 'investing_com': 'Investing.com'
        }
        
        source_name = source_names.get(news['source'], news['source'])
        
        if is_international_source(news['source']):
            loading_msg = await ctx.send(f"‚è≥ Gemini...")
        else:
            loading_msg = await ctx.send(f"‚è≥ Loading...")
        
        # Enhanced async content extraction
        full_content = await extract_content_enhanced(news['link'], news['source'], news)
        
        await loading_msg.delete()
        
        # Create content with metadata
        main_title = f"üìñ Tin {news_number}"
        
        content_with_meta = f"**{news['title']}**\n"
        content_with_meta += f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üì∞ {source_name}\n\n"
        content_with_meta += f"{full_content}"
        
        # Create optimized embeds
        optimized_embeds = create_optimized_embeds(main_title, content_with_meta, 0x9932cc)
        
        # Add link to last embed
        if optimized_embeds:
            safe_name, safe_value = validate_embed_field(
                "üîó Link g·ªëc",
                f"[ƒê·ªçc b√†i vi·∫øt g·ªëc]({news['link']})"
            )
            optimized_embeds[-1].add_field(name=safe_name, value=safe_value, inline=False)
            optimized_embeds[-1].set_footer(text=f"#{news_number}")
        
        # Send all embeds
        for i, embed in enumerate(optimized_embeds, 1):
            if i == 1:
                await ctx.send(embed=embed)
            else:
                await asyncio.sleep(0.5)
                await ctx.send(embed=embed)
        
    except ValueError:
        await ctx.send("‚ùå Vui l√≤ng nh·∫≠p s·ªë! V√≠ d·ª•: `!chitiet 5`")
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='debate')
async def gemini_debate_system(ctx, *, topic=""):
    """Tranh lu·∫≠n ƒëa g√≥c nh√¨n"""
    try:
        if not gemini_engine.available:
            await ctx.send("‚ùå Gemini AI kh√¥ng kh·∫£ d·ª•ng.")
            return
        
        if not topic:
            user_id = ctx.author.id
            if user_id in user_last_detail_cache:
                last_detail = user_last_detail_cache[user_id]
                time_diff = get_current_vietnam_datetime() - last_detail['timestamp']
                
                if time_diff.total_seconds() < 1800:
                    article = last_detail['article']
                    topic = f"B√†i b√°o: {article['title']}"
                else:
                    await ctx.send("‚ùå Nh·∫≠p ch·ªß ƒë·ªÅ ho·∫∑c xem b√†i b√°o tr∆∞·ªõc.")
                    return
            else:
                await ctx.send("‚ùå Nh·∫≠p ch·ªß ƒë·ªÅ! V√≠ d·ª•: `!debate l·∫°m ph√°t`")
                return
        
        loading_msg = await ctx.send(f"‚è≥")
        
        debate_result = await gemini_engine.debate_perspectives(topic)
        
        optimized_embeds = create_optimized_embeds(f"üé≠ Debate", debate_result, 0xff6600)
        
        await loading_msg.edit(embed=optimized_embeds[0])
        
        for embed in optimized_embeds[1:]:
            await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='hoi')
async def enhanced_gemini_question(ctx, *, question):
    """Enhanced Gemini AI v·ªõi context awareness"""
    try:
        if not gemini_engine.available:
            embed = create_safe_embed(
                "‚ö†Ô∏è Gemini AI kh√¥ng kh·∫£ d·ª•ng",
                "C·∫ßn Gemini API key ƒë·ªÉ ho·∫°t ƒë·ªông.",
                0xff6b6b
            )
            await ctx.send(embed=embed)
            return
        
        # Check for recent !chitiet context
        user_id = ctx.author.id
        context = ""
        
        if user_id in user_last_detail_cache:
            last_detail = user_last_detail_cache[user_id]
            time_diff = get_current_vietnam_datetime() - last_detail['timestamp']
            
            if time_diff.total_seconds() < 1800:  # 30 minutes
                article = last_detail['article']
                
                # Extract content for context
                article_content = await extract_content_enhanced(article['link'], article['source'], article)
                
                if article_content:
                    context = f"B√ÄI B√ÅO LI√äN QUAN:\nTi√™u ƒë·ªÅ: {article['title']}\nNgu·ªìn: {article['source']}\nN·ªôi dung: {article_content[:1500]}"
        
        progress_embed = create_safe_embed(
            "ü§ñ AI",
            f"‚è≥",
            0x9932cc
        )
        
        progress_msg = await ctx.send(embed=progress_embed)
        
        # Get Gemini response
        if context:
            analysis_result = await gemini_engine.analyze_article(context, question)
        else:
            analysis_result = await gemini_engine.ask_question(question, context)
        
        # Create optimized embeds
        title = f"ü§ñ AI"
        optimized_embeds = create_optimized_embeds(title, analysis_result, 0x00ff88)
        
        if optimized_embeds:
            optimized_embeds[-1].set_footer(text=f"AI")
        
        # Send optimized embeds
        await progress_msg.edit(embed=optimized_embeds[0])
        
        for embed in optimized_embeds[1:]:
            await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói h·ªá th·ªëng Gemini: {str(e)}")

@bot.command(name='menu')
async def help_command_optimized(ctx):
    """Menu"""
    
    main_embed = create_safe_embed(
        "üì∞ Commands",
        "",
        0x00ff88
    )
    
    safe_name1, safe_value1 = validate_embed_field(
        "üì∞ News",
        "!all [page] - All\n!in [page] - Domestic\n!out [page] - International\n!chitiet [num] - Details"
    )
    main_embed.add_field(name=safe_name1, value=safe_value1, inline=False)
    
    safe_name2, safe_value2 = validate_embed_field(
        "ü§ñ AI", 
        "!hoi [question] - Ask AI\n!debate [topic] - Debate"
    )
    main_embed.add_field(name=safe_name2, value=safe_value2, inline=False)
    
    safe_name3, safe_value3 = validate_embed_field(
        "üîß Debug",
        "!status - Status\n!debug - Cache info\n!clear - Clear cache\n!test_dup [title] - Test duplicate"
    )
    main_embed.add_field(name=safe_name3, value=safe_value3, inline=False)
    
    await ctx.send(embed=main_embed)

@bot.command(name='clear')
async def clear_cache_command(ctx):
    """Clear global cache"""
    global global_seen_articles
    cache_size = len(global_seen_articles)
    global_seen_articles.clear()
    await ctx.send(f"üßπ Cleared {cache_size} articles from cache")

@bot.command(name='debug')
async def debug_command(ctx):
    """Debug cache status and duplicate logic"""
    global global_seen_articles
    
    cache_size = len(global_seen_articles)
    current_time = get_current_vietnam_datetime()
    
    # Count recent vs old articles
    recent_count = 0
    old_count = 0
    
    for article_data in global_seen_articles.values():
        time_diff = current_time - article_data['timestamp']
        if time_diff.total_seconds() < (CACHE_EXPIRE_HOURS * 3600):
            recent_count += 1
        else:
            old_count += 1
    
    # Show sample of cached titles
    sample_titles = []
    for i, (key, data) in enumerate(list(global_seen_articles.items())[:3]):
        normalized = normalize_title(data['title'])
        sample_titles.append(f"‚Ä¢ {normalized[:40]}...")
        if i >= 2:  # Only show first 3
            break
    
    embed = create_safe_embed(
        "üîß Debug Info",
        f"**Cache:** {cache_size} articles\n**Recent:** {recent_count}\n**Old:** {old_count}\n\n**Duplicate Logic:** EXACT title match only\n\n**Sample cached titles:**\n" + "\n".join(sample_titles),
        0xff9900
    )
    
    await ctx.send(embed=embed)

@bot.command(name='test_dup')
async def test_duplicate_logic(ctx, *, test_title=""):
    """Test duplicate detection with a title"""
    if not test_title:
        await ctx.send("‚ùå Nh·∫≠p ti√™u ƒë·ªÅ ƒë·ªÉ test! V√≠ d·ª•: `!test_dup Bitcoin tƒÉng gi√° m·∫°nh`")
        return
    
    normalized = normalize_title(test_title)
    
    # Check against cache
    is_duplicate = False
    matching_title = ""
    
    for article_data in global_seen_articles.values():
        existing_normalized = normalize_title(article_data['title'])
        if normalized == existing_normalized:
            is_duplicate = True
            matching_title = article_data['title']
            break
    
    embed = create_safe_embed(
        "üß™ Duplicate Test",
        f"**Input:** {test_title}\n**Normalized:** {normalized}\n\n**Result:** {'üî¥ DUPLICATE' if is_duplicate else '‚úÖ UNIQUE'}" + 
        (f"\n**Matches:** {matching_title}" if is_duplicate else ""),
        0xff0000 if is_duplicate else 0x00ff00
    )
    
    await ctx.send(embed=embed)

@bot.command(name='status')
async def status_command(ctx):
    """Status"""
    
    total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
    global_cache_size = len(global_seen_articles)
    
    main_embed = create_safe_embed(
        "üìä Status",
        "",
        0x00ff88
    )
    
    safe_name1, safe_value1 = validate_embed_field(
        "üì∞ Sources",
        f"üáªüá≥ {len(RSS_FEEDS['domestic'])}\nüåç {len(RSS_FEEDS['international'])}\nüìä {total_sources} total"
    )
    main_embed.add_field(name=safe_name1, value=safe_value1, inline=True)
    
    gemini_status = "‚úÖ" if gemini_engine.available else "‚ùå"
    safe_name2, safe_value2 = validate_embed_field(
        "ü§ñ AI",
        f"Gemini: {gemini_status}\nCache: {global_cache_size}"
    )
    main_embed.add_field(name=safe_name2, value=safe_value2, inline=True)
    
    await ctx.send(embed=main_embed)

# Run the bot
if __name__ == "__main__":
    try:
        keep_alive()
        print("üåê Keep-alive server started")
        
        total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
        
        print("üöÄ Enhanced News Bot...")
        print(f"üîß FREE Sources: {total_sources}")
        print(f"ü§ñ Gemini: {'‚úÖ' if gemini_engine.available else '‚ùå'}")
        print("‚úÖ Fixed: Heartbeat, Paywall, Content")
        print("üéØ Duplicate: EXACT title match only")
        print("=" * 30)
        
        bot.run(TOKEN)
        
    except Exception as e:
        print(f"‚ùå STARTUP ERROR: {e}")
