import discord
from discord.ext import commands
import feedparser
import requests
import asyncio
import os
import re
from datetime import datetime
import time
import calendar
from urllib.parse import urljoin
import html
import chardet
import pytz
from keep_alive import keep_alive

# ğŸ†• THÃŠM CÃC THá»¬ VIá»†N Máº NH Máº¼ CHO TRÃCH XUáº¤T Ná»˜I DUNG
try:
    import trafilatura
    TRAFILATURA_AVAILABLE = True
    print("âœ… Trafilatura Ä‘Ã£ Ä‘Æ°á»£c tÃ­ch há»£p - TrÃ­ch xuáº¥t ná»™i dung cáº£i tiáº¿n!")
except ImportError:
    TRAFILATURA_AVAILABLE = False
    print("âš ï¸ Trafilatura chÆ°a cÃ i Ä‘áº·t. Cháº¡y: pip install trafilatura")

try:
    import newspaper
    from newspaper import Article
    NEWSPAPER_AVAILABLE = True
    print("âœ… Newspaper3k Ä‘Ã£ Ä‘Æ°á»£c tÃ­ch há»£p - Fallback extraction!")
except ImportError:
    NEWSPAPER_AVAILABLE = False
    print("âš ï¸ Newspaper3k chÆ°a cÃ i Ä‘áº·t. Cháº¡y: pip install newspaper3k")

# ğŸ¤– THÃŠM AI GIáº¢I THÃCH THÃ”NG MINH
try:
    import groq
    GROQ_AVAILABLE = True
    print("ğŸš€ Groq AI Ä‘Ã£ Ä‘Æ°á»£c tÃ­ch há»£p - Giáº£i thÃ­ch thÃ´ng minh!")
except ImportError:
    GROQ_AVAILABLE = False
    print("âš ï¸ Groq chÆ°a cÃ i Ä‘áº·t. Cháº¡y: pip install groq")

try:
    from googleapiclient.discovery import build
    GOOGLE_SEARCH_AVAILABLE = True
    print("ğŸ” Google Search API Ä‘Ã£ Ä‘Æ°á»£c tÃ­ch há»£p - TÃ¬m nguá»“n tin Ä‘Ã¡ng tin cáº­y!")
except ImportError:
    GOOGLE_SEARCH_AVAILABLE = False
    print("âš ï¸ Google API Client chÆ°a cÃ i Ä‘áº·t. Cháº¡y: pip install google-api-python-client")

import json

# Cáº¥u hÃ¬nh bot
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix='!', intents=intents)

# ğŸ”’ Báº¢O Máº¬T: Láº¥y token tá»« environment variable
TOKEN = os.getenv('DISCORD_TOKEN')

# ğŸ¤– Cáº¤U HÃŒNH AI APIS
GROQ_API_KEY = os.getenv('GROQ_API_KEY')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
GOOGLE_CSE_ID = os.getenv('GOOGLE_CSE_ID')

# Khá»Ÿi táº¡o Groq client
if GROQ_AVAILABLE and GROQ_API_KEY:
    try:
        groq_client = groq.Groq(api_key=GROQ_API_KEY)
        print("ğŸš€ Groq AI client Ä‘Ã£ sáºµn sÃ ng!")
    except Exception as e:
        print(f"âš ï¸ Lá»—i khá»Ÿi táº¡o Groq client: {e}")
        GROQ_AVAILABLE = False
else:
    GROQ_AVAILABLE = False

# Khá»Ÿi táº¡o Google Search client
if GOOGLE_SEARCH_AVAILABLE and GOOGLE_API_KEY and GOOGLE_CSE_ID:
    try:
        google_search_service = build("customsearch", "v1", developerKey=GOOGLE_API_KEY)
        print("ğŸ” Google Search API Ä‘Ã£ sáºµn sÃ ng!")
    except Exception as e:
        print(f"âš ï¸ Lá»—i khá»Ÿi táº¡o Google Search: {e}")
        GOOGLE_SEARCH_AVAILABLE = False
else:
    GOOGLE_SEARCH_AVAILABLE = False

if not TOKEN:
    print("âŒ Cáº¢NH BÃO: KhÃ´ng tÃ¬m tháº¥y DISCORD_TOKEN trong environment variables!")
    print("ğŸ”§ Vui lÃ²ng thÃªm DISCORD_TOKEN vÃ o Render Environment Variables")
    exit(1)

# ğŸ‡»ğŸ‡³ TIMEZONE VIá»†T NAM - ÄÃƒ Sá»¬A Lá»–I MÃšI GIá»œ
VN_TIMEZONE = pytz.timezone('Asia/Ho_Chi_Minh')
UTC_TIMEZONE = pytz.UTC

# LÆ°u trá»¯ tin tá»©c theo tá»«ng user
user_news_cache = {}

# RSS feeds Ä‘Ã£ Ä‘Æ°á»£c kiá»ƒm tra vÃ  xÃ¡c nháº­n hoáº¡t Ä‘á»™ng
RSS_FEEDS = {
    # === KINH Táº¾ TRONG NÆ¯á»šC - ÄÃƒ KIá»‚M TRA ===
    'domestic': {
        # CafeF - RSS chÃ­nh hoáº¡t Ä‘á»™ng tá»‘t
        'cafef_main': 'https://cafef.vn/index.rss',
        'cafef_chungkhoan': 'https://cafef.vn/thi-truong-chung-khoan.rss',
        'cafef_batdongsan': 'https://cafef.vn/bat-dong-san.rss',
        'cafef_taichinh': 'https://cafef.vn/tai-chinh-ngan-hang.rss',
        'cafef_vimo': 'https://cafef.vn/vi-mo-dau-tu.rss',
        
        # CafeBiz - RSS tá»•ng há»£p
        'cafebiz_main': 'https://cafebiz.vn/index.rss',
        
        # BÃ¡o Äáº§u tÆ° - RSS hoáº¡t Ä‘á»™ng
        'baodautu_main': 'https://baodautu.vn/rss.xml',
        
        # VnEconomy - RSS tin tá»©c chÃ­nh
        'vneconomy_main': 'https://vneconomy.vn/rss/home.rss',
        'vneconomy_chungkhoan': 'https://vneconomy.vn/rss/chung-khoan.rss',
        
        # VnExpress Kinh doanh 
        'vnexpress_kinhdoanh': 'https://vnexpress.net/rss/kinh-doanh.rss',
        'vnexpress_chungkhoan': 'https://vnexpress.net/rss/kinh-doanh/chung-khoan.rss',
        
        # Thanh NiÃªn - RSS kinh táº¿
        'thanhnien_kinhtevimo': 'https://thanhnien.vn/rss/kinh-te/vi-mo.rss',
        'thanhnien_chungkhoan': 'https://thanhnien.vn/rss/kinh-te/chung-khoan.rss',
        
        # NhÃ¢n DÃ¢n - RSS tÃ i chÃ­nh chá»©ng khoÃ¡n
        'nhandanonline_tc': 'https://nhandan.vn/rss/tai-chinh-chung-khoan.rss'
    },
    
    # === KINH Táº¾ QUá»C Táº¾ ===
    'international': {
        'yahoo_finance': 'https://feeds.finance.yahoo.com/rss/2.0/headline',
        'reuters_business': 'https://feeds.reuters.com/reuters/businessNews',
        'bloomberg_markets': 'https://feeds.bloomberg.com/markets/news.rss',
        'marketwatch_latest': 'https://feeds.marketwatch.com/marketwatch/realtimeheadlines/',
        'forbes_money': 'https://www.forbes.com/money/feed/',
        'financial_times': 'https://www.ft.com/rss/home',
        'business_insider': 'https://feeds.businessinsider.com/custom/all',
        'the_economist': 'https://www.economist.com/rss'
    }
}

def convert_utc_to_vietnam_time(utc_time_tuple):
    """ğŸ”§ Sá»¬A Lá»–I MÃšI GIá»œ: Chuyá»ƒn Ä‘á»•i UTC sang giá» Viá»‡t Nam chÃ­nh xÃ¡c"""
    try:
        # Sá»­ dá»¥ng calendar.timegm() thay vÃ¬ time.mktime() Ä‘á»ƒ xá»­ lÃ½ UTC Ä‘Ãºng cÃ¡ch
        utc_timestamp = calendar.timegm(utc_time_tuple)
        
        # Táº¡o datetime object UTC
        utc_dt = datetime.fromtimestamp(utc_timestamp, tz=UTC_TIMEZONE)
        
        # Chuyá»ƒn sang mÃºi giá» Viá»‡t Nam
        vn_dt = utc_dt.astimezone(VN_TIMEZONE)
        
        return vn_dt
    except Exception as e:
        print(f"âš ï¸ Lá»—i chuyá»ƒn Ä‘á»•i mÃºi giá»: {e}")
        # Fallback: sá»­ dá»¥ng thá»i gian hiá»‡n táº¡i
        return datetime.now(VN_TIMEZONE)

@bot.event
async def on_ready():
    print(f'âœ… {bot.user} Ä‘Ã£ online!')
    print(f'ğŸ“Š Káº¿t ná»‘i vá»›i {len(bot.guilds)} server(s)')
    total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
    print(f'ğŸ“° Sáºµn sÃ ng cung cáº¥p tin tá»« {total_sources} nguá»“n ÄÃƒ KIá»‚M TRA')
    print(f'ğŸ‡»ğŸ‡³ Trong nÆ°á»›c: {len(RSS_FEEDS["domestic"])} nguá»“n')
    print(f'ğŸŒ Quá»‘c táº¿: {len(RSS_FEEDS["international"])} nguá»“n')
    print('ğŸ¯ LÄ©nh vá»±c: Kinh táº¿, Chá»©ng khoÃ¡n, VÄ© mÃ´, Báº¥t Ä‘á»™ng sáº£n')
    
    # Kiá»ƒm tra thÆ° viá»‡n Ä‘Ã£ cÃ i Ä‘áº·t
    if TRAFILATURA_AVAILABLE:
        print('ğŸš€ Trafilatura: TrÃ­ch xuáº¥t ná»™i dung cáº£i tiáº¿n (94.5% Ä‘á»™ chÃ­nh xÃ¡c)')
    if NEWSPAPER_AVAILABLE:
        print('ğŸ“° Newspaper3k: Fallback extraction cho tin tá»©c')
    
    print('ğŸ•°ï¸ MÃºi giá»: ÄÃ£ sá»­a lá»—i - Hiá»ƒn thá»‹ chÃ­nh xÃ¡c giá» Viá»‡t Nam')
    print('ğŸ¯ GÃµ !menu Ä‘á»ƒ xem hÆ°á»›ng dáº«n')
    
    # Set bot status
    await bot.change_presence(
        activity=discord.Activity(
            type=discord.ActivityType.watching, 
            name="tin tá»©c VN chuáº©n giá» | !menu"
        )
    )

async def search_reliable_sources(query, max_results=5):
    """ğŸ” TÃŒM KIáº¾M NGUá»’N TIN ÄÃNG TIN Cáº¬Y Báº°NG GOOGLE SEARCH API"""
    try:
        if not GOOGLE_SEARCH_AVAILABLE:
            return []
        
        print(f"ğŸ” TÃ¬m kiáº¿m: {query}")
        
        # ThÃªm tá»« khÃ³a Ä‘á»ƒ tÃ¬m nguá»“n tin uy tÃ­n
        enhanced_query = f"{query} site:reuters.com OR site:bloomberg.com OR site:bbc.com OR site:vnexpress.net OR site:cafef.vn OR site:tuoitre.vn OR site:economist.com OR site:ft.com"
        
        # Gá»i Google Custom Search API
        result = google_search_service.cse().list(
            q=enhanced_query,
            cx=GOOGLE_CSE_ID,
            num=max_results,
            lr='lang_vi|lang_en',  # Tiáº¿ng Viá»‡t vÃ  tiáº¿ng Anh
            safe='active'
        ).execute()
        
        sources = []
        if 'items' in result:
            for item in result['items']:
                source = {
                    'title': item.get('title', ''),
                    'link': item.get('link', ''),
                    'snippet': item.get('snippet', ''),
                    'source_name': extract_source_name(item.get('link', ''))
                }
                sources.append(source)
        
        print(f"âœ… TÃ¬m tháº¥y {len(sources)} nguá»“n tin Ä‘Ã¡ng tin cáº­y")
        return sources
        
    except Exception as e:
        print(f"âš ï¸ Lá»—i Google Search: {e}")
        return []

def extract_source_name(url):
    """TrÃ­ch xuáº¥t tÃªn nguá»“n tin tá»« URL"""
    domain_mapping = {
        'reuters.com': 'Reuters',
        'bloomberg.com': 'Bloomberg',
        'bbc.com': 'BBC',
        'vnexpress.net': 'VnExpress',
        'cafef.vn': 'CafeF',
        'tuoitre.vn': 'Tuá»•i Tráº»',
        'economist.com': 'The Economist',
        'ft.com': 'Financial Times',
        'forbes.com': 'Forbes',
        'marketwatch.com': 'MarketWatch',
        'cnbc.com': 'CNBC'
    }
    
    for domain, name in domain_mapping.items():
        if domain in url:
            return name
    
    # Fallback: extract domain name
    try:
        from urllib.parse import urlparse
        domain = urlparse(url).netloc
        return domain.replace('www.', '').title()
    except:
        return 'Unknown Source'

async def detect_and_translate_content(content, source_name):
    """ğŸŒ PHÃT HIá»†N VÃ€ Dá»ŠCH Ná»˜I DUNG TIáº¾NG ANH SANG TIáº¾NG VIá»†T"""
    try:
        # Danh sÃ¡ch nguá»“n tin nÆ°á»›c ngoÃ i (tiáº¿ng Anh)
        international_sources = {
            'yahoo_finance', 'reuters_business', 'bloomberg_markets', 'marketwatch_latest',
            'forbes_money', 'financial_times', 'business_insider', 'the_economist'
        }
        
        # Chá»‰ dá»‹ch náº¿u lÃ  nguá»“n nÆ°á»›c ngoÃ i vÃ  cÃ³ Groq AI
        if source_name not in international_sources or not GROQ_AVAILABLE:
            return content, False
        
        # Kiá»ƒm tra náº¿u ná»™i dung cÃ³ váº» lÃ  tiáº¿ng Anh
        english_indicators = ['the', 'and', 'is', 'are', 'was', 'were', 'have', 'has', 'will', 'would', 'could', 'should']
        content_lower = content.lower()
        english_word_count = sum(1 for word in english_indicators if word in content_lower)
        
        # Náº¿u cÃ³ Ã­t nháº¥t 3 tá»« tiáº¿ng Anh thÃ´ng dá»¥ng thÃ¬ tiáº¿n hÃ nh dá»‹ch
        if english_word_count < 3:
            return content, False
        
        print(f"ğŸŒ Äang dá»‹ch ná»™i dung tá»« {source_name} sang tiáº¿ng Viá»‡t...")
        
        # Táº¡o prompt dá»‹ch thuáº­t chuyÃªn nghiá»‡p
        translation_prompt = f"""Báº¡n lÃ  má»™t chuyÃªn gia dá»‹ch thuáº­t kinh táº¿. HÃ£y dá»‹ch Ä‘oáº¡n vÄƒn tiáº¿ng Anh sau sang tiáº¿ng Viá»‡t má»™t cÃ¡ch chÃ­nh xÃ¡c, tá»± nhiÃªn vÃ  dá»… hiá»ƒu.

YÃŠU Cáº¦U Dá»ŠCH:
1. Giá»¯ nguyÃªn Ã½ nghÄ©a vÃ  ngá»¯ cáº£nh kinh táº¿
2. Sá»­ dá»¥ng thuáº­t ngá»¯ kinh táº¿ tiáº¿ng Viá»‡t chuáº©n
3. Dá»‹ch tá»± nhiÃªn, khÃ´ng mÃ¡y mÃ³c
4. Giá»¯ nguyÃªn cÃ¡c con sá»‘, tá»· lá»‡ pháº§n trÄƒm
5. KhÃ´ng thÃªm giáº£i thÃ­ch hay bÃ¬nh luáº­n

ÄOáº N VÄ‚N Cáº¦N Dá»ŠCH:
{content}

Báº¢N Dá»ŠCH TIáº¾NG VIá»†T:"""

        # Gá»i Groq AI Ä‘á»ƒ dá»‹ch
        chat_completion = groq_client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": translation_prompt
                }
            ],
            model="llama-3.3-70b-versatile",
            temperature=0.1,  # Ãt creativity Ä‘á»ƒ dá»‹ch chÃ­nh xÃ¡c
            max_tokens=2000
        )
        
        translated_content = chat_completion.choices[0].message.content.strip()
        print("âœ… Dá»‹ch thuáº­t thÃ nh cÃ´ng")
        return translated_content, True
        
    except Exception as e:
        print(f"âš ï¸ Lá»—i dá»‹ch thuáº­t: {e}")
        return content, False

async def ai_explain_with_sources(question, sources):
    """ğŸ¤– Sá»¬ Dá»¤NG GROQ AI Äá»‚ GIáº¢I THÃCH Vá»šI NGUá»’N TIN"""
    try:
        if not GROQ_AVAILABLE:
            return "âš ï¸ Groq AI khÃ´ng kháº£ dá»¥ng. Vui lÃ²ng cáº¥u hÃ¬nh GROQ_API_KEY."
        
        # Táº¡o context tá»« cÃ¡c nguá»“n tin
        context = "\n".join([
            f"Nguá»“n {i+1} ({source['source_name']}): {source['snippet']}"
            for i, source in enumerate(sources[:3])  # Chá»‰ láº¥y 3 nguá»“n Ä‘áº§u
        ])
        
        # Táº¡o prompt cho AI
        prompt = f"""Báº¡n lÃ  chuyÃªn gia kinh táº¿. HÃ£y giáº£i thÃ­ch thuáº­t ngá»¯ hoáº·c khÃ¡i niá»‡m sau má»™t cÃ¡ch Ä‘Æ¡n giáº£n, dá»… hiá»ƒu:

CÃ¢u há»i: {question}

ThÃ´ng tin tá»« cÃ¡c nguá»“n tin Ä‘Ã¡ng tin cáº­y:
{context}

YÃªu cáº§u:
1. Giáº£i thÃ­ch Ä‘Æ¡n giáº£n, dá»… hiá»ƒu cho ngÆ°á»i bÃ¬nh thÆ°á»ng
2. Sá»­ dá»¥ng thÃ´ng tin tá»« cÃ¡c nguá»“n Ä‘Ã£ cung cáº¥p
3. ÄÆ°a ra vÃ­ dá»¥ cá»¥ thá»ƒ náº¿u cÃ³ thá»ƒ
4. Giá»¯ cÃ¢u tráº£ lá»i trong khoáº£ng 300-500 tá»«
5. Viáº¿t báº±ng tiáº¿ng Viá»‡t

CÃ¢u tráº£ lá»i:"""

        # Gá»i Groq AI
        chat_completion = groq_client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            model="llama-3.3-70b-versatile",  # Model máº¡nh nháº¥t cá»§a Groq
            temperature=0.3,  # Ãt creativity, nhiá»u accuracy
            max_tokens=1000
        )
        
        explanation = chat_completion.choices[0].message.content
        print("ğŸ¤– AI Ä‘Ã£ táº¡o giáº£i thÃ­ch thÃ nh cÃ´ng")
        return explanation
        
    except Exception as e:
        print(f"âš ï¸ Lá»—i Groq AI: {e}")
        return f"âš ï¸ KhÃ´ng thá»ƒ táº¡o giáº£i thÃ­ch AI. Lá»—i: {str(e)}"
    """ğŸ†• TRÃCH XUáº¤T Ná»˜I DUNG Báº°NG TRAFILATURA - Tá»T NHáº¤T 2024"""
    try:
        if not TRAFILATURA_AVAILABLE:
            return None
        
        print(f"ğŸš€ Sá»­ dá»¥ng Trafilatura cho: {url}")
        
        # Táº£i ná»™i dung
        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            return None
        
        # TrÃ­ch xuáº¥t vá»›i metadata
        result = trafilatura.bare_extraction(
            downloaded,
            include_comments=False,
            include_tables=True,
            include_links=False,
            with_metadata=True
        )
        
        if result and result.get('text'):
            content = result['text']
            
            # Giá»›i háº¡n Ä‘á»™ dÃ i vÃ  lÃ m sáº¡ch
            if len(content) > 2000:
                content = content[:2000] + "..."
            
            return content.strip()
        
async def fetch_content_with_newspaper(url):
    """ğŸ“° TRÃCH XUáº¤T Báº°NG NEWSPAPER3K - FALLBACK"""
    try:
        if not NEWSPAPER_AVAILABLE:
            return None
        
        print(f"ğŸ“° Sá»­ dá»¥ng Newspaper3k cho: {url}")
        
        # Táº¡o article object
        article = Article(url)
        article.download()
        article.parse()
        
        if article.text:
            content = article.text
            
            # Giá»›i háº¡n Ä‘á»™ dÃ i
            if len(content) > 2000:
                content = content[:2000] + "..."
            
            return content.strip()
        
        return None
        
    except Exception as e:
        print(f"âš ï¸ Lá»—i Newspaper3k cho {url}: {e}")
        return None

async def fetch_content_legacy(url):
    """ğŸ”„ PHÆ¯Æ NG PHÃP CÅ¨ - CUá»I CÃ™NG FALLBACK"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        response = requests.get(url, headers=headers, timeout=8, stream=True)
        response.raise_for_status()
        
        # Xá»­ lÃ½ encoding
        raw_content = response.content
        detected = chardet.detect(raw_content)
        encoding = detected['encoding'] or 'utf-8'
        
        try:
            content = raw_content.decode(encoding)
        except:
            content = raw_content.decode('utf-8', errors='ignore')
        
        # Loáº¡i bá» HTML tags cÆ¡ báº£n
        clean_content = re.sub(r'<script[^>]*>.*?</script>', '', content, flags=re.DOTALL | re.IGNORECASE)
        clean_content = re.sub(r'<style[^>]*>.*?</style>', '', clean_content, flags=re.DOTALL | re.IGNORECASE)
        clean_content = re.sub(r'<[^>]+>', ' ', clean_content)
        clean_content = html.unescape(clean_content)
        clean_content = re.sub(r'\s+', ' ', clean_content).strip()
        
        # Láº¥y pháº§n Ä‘áº§u cÃ³ Ã½ nghÄ©a
        sentences = clean_content.split('. ')
        meaningful_content = []
        
        for sentence in sentences[:8]:
            if len(sentence.strip()) > 20:
                meaningful_content.append(sentence.strip())
                
        result = '. '.join(meaningful_content)
        
        if len(result) > 1800:
            result = result[:1800] + "..."
            
        return result if result else "KhÃ´ng thá»ƒ trÃ­ch xuáº¥t ná»™i dung tá»« bÃ i viáº¿t nÃ y."
        
    except Exception as e:
        print(f"âš ï¸ Lá»—i legacy extraction tá»« {url}: {e}")
        return f"KhÃ´ng thá»ƒ láº¥y ná»™i dung chi tiáº¿t. Lá»—i: {str(e)}"

async def fetch_full_content_improved(url):
    """ğŸ†• TRÃCH XUáº¤T Ná»˜I DUNG Cáº¢I TIáº¾N - Sá»¬ Dá»¤NG 3 PHÆ¯Æ NG PHÃP"""
    # Thá»­ phÆ°Æ¡ng phÃ¡p 1: Trafilatura (tá»‘t nháº¥t)
    content = await fetch_content_with_trafilatura(url)
    if content and len(content) > 50:
        print("âœ… ThÃ nh cÃ´ng vá»›i Trafilatura")
        return content
    
    # Thá»­ phÆ°Æ¡ng phÃ¡p 2: Newspaper3k (fallback)
    content = await fetch_content_with_newspaper(url)
    if content and len(content) > 50:
        print("âœ… ThÃ nh cÃ´ng vá»›i Newspaper3k")
        return content
    
    # PhÆ°Æ¡ng phÃ¡p 3: Legacy method (cuá»‘i cÃ¹ng)
    content = await fetch_content_legacy(url)
    print("âš ï¸ Sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p legacy")
    return content

async def fetch_content_with_newspaper(url):
    """ğŸ“° TRÃCH XUáº¤T Báº°NG NEWSPAPER3K - FALLBACK"""
    try:
        if not NEWSPAPER_AVAILABLE:
            return None
        
        print(f"ğŸ“° Sá»­ dá»¥ng Newspaper3k cho: {url}")
        
        # Táº¡o article object
        article = Article(url)
        article.download()
        article.parse()
        
        if article.text:
            content = article.text
            
            # Giá»›i háº¡n Ä‘á»™ dÃ i
            if len(content) > 2000:
                content = content[:2000] + "..."
            
            return content.strip()
        
        return None
        
    except Exception as e:
        print(f"âš ï¸ Lá»—i Newspaper3k cho {url}: {e}")
        return None

async def fetch_content_legacy(url):
    """ğŸ”„ PHÆ¯Æ NG PHÃP CÅ¨ - CUá»I CÃ™NG FALLBACK"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        response = requests.get(url, headers=headers, timeout=8, stream=True)
        response.raise_for_status()
        
        # Xá»­ lÃ½ encoding
        raw_content = response.content
        detected = chardet.detect(raw_content)
        encoding = detected['encoding'] or 'utf-8'
        
        try:
            content = raw_content.decode(encoding)
        except:
            content = raw_content.decode('utf-8', errors='ignore')
        
        # Loáº¡i bá» HTML tags cÆ¡ báº£n
        clean_content = re.sub(r'<script[^>]*>.*?</script>', '', content, flags=re.DOTALL | re.IGNORECASE)
        clean_content = re.sub(r'<style[^>]*>.*?</style>', '', clean_content, flags=re.DOTALL | re.IGNORECASE)
        clean_content = re.sub(r'<[^>]+>', ' ', clean_content)
        clean_content = html.unescape(clean_content)
        clean_content = re.sub(r'\s+', ' ', clean_content).strip()
        
        # Láº¥y pháº§n Ä‘áº§u cÃ³ Ã½ nghÄ©a
        sentences = clean_content.split('. ')
        meaningful_content = []
        
        for sentence in sentences[:8]:
            if len(sentence.strip()) > 20:
                meaningful_content.append(sentence.strip())
                
        result = '. '.join(meaningful_content)
        
        if len(result) > 1800:
            result = result[:1800] + "..."
            
        return result if result else "KhÃ´ng thá»ƒ trÃ­ch xuáº¥t ná»™i dung tá»« bÃ i viáº¿t nÃ y."
        
    except Exception as e:
        print(f"âš ï¸ Lá»—i legacy extraction tá»« {url}: {e}")
        return f"KhÃ´ng thá»ƒ láº¥y ná»™i dung chi tiáº¿t. Lá»—i: {str(e)}"

async def fetch_full_content_improved(url):
    """ğŸ†• TRÃCH XUáº¤T Ná»˜I DUNG Cáº¢I TIáº¾N - Sá»¬ Dá»¤NG 3 PHÆ¯Æ NG PHÃP"""
    # Thá»­ phÆ°Æ¡ng phÃ¡p 1: Trafilatura (tá»‘t nháº¥t)
    content = await fetch_content_with_trafilatura(url)
    if content and len(content) > 50:
        print("âœ… ThÃ nh cÃ´ng vá»›i Trafilatura")
        return content
    
    # Thá»­ phÆ°Æ¡ng phÃ¡p 2: Newspaper3k (fallback)
    content = await fetch_content_with_newspaper(url)
    if content and len(content) > 50:
        print("âœ… ThÃ nh cÃ´ng vá»›i Newspaper3k")
        return content
    
    # PhÆ°Æ¡ng phÃ¡p 3: Legacy method (cuá»‘i cÃ¹ng)
    content = await fetch_content_legacy(url)
    print("âš ï¸ Sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p legacy")
    return content

async def collect_news_from_sources(sources_dict, limit_per_source=8):
    """Thu tháº­p tin tá»©c vá»›i xá»­ lÃ½ mÃºi giá» chÃ­nh xÃ¡c"""
    all_news = []
    
    for source_name, rss_url in sources_dict.items():
        try:
            print(f"ğŸ”„ Äang láº¥y tin tá»« {source_name}...")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/rss+xml, application/xml, text/xml',
                'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8'
            }
            
            try:
                response = requests.get(rss_url, headers=headers, timeout=10)
                response.raise_for_status()
                feed = feedparser.parse(response.content)
            except Exception as req_error:
                print(f"âš ï¸ Lá»—i request tá»« {source_name}: {req_error}")
                feed = feedparser.parse(rss_url)
            
            if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                print(f"âš ï¸ KhÃ´ng cÃ³ tin tá»« {source_name}")
                continue
                
            entries_processed = 0
            for entry in feed.entries[:limit_per_source]:
                try:
                    # ğŸ”§ Xá»¬ LÃ THá»œI GIAN CHÃNH XÃC
                    vn_time = datetime.now(VN_TIMEZONE)  # Default fallback
                    
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        vn_time = convert_utc_to_vietnam_time(entry.published_parsed)
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        vn_time = convert_utc_to_vietnam_time(entry.updated_parsed)
                    
                    # Láº¥y mÃ´ táº£
                    description = ""
                    if hasattr(entry, 'summary'):
                        description = entry.summary[:500] + "..." if len(entry.summary) > 500 else entry.summary
                    elif hasattr(entry, 'description'):
                        description = entry.description[:500] + "..." if len(entry.description) > 500 else entry.description
                    
                    if not hasattr(entry, 'title') or not hasattr(entry, 'link'):
                        continue
                    
                    title = html.unescape(entry.title.strip())
                    
                    news_item = {
                        'title': title,
                        'link': entry.link,
                        'source': source_name,
                        'published': vn_time,
                        'published_str': vn_time.strftime("%H:%M %d/%m"),
                        'description': html.unescape(description) if description else ""
                    }
                    all_news.append(news_item)
                    entries_processed += 1
                    
                except Exception as entry_error:
                    print(f"âš ï¸ Lá»—i xá»­ lÃ½ tin tá»« {source_name}: {entry_error}")
                    continue
                    
            print(f"âœ… Láº¥y Ä‘Æ°á»£c {entries_processed} tin tá»« {source_name}")
            
        except Exception as e:
            print(f"âŒ Lá»—i khi láº¥y tin tá»« {source_name}: {e}")
            continue
    
    print(f"ğŸ“Š Tá»•ng cá»™ng láº¥y Ä‘Æ°á»£c {len(all_news)} tin tá»« táº¥t cáº£ nguá»“n")
    
    # Loáº¡i bá» tin trÃ¹ng láº·p
    unique_news = remove_duplicate_news(all_news)
    print(f"ğŸ”„ Sau khi loáº¡i trÃ¹ng cÃ²n {len(unique_news)} tin")
    
    # Sáº¯p xáº¿p theo thá»i gian má»›i nháº¥t
    unique_news.sort(key=lambda x: x['published'], reverse=True)
    return unique_news

def remove_duplicate_news(news_list):
    """Loáº¡i bá» tin tá»©c trÃ¹ng láº·p"""
    seen_links = set()
    seen_titles = set()
    unique_news = []
    
    for news in news_list:
        normalized_title = normalize_title(news['title'])
        
        is_duplicate = False
        
        if news['link'] in seen_links:
            is_duplicate = True
        else:
            for existing_title in seen_titles:
                similarity = calculate_title_similarity(normalized_title, existing_title)
                if similarity > 0.75:
                    is_duplicate = True
                    break
        
        if not is_duplicate:
            seen_links.add(news['link'])
            seen_titles.add(normalized_title)
            unique_news.append(news)
    
    return unique_news

def calculate_title_similarity(title1, title2):
    """TÃ­nh Ä‘á»™ tÆ°Æ¡ng tá»± giá»¯a 2 tiÃªu Ä‘á»"""
    words1 = set(title1.split())
    words2 = set(title2.split())
    
    if not words1 or not words2:
        return 0
    
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    return len(intersection) / len(union) if union else 0

def normalize_title(title):
    """Chuáº©n hÃ³a tiÃªu Ä‘á» Ä‘á»ƒ so sÃ¡nh trÃ¹ng láº·p"""
    import re
    title = title.lower()
    title = re.sub(r'[^\w\s]', '', title)
    title = ' '.join(title.split())
    
    words = title.split()[:10]
    return ' '.join(words)

def save_user_news(user_id, news_list, command_type):
    """LÆ°u tin tá»©c cá»§a user Ä‘á»ƒ sá»­ dá»¥ng cho lá»‡nh !detail"""
    user_news_cache[user_id] = {
        'news': news_list,
        'command': command_type,
        'timestamp': datetime.now(VN_TIMEZONE)
    }

@bot.command(name='all')
async def get_all_news(ctx, page=1):
    """Láº¥y tin tá»©c tá»« táº¥t cáº£ nguá»“n vá»›i mÃºi giá» chÃ­nh xÃ¡c"""
    try:
        page = max(1, int(page))
        
        loading_msg = await ctx.send("â³ Äang táº£i tin tá»©c tá»« táº¥t cáº£ nguá»“n...")
        
        domestic_news = await collect_news_from_sources(RSS_FEEDS['domestic'], 8)
        international_news = await collect_news_from_sources(RSS_FEEDS['international'], 6)
        
        await loading_msg.delete()
        
        all_news = domestic_news + international_news
        all_news.sort(key=lambda x: x['published'], reverse=True)
        
        # PhÃ¢n trang
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = all_news[start_index:end_index]
        
        if not page_news:
            total_pages = (len(all_news) + items_per_page - 1) // items_per_page
            await ctx.send(f"âŒ KhÃ´ng cÃ³ tin tá»©c á»Ÿ trang {page}! Tá»•ng cá»™ng cÃ³ {total_pages} trang.")
            return
        
        # Táº¡o embed vá»›i thÃ´ng tin mÃºi giá»
        embed = discord.Embed(
            title=f"ğŸ“° Tin tá»©c kinh táº¿ tá»•ng há»£p (Trang {page})",
            description=f"ğŸ•°ï¸ Giá» Viá»‡t Nam chÃ­nh xÃ¡c â€¢ ğŸš€ Trafilatura â€¢ ğŸ“° Tá»« {len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])} nguá»“n",
            color=0x00ff88,
            timestamp=ctx.message.created_at
        )
        
        # Emoji map
        emoji_map = {
            'cafef_main': 'â˜•', 'cafef_chungkhoan': 'ğŸ“ˆ', 'cafef_batdongsan': 'ğŸ¢', 'cafef_taichinh': 'ğŸ’°', 'cafef_vimo': 'ğŸ“Š',
            'cafebiz_main': 'ğŸ’¼', 'baodautu_main': 'ğŸ¯', 'vneconomy_main': 'ğŸ“°', 'vneconomy_chungkhoan': 'ğŸ“ˆ',
            'vnexpress_kinhdoanh': 'âš¡', 'vnexpress_chungkhoan': 'ğŸ“ˆ', 'thanhnien_kinhtevimo': 'ğŸ“Š', 'thanhnien_chungkhoan': 'ğŸ“ˆ',
            'nhandanonline_tc': 'ğŸ›ï¸', 'yahoo_finance': 'ğŸ’°', 'reuters_business': 'ğŸŒ', 'bloomberg_markets': 'ğŸ’¹', 
            'marketwatch_latest': 'ğŸ“ˆ', 'forbes_money': 'ğŸ’', 'financial_times': 'ğŸ’¼', 'business_insider': 'ğŸ“°', 'the_economist': 'ğŸ“'
        }
        
        # Thá»‘ng kÃª
        domestic_count = sum(1 for news in page_news if news['source'] in RSS_FEEDS['domestic'])
        international_count = len(page_news) - domestic_count
        
        embed.add_field(
            name="ğŸ“Š Thá»‘ng kÃª trang nÃ y",
            value=f"ğŸ‡»ğŸ‡³ Trong nÆ°á»›c: {domestic_count} tin\nğŸŒ Quá»‘c táº¿: {international_count} tin\nğŸ“Š Tá»•ng cÃ³ sáºµn: {len(all_news)} tin",
            inline=False
        )
        
        # Hiá»ƒn thá»‹ tin tá»©c vá»›i thá»i gian chÃ­nh xÃ¡c
        source_names = {
            'cafef_main': 'CafeF', 'cafef_chungkhoan': 'CafeF CK', 'cafef_batdongsan': 'CafeF BÄS',
            'cafef_taichinh': 'CafeF TC', 'cafef_vimo': 'CafeF VM', 'cafebiz_main': 'CafeBiz',
            'baodautu_main': 'BÃ¡o Äáº§u tÆ°', 'vneconomy_main': 'VnEconomy', 'vneconomy_chungkhoan': 'VnEconomy CK',
            'vnexpress_kinhdoanh': 'VnExpress KD', 'vnexpress_chungkhoan': 'VnExpress CK',
            'thanhnien_kinhtevimo': 'Thanh NiÃªn VM', 'thanhnien_chungkhoan': 'Thanh NiÃªn CK',
            'nhandanonline_tc': 'NhÃ¢n DÃ¢n TC', 'yahoo_finance': 'Yahoo Finance', 'reuters_business': 'Reuters',
            'bloomberg_markets': 'Bloomberg', 'marketwatch_latest': 'MarketWatch', 'forbes_money': 'Forbes',
            'financial_times': 'Financial Times', 'business_insider': 'Business Insider', 'the_economist': 'The Economist'
        }
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'ğŸ“°')
            title = news['title'][:70] + "..." if len(news['title']) > 70 else news['title']
            source_display = source_names.get(news['source'], news['source'])
            
            embed.add_field(
                name=f"{i}. {emoji} {title}",
                value=f"ğŸ•°ï¸ {news['published_str']} (VN) â€¢ ğŸ“° {source_display}\nğŸ”— [Äá»c bÃ i viáº¿t]({news['link']})",
                inline=False
            )
        
        save_user_news(ctx.author.id, page_news, f"all_page_{page}")
        
        total_pages = (len(all_news) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"ğŸš€ Bot cáº£i tiáº¿n â€¢ Trang {page}/{total_pages} â€¢ !all {page+1} tiáº¿p â€¢ !chitiet [sá»‘] xem chi tiáº¿t")
        
        await ctx.send(embed=embed)
        
    except ValueError:
        await ctx.send("âŒ Sá»‘ trang khÃ´ng há»£p lá»‡! Sá»­ dá»¥ng: `!all [sá»‘]`")
    except Exception as e:
        await ctx.send(f"âŒ Lá»—i: {str(e)}")

@bot.command(name='in')
async def get_domestic_news(ctx, page=1):
    """Láº¥y tin tá»©c tá»« cÃ¡c nguá»“n trong nÆ°á»›c vá»›i mÃºi giá» chÃ­nh xÃ¡c"""
    try:
        page = max(1, int(page))
        
        loading_msg = await ctx.send("â³ Äang táº£i tin tá»©c trong nÆ°á»›c...")
        
        news_list = await collect_news_from_sources(RSS_FEEDS['domestic'], 10)
        
        await loading_msg.delete()
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page
            await ctx.send(f"âŒ KhÃ´ng cÃ³ tin tá»©c á»Ÿ trang {page}! Tá»•ng cá»™ng cÃ³ {total_pages} trang.")
            return
        
        embed = discord.Embed(
            title=f"ğŸ‡»ğŸ‡³ Tin kinh táº¿ trong nÆ°á»›c (Trang {page})",
            description=f"ğŸ•°ï¸ Giá» Viá»‡t Nam chÃ­nh xÃ¡c â€¢ ğŸš€ Trafilatura â€¢ Tá»« {len(RSS_FEEDS['domestic'])} nguá»“n chuyÃªn ngÃ nh",
            color=0xff0000,
            timestamp=ctx.message.created_at
        )
        
        embed.add_field(
            name="ğŸ“Š ThÃ´ng tin",
            value=f"ğŸ“° Tá»•ng tin cÃ³ sáºµn: {len(news_list)} tin\nğŸ¯ LÄ©nh vá»±c: Kinh táº¿, Chá»©ng khoÃ¡n, Báº¥t Ä‘á»™ng sáº£n, VÄ© mÃ´",
            inline=False
        )
        
        # Hiá»ƒn thá»‹ tin tá»©c trong nÆ°á»›c
        emoji_map = {
            'cafef_main': 'â˜•', 'cafef_chungkhoan': 'ğŸ“ˆ', 'cafef_batdongsan': 'ğŸ¢', 'cafef_taichinh': 'ğŸ’°', 'cafef_vimo': 'ğŸ“Š',
            'cafebiz_main': 'ğŸ’¼', 'baodautu_main': 'ğŸ¯', 'vneconomy_main': 'ğŸ“°', 'vneconomy_chungkhoan': 'ğŸ“ˆ',
            'vnexpress_kinhdoanh': 'âš¡', 'vnexpress_chungkhoan': 'ğŸ“ˆ', 'thanhnien_kinhtevimo': 'ğŸ“Š', 'thanhnien_chungkhoan': 'ğŸ“ˆ',
            'nhandanonline_tc': 'ğŸ›ï¸'
        }
        
        source_names = {
            'cafef_main': 'CafeF', 'cafef_chungkhoan': 'CafeF CK', 'cafef_batdongsan': 'CafeF BÄS',
            'cafef_taichinh': 'CafeF TC', 'cafef_vimo': 'CafeF VM', 'cafebiz_main': 'CafeBiz',
            'baodautu_main': 'BÃ¡o Äáº§u tÆ°', 'vneconomy_main': 'VnEconomy', 'vneconomy_chungkhoan': 'VnEconomy CK',
            'vnexpress_kinhdoanh': 'VnExpress KD', 'vnexpress_chungkhoan': 'VnExpress CK',
            'thanhnien_kinhtevimo': 'Thanh NiÃªn VM', 'thanhnien_chungkhoan': 'Thanh NiÃªn CK',
            'nhandanonline_tc': 'NhÃ¢n DÃ¢n TC'
        }
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'ğŸ“°')
            title = news['title'][:70] + "..." if len(news['title']) > 70 else news['title']
            source_display = source_names.get(news['source'], news['source'])
            
            embed.add_field(
                name=f"{i}. {emoji} {title}",
                value=f"ğŸ•°ï¸ {news['published_str']} (VN) â€¢ ğŸ“° {source_display}\nğŸ”— [Äá»c bÃ i viáº¿t]({news['link']})",
                inline=False
            )
        
        save_user_news(ctx.author.id, page_news, f"in_page_{page}")
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"ğŸš€ Bot cáº£i tiáº¿n â€¢ Trang {page}/{total_pages} â€¢ !in {page+1} tiáº¿p â€¢ !chitiet [sá»‘] xem chi tiáº¿t")
        
        await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"âŒ Lá»—i: {str(e)}")

@bot.command(name='out')
async def get_international_news(ctx, page=1):
    """Láº¥y tin tá»©c tá»« cÃ¡c nguá»“n quá»‘c táº¿ vá»›i mÃºi giá» chÃ­nh xÃ¡c"""
    try:
        page = max(1, int(page))
        
        loading_msg = await ctx.send("â³ Äang táº£i tin tá»©c quá»‘c táº¿...")
        
        news_list = await collect_news_from_sources(RSS_FEEDS['international'], 8)
        
        await loading_msg.delete()
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page
            await ctx.send(f"âŒ KhÃ´ng cÃ³ tin tá»©c á»Ÿ trang {page}! Tá»•ng cá»™ng cÃ³ {total_pages} trang.")
            return
        
        embed = discord.Embed(
            title=f"ğŸŒ Tin kinh táº¿ quá»‘c táº¿ (Trang {page})",
            description=f"ğŸ•°ï¸ Giá» Viá»‡t Nam chÃ­nh xÃ¡c â€¢ ğŸš€ Trafilatura â€¢ Tá»« {len(RSS_FEEDS['international'])} nguá»“n hÃ ng Ä‘áº§u",
            color=0x0066ff,
            timestamp=ctx.message.created_at
        )
        
        embed.add_field(
            name="ğŸ“Š ThÃ´ng tin",
            value=f"ğŸ“° Tá»•ng tin cÃ³ sáºµn: {len(news_list)} tin",
            inline=False
        )
        
        emoji_map = {
            'yahoo_finance': 'ğŸ’°', 'reuters_business': 'ğŸŒ', 'bloomberg_markets': 'ğŸ’¹', 'marketwatch_latest': 'ğŸ“ˆ',
            'forbes_money': 'ğŸ’', 'financial_times': 'ğŸ’¼', 'business_insider': 'ğŸ“°', 'the_economist': 'ğŸ“'
        }
        
        source_names = {
            'yahoo_finance': 'Yahoo Finance', 'reuters_business': 'Reuters', 'bloomberg_markets': 'Bloomberg', 
            'marketwatch_latest': 'MarketWatch', 'forbes_money': 'Forbes', 'financial_times': 'Financial Times', 
            'business_insider': 'Business Insider', 'the_economist': 'The Economist'
        }
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'ğŸŒ')
            title = news['title'][:70] + "..." if len(news['title']) > 70 else news['title']
            source_display = source_names.get(news['source'], news['source'])
            
            embed.add_field(
                name=f"{i}. {emoji} {title}",
                value=f"ğŸ•°ï¸ {news['published_str']} (VN) â€¢ ğŸ“° {source_display}\nğŸ”— [Äá»c bÃ i viáº¿t]({news['link']})",
                inline=False
            )
        
        save_user_news(ctx.author.id, page_news, f"out_page_{page}")
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"ğŸš€ Bot cáº£i tiáº¿n â€¢ Trang {page}/{total_pages} â€¢ !out {page+1} tiáº¿p â€¢ !chitiet [sá»‘] xem chi tiáº¿t")
        
        await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"âŒ Lá»—i: {str(e)}")

@bot.command(name='chitiet')
async def get_news_detail(ctx, news_number: int):
    """ğŸ†• XEM CHI TIáº¾T Báº°NG TRAFILATURA + NEWSPAPER3K + Tá»° Äá»˜NG Dá»ŠCH"""
    try:
        user_id = ctx.author.id
        
        if user_id not in user_news_cache:
            await ctx.send("âŒ Báº¡n chÆ°a xem tin tá»©c nÃ o! HÃ£y dÃ¹ng `!all`, `!in`, hoáº·c `!out` trÆ°á»›c.")
            return
        
        user_data = user_news_cache[user_id]
        news_list = user_data['news']
        
        if news_number < 1 or news_number > len(news_list):
            await ctx.send(f"âŒ Sá»‘ khÃ´ng há»£p lá»‡! Chá»n tá»« 1 Ä‘áº¿n {len(news_list)}")
            return
        
        news = news_list[news_number - 1]
        
        # ThÃ´ng bÃ¡o Ä‘ang táº£i vá»›i thÃ´ng tin cÃ´ng nghá»‡
        if TRAFILATURA_AVAILABLE and NEWSPAPER_AVAILABLE:
            loading_msg = await ctx.send("ğŸš€ Äang trÃ­ch xuáº¥t ná»™i dung báº±ng Trafilatura + Newspaper3k...")
        elif TRAFILATURA_AVAILABLE:
            loading_msg = await ctx.send("ğŸš€ Äang trÃ­ch xuáº¥t ná»™i dung báº±ng Trafilatura...")
        else:
            loading_msg = await ctx.send("â³ Äang trÃ­ch xuáº¥t ná»™i dung...")
        
        # Sá»­ dá»¥ng function cáº£i tiáº¿n
        full_content = await fetch_full_content_improved(news['link'])
        
        # ğŸŒ TÃNH NÄ‚NG Má»šI: Tá»± Ä‘á»™ng dá»‹ch náº¿u lÃ  tin nÆ°á»›c ngoÃ i
        translated_content, is_translated = await detect_and_translate_content(full_content, news['source'])
        
        await loading_msg.delete()
        
        # Táº¡o embed Ä‘áº¹p hÆ¡n
        embed = discord.Embed(
            title="ğŸ“– Chi tiáº¿t bÃ i viáº¿t",
            color=0x9932cc,
            timestamp=ctx.message.created_at
        )
        
        # Emoji cho nguá»“n
        emoji_map = {
            'cafef_main': 'â˜•', 'cafef_chungkhoan': 'ğŸ“ˆ', 'cafef_batdongsan': 'ğŸ¢', 'cafef_taichinh': 'ğŸ’°', 'cafef_vimo': 'ğŸ“Š',
            'cafebiz_main': 'ğŸ’¼', 'baodautu_main': 'ğŸ¯', 'vneconomy_main': 'ğŸ“°', 'vneconomy_chungkhoan': 'ğŸ“ˆ',
            'vnexpress_kinhdoanh': 'âš¡', 'vnexpress_chungkhoan': 'ğŸ“ˆ', 'thanhnien_kinhtevimo': 'ğŸ“Š', 'thanhnien_chungkhoan': 'ğŸ“ˆ',
            'nhandanonline_tc': 'ğŸ›ï¸', 'yahoo_finance': 'ğŸ’°', 'reuters_business': 'ğŸŒ', 'bloomberg_markets': 'ğŸ’¹', 
            'marketwatch_latest': 'ğŸ“ˆ', 'forbes_money': 'ğŸ’', 'financial_times': 'ğŸ’¼', 'business_insider': 'ğŸ“°', 'the_economist': 'ğŸ“'
        }
        
        source_names = {
            'cafef_main': 'CafeF', 'cafef_chungkhoan': 'CafeF Chá»©ng khoÃ¡n', 'cafef_batdongsan': 'CafeF Báº¥t Ä‘á»™ng sáº£n',
            'cafef_taichinh': 'CafeF TÃ i chÃ­nh', 'cafef_vimo': 'CafeF VÄ© mÃ´', 'cafebiz_main': 'CafeBiz',
            'baodautu_main': 'BÃ¡o Äáº§u tÆ°', 'vneconomy_main': 'VnEconomy', 'vneconomy_chungkhoan': 'VnEconomy Chá»©ng khoÃ¡n',
            'vnexpress_kinhdoanh': 'VnExpress Kinh doanh', 'vnexpress_chungkhoan': 'VnExpress Chá»©ng khoÃ¡n',
            'thanhnien_kinhtevimo': 'Thanh NiÃªn VÄ© mÃ´', 'thanhnien_chungkhoan': 'Thanh NiÃªn Chá»©ng khoÃ¡n',
            'nhandanonline_tc': 'NhÃ¢n DÃ¢n TÃ i chÃ­nh', 'yahoo_finance': 'Yahoo Finance', 'reuters_business': 'Reuters Business',
            'bloomberg_markets': 'Bloomberg Markets', 'marketwatch_latest': 'MarketWatch', 'forbes_money': 'Forbes Money',
            'financial_times': 'Financial Times', 'business_insider': 'Business Insider', 'the_economist': 'The Economist'
        }
        
        emoji = emoji_map.get(news['source'], 'ğŸ“°')
        source_display = source_names.get(news['source'], news['source'])
        
        # ThÃªm indicator dá»‹ch thuáº­t vÃ o tiÃªu Ä‘á»
        title_suffix = " ğŸŒ (ÄÃ£ dá»‹ch)" if is_translated else ""
        embed.add_field(
            name=f"{emoji} TiÃªu Ä‘á»{title_suffix}",
            value=news['title'],
            inline=False
        )
        
        embed.add_field(
            name="ğŸ•°ï¸ Thá»i gian (VN)",
            value=news['published_str'],
            inline=True
        )
        
        embed.add_field(
            name="ğŸ“° Nguá»“n",
            value=source_display + (" ğŸŒ" if is_translated else ""),
            inline=True
        )
        
        # Sá»­ dá»¥ng ná»™i dung Ä‘Ã£ dá»‹ch (náº¿u cÃ³)
        content_to_display = translated_content
        
        # Hiá»ƒn thá»‹ ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½
        if len(content_to_display) > 1000:
            # Chia ná»™i dung thÃ nh 2 pháº§n
            content_title = "ğŸ“„ Ná»™i dung chi tiáº¿t ğŸŒ (ÄÃ£ dá»‹ch sang tiáº¿ng Viá»‡t)" if is_translated else "ğŸ“„ Ná»™i dung chi tiáº¿t"
            
            embed.add_field(
                name=f"{content_title} (Pháº§n 1)",
                value=content_to_display[:1000] + "...",
                inline=False
            )
            
            await ctx.send(embed=embed)
            
            # Táº¡o embed thá»© 2
            embed2 = discord.Embed(
                title=f"ğŸ“– Chi tiáº¿t bÃ i viáº¿t (tiáº¿p theo){'ğŸŒ' if is_translated else ''}",
                color=0x9932cc
            )
            
            embed2.add_field(
                name=f"{content_title} (Pháº§n 2)",
                value=content_to_display[1000:2000],
                inline=False
            )
            
            # ThÃªm thÃ´ng tin vá» báº£n gá»‘c náº¿u Ä‘Ã£ dá»‹ch
            if is_translated:
                embed2.add_field(
                    name="ğŸ”„ ThÃ´ng tin dá»‹ch thuáº­t",
                    value="ğŸ“ Ná»™i dung gá»‘c báº±ng tiáº¿ng Anh Ä‘Ã£ Ä‘Æ°á»£c dá»‹ch sang tiáº¿ng Viá»‡t báº±ng Groq AI\nğŸ’¡ Äá»ƒ xem báº£n gá»‘c, vui lÃ²ng truy cáº­p link bÃ i viáº¿t",
                    inline=False
                )
            
            embed2.add_field(
                name="ğŸ”— Äá»c bÃ i viáº¿t Ä‘áº§y Ä‘á»§",
                value=f"[Nháº¥n Ä‘á»ƒ Ä‘á»c toÃ n bá»™ bÃ i viáº¿t gá»‘c]({news['link']})",
                inline=False
            )
            
            # ThÃ´ng tin cÃ´ng nghá»‡ sá»­ dá»¥ng
            tech_info = "ğŸš€ Trafilatura" if TRAFILATURA_AVAILABLE else "ğŸ“° Legacy"
            if NEWSPAPER_AVAILABLE:
                tech_info += " + Newspaper3k"
            if is_translated:
                tech_info += " + ğŸŒ Groq AI Translation"
            
            embed2.set_footer(text=f"{tech_info} â€¢ Tá»« lá»‡nh: {user_data['command']} â€¢ Tin sá»‘ {news_number}")
            
            await ctx.send(embed=embed2)
            return
        else:
            content_title = "ğŸ“„ Ná»™i dung chi tiáº¿t ğŸŒ (ÄÃ£ dá»‹ch sang tiáº¿ng Viá»‡t)" if is_translated else "ğŸ“„ Ná»™i dung chi tiáº¿t"
            embed.add_field(
                name=content_title,
                value=content_to_display,
                inline=False
            )
        
        # ThÃªm thÃ´ng tin vá» dá»‹ch thuáº­t náº¿u cÃ³
        if is_translated:
            embed.add_field(
                name="ğŸ”„ ThÃ´ng tin dá»‹ch thuáº­t",
                value="ğŸ“ BÃ i viáº¿t gá»‘c báº±ng tiáº¿ng Anh Ä‘Ã£ Ä‘Æ°á»£c dá»‹ch sang tiáº¿ng Viá»‡t báº±ng Groq AI",
                inline=False
            )
        
        embed.add_field(
            name="ğŸ”— Äá»c bÃ i viáº¿t Ä‘áº§y Ä‘á»§",
            value=f"[Nháº¥n Ä‘á»ƒ Ä‘á»c toÃ n bá»™ bÃ i viáº¿t{'gá»‘c' if is_translated else ''}]({news['link']})",
            inline=False
        )
        
        # ThÃ´ng tin cÃ´ng nghá»‡ sá»­ dá»¥ng
        tech_info = "ğŸš€ Trafilatura" if TRAFILATURA_AVAILABLE else "ğŸ“° Legacy"
        if NEWSPAPER_AVAILABLE:
            tech_info += " + Newspaper3k"
        if is_translated:
            tech_info += " + ğŸŒ Groq AI Translation"
        
        embed.set_footer(text=f"{tech_info} â€¢ Tá»« lá»‡nh: {user_data['command']} â€¢ Tin sá»‘ {news_number} â€¢ !menu Ä‘á»ƒ xem thÃªm lá»‡nh")
        
        await ctx.send(embed=embed)
        
    except ValueError:
        await ctx.send("âŒ Vui lÃ²ng nháº­p sá»‘! VÃ­ dá»¥: `!chitiet 5`")
    except Exception as e:
        await ctx.send(f"âŒ Lá»—i: {str(e)}")

# Alias cho lá»‡nh chitiet
@bot.command(name='cuthe')
async def get_news_detail_alias(ctx, news_number: int):
    """Alias cho lá»‡nh !chitiet"""
    await get_news_detail(ctx, news_number)

@bot.command(name='menu')
async def help_command(ctx):
    """Hiá»ƒn thá»‹ menu lá»‡nh - ÄÃƒ Cáº¬P NHáº¬T Vá»šI AI"""
    embed = discord.Embed(
        title="ğŸ¤–ğŸš€ Menu News Bot",
        description="Bot tin tá»©c kinh táº¿ vá»›i AI giáº£i thÃ­ch thÃ´ng minh",
        color=0xff9900
    )
    
    embed.add_field(
        name="ğŸ“° Lá»‡nh tin tá»©c",
        value="""
**!all [trang]** - Tin tá»« táº¥t cáº£ nguá»“n (12 tin/trang)
**!in [trang]** - Tin trong nÆ°á»›c (12 tin/trang)  
**!out [trang]** - Tin quá»‘c táº¿ (12 tin/trang)
**!chitiet [sá»‘]** - Xem ná»™i dung chi tiáº¿t + ğŸŒ Tá»± Ä‘á»™ng dá»‹ch
        """,
        inline=False
    )
    
    embed.add_field(
        name="ğŸ¤– Lá»‡nh AI thÃ´ng minh",
        value="""
**!hoi [cÃ¢u há»i]** - AI tráº£ lá»i vá»›i nguá»“n tin Ä‘Ã¡ng tin cáº­y
        """,
        inline=False
    )
    
    embed.add_field(
        name="ğŸ‡»ğŸ‡³ Nguá»“n trong nÆ°á»›c (9 nguá»“n)",
        value="CafeF, CafeBiz, BÃ¡o Äáº§u tÆ°, VnEconomy, VnExpress KD, Thanh NiÃªn, NhÃ¢n DÃ¢n",
        inline=True
    )
    
    embed.add_field(
        name="ğŸŒ Nguá»“n quá»‘c táº¿ (8 nguá»“n)",
        value="Yahoo Finance, Reuters, Bloomberg, MarketWatch, Forbes, Financial Times, Business Insider, The Economist",
        inline=True
    )
    
    # Kiá»ƒm tra tráº¡ng thÃ¡i AI services
    ai_status = ""
    if GROQ_AVAILABLE:
        ai_status += "ğŸš€ **Groq AI** - Giáº£i thÃ­ch + Dá»‹ch thuáº­t thÃ´ng minh âœ…\n"
    else:
        ai_status += "âš ï¸ **Groq AI** - ChÆ°a cáº¥u hÃ¬nh\n"
    
    if GOOGLE_SEARCH_AVAILABLE:
        ai_status += "ğŸ” **Google Search** - TÃ¬m nguá»“n tin Ä‘Ã¡ng tin cáº­y âœ…\n"
    else:
        ai_status += "âš ï¸ **Google Search** - ChÆ°a cáº¥u hÃ¬nh\n"
    
    if TRAFILATURA_AVAILABLE:
        ai_status += "ğŸš€ **Trafilatura** - TrÃ­ch xuáº¥t ná»™i dung 94.5% âœ…\n"
    else:
        ai_status += "âš ï¸ **Trafilatura** - ChÆ°a cÃ i\n"
    
    if NEWSPAPER_AVAILABLE:
        ai_status += "ğŸ“° **Newspaper3k** - Fallback extraction âœ…"
    else:
        ai_status += "âš ï¸ **Newspaper3k** - ChÆ°a cÃ i"
    
    embed.add_field(
        name="ğŸš€ CÃ´ng nghá»‡ tÃ­ch há»£p",
        value=ai_status,
        inline=False
    )
    
    embed.add_field(
        name="ğŸ’¡ VÃ­ dá»¥ sá»­ dá»¥ng AI",
        value="""
`!hoi láº¡m phÃ¡t lÃ  gÃ¬` - Há»i vá» láº¡m phÃ¡t
`!hoi GDP nghÄ©a lÃ  gÃ¬` - TÃ¬m hiá»ƒu vá» tá»•ng sáº£n pháº©m quá»‘c ná»™i
`!hoi blockchain lÃ  gÃ¬` - Há»i vá» cÃ´ng nghá»‡ blockchain
`!hoi chá»©ng khoÃ¡n hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o` - Há»i vá» thá»‹ trÆ°á»ng chá»©ng khoÃ¡n
        """,
        inline=False
    )
    
    embed.add_field(
        name="ğŸ“‹ HÆ°á»›ng dáº«n sá»­ dá»¥ng",
        value="""
1ï¸âƒ£ **Xem tin**: GÃµ **!all** Ä‘á»ƒ xem tin má»›i nháº¥t
2ï¸âƒ£ **Chi tiáº¿t**: GÃµ **!chitiet [sá»‘]** - tin nÆ°á»›c ngoÃ i tá»± Ä‘á»™ng dá»‹ch ğŸŒ
3ï¸âƒ£ **Há»i AI**: GÃµ **!hoi [cÃ¢u há»i]** Ä‘á»ƒ AI tráº£ lá»i
4ï¸âƒ£ **PhÃ¢n trang**: DÃ¹ng **!all 2**, **!all 3** cho trang tiáº¿p theo
        """,
        inline=False
    )
    
    if not GROQ_AVAILABLE or not GOOGLE_SEARCH_AVAILABLE:
        embed.add_field(
            name="âš™ï¸ Cáº¥u hÃ¬nh AI (cho admin)",
            value="""
Äá»ƒ kÃ­ch hoáº¡t AI, thÃªm vÃ o Environment Variables:
â€¢ **GROQ_API_KEY** - ÄÄƒng kÃ½ miá»…n phÃ­ táº¡i groq.com
â€¢ **GOOGLE_API_KEY** - Láº¥y tá»« Google Cloud Console
â€¢ **GOOGLE_CSE_ID** - Táº¡o Custom Search Engine
            """,
            inline=False
        )
    
    embed.set_footer(text="ğŸ¤– Bot vá»›i AI thÃ´ng minh â€¢ ğŸŒ Tá»± Ä‘á»™ng dá»‹ch tin nÆ°á»›c ngoÃ i â€¢ MÃºi giá» VN chÃ­nh xÃ¡c â€¢ Groq + Google Search")
    await ctx.send(embed=embed)

# Cháº¡y bot vá»›i error handling tá»‘t hÆ¡n
if __name__ == "__main__":
    try:
        print("ğŸš€ Äang khá»Ÿi Ä‘á»™ng News Bot cáº£i tiáº¿n...")
# Khá»Ÿi Ä‘á»™ng web server Ä‘á»ƒ keep alive
        keep_alive()
        print("ğŸ”‘ Äang kiá»ƒm tra token tá»« Environment Variables...")
        
        if TOKEN:
            print("âœ… Token Ä‘Ã£ Ä‘Æ°á»£c táº£i tá»« Environment Variables")
        
        total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
        print(f"ğŸ“Š ÄÃ£ load {total_sources} nguá»“n RSS ÄÃƒ KIá»‚M TRA")
        print(f"ğŸ‡»ğŸ‡³ Trong nÆ°á»›c: {len(RSS_FEEDS['domestic'])} nguá»“n")
        print(f"ğŸŒ Quá»‘c táº¿: {len(RSS_FEEDS['international'])} nguá»“n")
        print("ğŸ¯ LÄ©nh vá»±c: Kinh táº¿, Chá»©ng khoÃ¡n, VÄ© mÃ´, Báº¥t Ä‘á»™ng sáº£n")
        print("ğŸ•°ï¸ MÃºi giá»: ÄÃ£ sá»­a lá»—i - Hiá»ƒn thá»‹ chÃ­nh xÃ¡c giá» Viá»‡t Nam")
        
        if TRAFILATURA_AVAILABLE:
            print("ğŸš€ Trafilatura: Sáºµn sÃ ng - TrÃ­ch xuáº¥t ná»™i dung 94.5% Ä‘á»™ chÃ­nh xÃ¡c")
        else:
            print("âš ï¸ Trafilatura: ChÆ°a cÃ i Ä‘áº·t - Sáº½ sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p cÅ©")
            
        if NEWSPAPER_AVAILABLE:
            print("ğŸ“° Newspaper3k: Sáºµn sÃ ng - Fallback extraction")
        else:
            print("âš ï¸ Newspaper3k: ChÆ°a cÃ i Ä‘áº·t - Chá»‰ dÃ¹ng Trafilatura")
        
        # ThÃ´ng tin AI Services
        if GROQ_AVAILABLE:
            print("ğŸ¤– Groq AI: Sáºµn sÃ ng - AI giáº£i thÃ­ch + dá»‹ch thuáº­t thÃ´ng minh (1000 calls/ngÃ y)")
        else:
            print("âš ï¸ Groq AI: ChÆ°a cáº¥u hÃ¬nh - Thiáº¿u GROQ_API_KEY")
            
        if GOOGLE_SEARCH_AVAILABLE:
            print("ğŸ” Google Search: Sáºµn sÃ ng - TÃ¬m nguá»“n tin Ä‘Ã¡ng tin cáº­y (100 queries/ngÃ y)")
        else:
            print("âš ï¸ Google Search: ChÆ°a cáº¥u hÃ¬nh - Thiáº¿u GOOGLE_API_KEY hoáº·c GOOGLE_CSE_ID")
        
        print("âœ… Bot sáºµn sÃ ng nháº­n lá»‡nh!")
        print("ğŸ’¡ Lá»‡nh AI: !hoi [cÃ¢u há»i] - AI tráº£ lá»i vá»›i nguá»“n tin Ä‘Ã¡ng tin cáº­y")
        print("ğŸŒ TÃ­nh nÄƒng má»›i: !chitiet tá»± Ä‘á»™ng dá»‹ch tin nÆ°á»›c ngoÃ i sang tiáº¿ng Viá»‡t")
        
        bot.run(TOKEN)
        
    except discord.LoginFailure:
        print("âŒ Lá»—i Ä‘Äƒng nháº­p Discord!")
        print("ğŸ”§ Token cÃ³ thá»ƒ khÃ´ng há»£p lá»‡ hoáº·c Ä‘Ã£ bá»‹ reset")
        print("ğŸ”§ Kiá»ƒm tra DISCORD_TOKEN trong Environment Variables")
        
    except Exception as e:
        print(f"âŒ Lá»—i khi cháº¡y bot: {e}")
        print("ğŸ”§ Kiá»ƒm tra káº¿t ná»‘i internet vÃ  Environment Variables")
        
    input("Nháº¥n Enter Ä‘á»ƒ thoÃ¡t...")
