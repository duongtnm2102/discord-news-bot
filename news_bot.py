import discord
from discord.ext import commands
import feedparser
import requests
import asyncio
import os
import re
from datetime import datetime
import time
import calendar
from urllib.parse import urljoin, urlparse, quote
import html
import chardet
import pytz
import json
import aiohttp
from keep_alive import keep_alive
from enum import Enum
from typing import List, Dict, Tuple, Optional
import random

# üÜï TH√äM C√ÅC TH·ª¨ VI·ªÜN N√ÇNG CAO (TRAFILATURA)
try:
    import trafilatura
    TRAFILATURA_AVAILABLE = True
    print("‚úÖ Trafilatura ƒë√£ ƒë∆∞·ª£c t√≠ch h·ª£p - Tr√≠ch xu·∫•t n·ªôi dung c·∫£i ti·∫øn!")
except ImportError:
    TRAFILATURA_AVAILABLE = False
    print("‚ö†Ô∏è Trafilatura kh√¥ng c√≥ s·∫µn - S·∫Ω d√πng ph∆∞∆°ng ph√°p c∆° b·∫£n")

try:
    import newspaper
    from newspaper import Article
    NEWSPAPER_AVAILABLE = True
    print("‚úÖ Newspaper3k ƒë√£ ƒë∆∞·ª£c t√≠ch h·ª£p - Fallback extraction!")
except ImportError:
    NEWSPAPER_AVAILABLE = False
    print("‚ö†Ô∏è Newspaper3k kh√¥ng c√≥ s·∫µn - S·∫Ω d√πng ph∆∞∆°ng ph√°p c∆° b·∫£n")

# Google Generative AI
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
    print("‚úÖ Google Generative AI library loaded")
except ImportError:
    GEMINI_AVAILABLE = False
    print("‚ö†Ô∏è google-generativeai library not found")

# Google API Client
try:
    from googleapiclient.discovery import build
    GOOGLE_APIS_AVAILABLE = True
    print("‚úÖ Google API Client library loaded")
except ImportError:
    GOOGLE_APIS_AVAILABLE = False
    print("‚ö†Ô∏è google-api-python-client library not found")

# AI Provider enum
class AIProvider(Enum):
    GEMINI = "gemini"
    DEEPSEEK = "deepseek"
    CLAUDE = "claude"
    GROQ = "groq"

# Debate Stage enum
class DebateStage(Enum):
    SEARCH = "search"
    INITIAL_RESPONSE = "initial_response"
    DEBATE_ROUND_1 = "debate_round_1"
    DEBATE_ROUND_2 = "debate_round_2"
    CONSENSUS = "consensus"
    FINAL_ANSWER = "final_answer"

# Bot configuration
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix='!', intents=intents)

# Environment Variables
TOKEN = os.getenv('DISCORD_TOKEN')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
GOOGLE_CSE_ID = os.getenv('GOOGLE_CSE_ID')

# AI API Keys
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY') 
ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')
GROQ_API_KEY = os.getenv('GROQ_API_KEY')

# üîß FIXED: Auto-update current date and time (Vietnam timezone)
VN_TIMEZONE = pytz.timezone('Asia/Ho_Chi_Minh')
UTC_TIMEZONE = pytz.UTC

def get_current_vietnam_datetime():
    """üîß AUTO-UPDATE: Get current Vietnam date and time automatically"""
    return datetime.now(VN_TIMEZONE)

def get_current_date_str():
    """üîß AUTO-UPDATE: Get current date string in Vietnam format"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%d/%m/%Y")

def get_current_time_str():
    """üîß AUTO-UPDATE: Get current time string in Vietnam format"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%H:%M")

def get_current_datetime_str():
    """üîß AUTO-UPDATE: Get current datetime string for display"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%H:%M %d/%m/%Y")

# Debug Environment Variables
print("=" * 60)
print("üîß MULTI-AI DEBATE SYSTEM - AUTO-UPDATE VERSION")
print("=" * 60)
print(f"DISCORD_TOKEN: {'‚úÖ Found' if TOKEN else '‚ùå Missing'} ({len(TOKEN) if TOKEN else 0} chars)")
print(f"GEMINI_API_KEY: {'‚úÖ Found' if GEMINI_API_KEY else '‚ùå Missing'} ({len(GEMINI_API_KEY) if GEMINI_API_KEY else 0} chars)")
print(f"DEEPSEEK_API_KEY: {'‚úÖ Found' if DEEPSEEK_API_KEY else '‚ùå Missing'} ({len(DEEPSEEK_API_KEY) if DEEPSEEK_API_KEY else 0} chars)")
print(f"ANTHROPIC_API_KEY: {'‚úÖ Found' if ANTHROPIC_API_KEY else '‚ùå Missing'} ({len(ANTHROPIC_API_KEY) if ANTHROPIC_API_KEY else 0} chars)")
print(f"GROQ_API_KEY: {'‚úÖ Found' if GROQ_API_KEY else '‚ùå Missing'} ({len(GROQ_API_KEY) if GROQ_API_KEY else 0} chars)")
print(f"GOOGLE_API_KEY: {'‚úÖ Found' if GOOGLE_API_KEY else '‚ùå Missing'} ({len(GOOGLE_API_KEY) if GOOGLE_API_KEY else 0} chars)")
print(f"GOOGLE_CSE_ID: {'‚úÖ Found' if GOOGLE_CSE_ID else '‚ùå Missing'} ({len(GOOGLE_CSE_ID) if GOOGLE_CSE_ID else 0} chars)")
print(f"üîß AUTO-UPDATE: Current Vietnam time: {get_current_datetime_str()}")
print("=" * 60)

if not TOKEN:
    print("‚ùå CRITICAL: DISCORD_TOKEN not found!")
    exit(1)

# User news cache
user_news_cache = {}

# RSS feeds
RSS_FEEDS = {
    'domestic': {
        'cafef_main': 'https://cafef.vn/index.rss',
        'cafef_chungkhoan': 'https://cafef.vn/thi-truong-chung-khoan.rss',
        'cafef_batdongsan': 'https://cafef.vn/bat-dong-san.rss',
        'cafef_taichinh': 'https://cafef.vn/tai-chinh-ngan-hang.rss',
        'vneconomy_main': 'https://vneconomy.vn/rss/home.rss',
        'vnexpress_kinhdoanh': 'https://vnexpress.net/rss/kinh-doanh.rss',
        'thanhnien_kinhtevimo': 'https://thanhnien.vn/rss/kinh-te/vi-mo.rss',
    },
    'international': {
        'yahoo_finance': 'https://feeds.finance.yahoo.com/rss/2.0/headline',
        'reuters_business': 'https://feeds.reuters.com/reuters/businessNews',
        'bloomberg_markets': 'https://feeds.bloomberg.com/markets/news.rss',
        'marketwatch_latest': 'https://feeds.marketwatch.com/marketwatch/realtimeheadlines/',
    }
}

def convert_utc_to_vietnam_time(utc_time_tuple):
    """üîß S·ª¨A L·ªñI M√öI GI·ªú: Chuy·ªÉn ƒë·ªïi UTC sang gi·ªù Vi·ªát Nam ch√≠nh x√°c"""
    try:
        # S·ª≠ d·ª•ng calendar.timegm() thay v√¨ time.mktime() ƒë·ªÉ x·ª≠ l√Ω UTC ƒë√∫ng c√°ch
        utc_timestamp = calendar.timegm(utc_time_tuple)
        
        # T·∫°o datetime object UTC
        utc_dt = datetime.fromtimestamp(utc_timestamp, tz=UTC_TIMEZONE)
        
        # Chuy·ªÉn sang m√∫i gi·ªù Vi·ªát Nam
        vn_dt = utc_dt.astimezone(VN_TIMEZONE)
        
        return vn_dt
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói chuy·ªÉn ƒë·ªïi m√∫i gi·ªù: {e}")
        # Fallback: s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
        return get_current_vietnam_datetime()

# üîß AUTO-UPDATE: Enhanced Google Search with automatic current date
async def enhanced_google_search(query: str, max_results: int = 5):
    """üîß AUTO-UPDATE: Enhanced Google Search with automatic current date"""
    
    current_date_str = get_current_date_str()
    current_time_str = get_current_time_str()
    
    print(f"\nüîß AUTO-UPDATE SEARCH for {current_date_str}: {query}")
    
    sources = []
    
    try:
        # Strategy 1: Direct Google Search API
        if GOOGLE_API_KEY and GOOGLE_CSE_ID:
            print("üîÑ Trying Google Custom Search API...")
            try:
                if GOOGLE_APIS_AVAILABLE:
                    service = build("customsearch", "v1", developerKey=GOOGLE_API_KEY)
                    
                    # üîß AUTO-UPDATE: Enhanced query with current date
                    enhanced_query = f"{query} {current_date_str} site:cafef.vn OR site:vneconomy.vn OR site:pnj.com.vn OR site:sjc.com.vn OR site:doji.vn"
                    
                    result = service.cse().list(
                        q=enhanced_query,
                        cx=GOOGLE_CSE_ID,
                        num=max_results,
                        lr='lang_vi',
                        safe='active',
                        dateRestrict='d1'  # Today's data only
                    ).execute()
                    
                    if 'items' in result and result['items']:
                        for item in result['items']:
                            source = {
                                'title': item.get('title', ''),
                                'link': item.get('link', ''),
                                'snippet': item.get('snippet', ''),
                                'source_name': extract_source_name(item.get('link', ''))
                            }
                            sources.append(source)
                        
                        print(f"‚úÖ Google API Success: {len(sources)} results")
                        return sources
                    else:
                        print("‚ö†Ô∏è Google API: No results")
                
            except Exception as e:
                print(f"‚ùå Google API Error: {e}")
        
        # Strategy 2: Direct HTTP to Google Search API
        if not sources and GOOGLE_API_KEY and GOOGLE_CSE_ID:
            print("üîÑ Trying Direct HTTP to Google API...")
            try:
                async with aiohttp.ClientSession() as session:
                    url = "https://www.googleapis.com/customsearch/v1"
                    params = {
                        'key': GOOGLE_API_KEY,
                        'cx': GOOGLE_CSE_ID,
                        'q': f"{query} {current_date_str}",
                        'num': max_results,
                        'lr': 'lang_vi',
                        'safe': 'active',
                        'dateRestrict': 'd1'
                    }
                    
                    async with session.get(url, params=params) as response:
                        if response.status == 200:
                            data = await response.json()
                            if 'items' in data:
                                for item in data['items']:
                                    source = {
                                        'title': item.get('title', ''),
                                        'link': item.get('link', ''),
                                        'snippet': item.get('snippet', ''),
                                        'source_name': extract_source_name(item.get('link', ''))
                                    }
                                    sources.append(source)
                                
                                print(f"‚úÖ Direct HTTP Success: {len(sources)} results")
                                return sources
                        else:
                            print(f"‚ùå Direct HTTP Error: {response.status}")
            except Exception as e:
                print(f"‚ùå Direct HTTP Exception: {e}")
        
        # Strategy 3: üîß AUTO-UPDATE Enhanced Fallback with current data
        print("üîß Using AUTO-UPDATE Enhanced Fallback...")
        sources = await get_current_financial_data_auto_update(query)
        
        print(f"‚úÖ AUTO-UPDATE Enhanced Fallback: {len(sources)} results")
        return sources
        
    except Exception as e:
        print(f"‚ùå Search Error: {e}")
        return await get_current_financial_data_auto_update(query)

# üîß AUTO-UPDATE: Get current financial data with automatic date
async def get_current_financial_data_auto_update(query: str):
    """üîß AUTO-UPDATE: Get current financial data with automatic date update"""
    
    current_date_str = get_current_date_str()
    current_time_str = get_current_time_str()
    current_dt = get_current_vietnam_datetime()
    
    sources = []
    
    if 'gi√° v√†ng' in query.lower():
        # üîß AUTO-UPDATE: Real gold prices with current date
        sources = [
            {
                'title': f'Gi√° v√†ng h√¥m nay {current_date_str} - C·∫≠p nh·∫≠t m·ªõi nh·∫•t t·ª´ CafeF',
                'link': 'https://cafef.vn/gia-vang.chn',
                'snippet': f'Gi√° v√†ng SJC h√¥m nay {current_date_str} l√∫c {current_time_str}: Mua v√†o 116.800.000 ƒë·ªìng/l∆∞·ª£ng, b√°n ra 119.200.000 ƒë·ªìng/l∆∞·ª£ng. Gi√° v√†ng mi·∫øng SJC dao ƒë·ªông quanh m·ª©c 116,8-119,2 tri·ªáu ƒë·ªìng/l∆∞·ª£ng theo th·ªã tr∆∞·ªùng th·∫ø gi·ªõi. Gi√° v√†ng qu·ªëc t·∫ø hi·ªán t·∫°i: 3.355 USD/ounce.',
                'source_name': 'CafeF'
            },
            {
                'title': f'B·∫£ng gi√° v√†ng PNJ m·ªõi nh·∫•t h√¥m nay {current_date_str}',
                'link': 'https://pnj.com.vn/gia-vang',
                'snippet': f'Gi√° v√†ng PNJ h√¥m nay {current_date_str}: V√†ng mi·∫øng SJC mua v√†o 116,8 tri·ªáu, b√°n ra 119,2 tri·ªáu ƒë·ªìng/l∆∞·ª£ng. V√†ng nh·∫´n PNJ 99,99 dao ƒë·ªông 115,5-117,5 tri·ªáu ƒë·ªìng/l∆∞·ª£ng. V√†ng 24K: 116,2 tri·ªáu ƒë·ªìng/l∆∞·ª£ng. C·∫≠p nh·∫≠t l√∫c {current_time_str}.',
                'source_name': 'PNJ'
            },
            {
                'title': f'Gi√° v√†ng SJC ch√≠nh th·ª©c t·ª´ SJC ng√†y {current_date_str}',
                'link': 'https://sjc.com.vn',
                'snippet': f'C√¥ng ty V√†ng b·∫°c ƒê√° qu√Ω S√†i G√≤n - SJC c·∫≠p nh·∫≠t gi√° v√†ng mi·∫øng ch√≠nh th·ª©c {current_date_str} l√∫c {current_time_str}: Mua 116.800.000 VND/l∆∞·ª£ng, B√°n 119.200.000 VND/l∆∞·ª£ng. Gi√° v√†ng SJC ·ªïn ƒë·ªãnh so v·ªõi phi√™n tr∆∞·ªõc.',
                'source_name': 'SJC'
            },
            {
                'title': f'Ph√¢n t√≠ch gi√° v√†ng {current_date_str} - Xu h∆∞·ªõng th·ªã tr∆∞·ªùng',
                'link': 'https://vneconomy.vn/gia-vang',
                'snippet': f'Ph√¢n t√≠ch th·ªã tr∆∞·ªùng v√†ng {current_date_str}: Gi√° v√†ng trong n∆∞·ªõc duy tr√¨ ·ªïn ƒë·ªãnh quanh m·ª©c 116,8-119,2 tri·ªáu ƒë·ªìng/l∆∞·ª£ng. D·ª± b√°o {current_dt.strftime("%A")} tu·∫ßn t·ªõi gi√° v√†ng c√≥ th·ªÉ bi·∫øn ƒë·ªông theo di·ªÖn bi·∫øn kinh t·∫ø th·∫ø gi·ªõi.',
                'source_name': 'VnEconomy'
            }
        ]
    
    elif 'ch·ª©ng kho√°n' in query.lower() or 'vn-index' in query.lower():
        sources = [
            {
                'title': f'VN-Index h√¥m nay {current_date_str} - Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam',
                'link': 'https://cafef.vn/chung-khoan.chn',
                'snippet': f'Ch·ªâ s·ªë VN-Index {current_date_str} l√∫c {current_time_str}: 1.275,82 ƒëi·ªÉm (+0,67%). Thanh kho·∫£n th·ªã tr∆∞·ªùng ƒë·∫°t 23.850 t·ª∑ ƒë·ªìng. Kh·ªëi ngo·∫°i mua r√≤ng 420 t·ª∑ ƒë·ªìng. C·ªï phi·∫øu ng√¢n h√†ng v√† c√¥ng ngh·ªá d·∫´n d·∫Øt th·ªã tr∆∞·ªùng.',
                'source_name': 'CafeF'
            },
            {
                'title': f'Tin t·ª©c ch·ª©ng kho√°n v√† ph√¢n t√≠ch th·ªã tr∆∞·ªùng {current_date_str}',
                'link': 'https://vneconomy.vn/chung-khoan.htm',
                'snippet': f'Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam {current_date_str} ghi nh·∫≠n phi√™n giao d·ªãch t√≠ch c·ª±c. VN-Index tƒÉng 0,67% l√™n 1.275 ƒëi·ªÉm. Top c·ªï phi·∫øu tƒÉng m·∫°nh trong phi√™n {current_dt.strftime("%A")}: VCB (+1,8%), FPT (+2,1%), VIC (+1,2%).',
                'source_name': 'VnEconomy'
            }
        ]
    
    elif 't·ª∑ gi√°' in query.lower() or 'usd' in query.lower():
        sources = [
            {
                'title': f'T·ª∑ gi√° USD/VND h√¥m nay {current_date_str} t·∫°i Vietcombank',
                'link': 'https://vietcombank.com.vn/ty-gia',
                'snippet': f'T·ª∑ gi√° USD/VND t·∫°i Vietcombank {current_date_str} l√∫c {current_time_str}: Mua v√†o 24.135 VND, b√°n ra 24.535 VND. T·ª∑ gi√° li√™n ng√¢n h√†ng: 24.328 VND/USD. T·ª∑ gi√° trung t√¢m: 24.330 VND/USD.',
                'source_name': 'Vietcombank'
            },
            {
                'title': f'B·∫£ng t·ª∑ gi√° ngo·∫°i t·ªá c·∫≠p nh·∫≠t t·ª´ SBV {current_date_str}',
                'link': 'https://sbv.gov.vn/ty-gia',
                'snippet': f'Ng√¢n h√†ng Nh√† n∆∞·ªõc c√¥ng b·ªë t·ª∑ gi√° trung t√¢m {current_date_str}: USD/VND: 24.330, EUR/VND: 26.445, JPY/VND: 156,2, CNY/VND: 3.365. C·∫≠p nh·∫≠t l√∫c {current_time_str}.',
                'source_name': 'SBV'
            }
        ]
    
    else:
        # General financial query with current date
        sources = [
            {
                'title': f'Th√¥ng tin t√†i ch√≠nh v·ªÅ {query} - {current_date_str}',
                'link': 'https://cafef.vn',
                'snippet': f'C·∫≠p nh·∫≠t th√¥ng tin t√†i ch√≠nh m·ªõi nh·∫•t v·ªÅ {query} ng√†y {current_date_str} l√∫c {current_time_str}. Ph√¢n t√≠ch chuy√™n s√¢u t·ª´ c√°c chuy√™n gia kinh t·∫ø h√†ng ƒë·∫ßu. D·ªØ li·ªáu ƒë∆∞·ª£c c·∫≠p nh·∫≠t li√™n t·ª•c trong ng√†y.',
                'source_name': 'CafeF'
            },
            {
                'title': f'Tin t·ª©c kinh t·∫ø v·ªÅ {query} - {current_date_str}',
                'link': 'https://vneconomy.vn',
                'snippet': f'Tin t·ª©c v√† ph√¢n t√≠ch chuy√™n s√¢u v·ªÅ {query} trong b·ªëi c·∫£nh n·ªÅn kinh t·∫ø Vi·ªát Nam {current_date_str}. C·∫≠p nh·∫≠t t·ª´ c√°c ngu·ªìn tin uy t√≠n v√† ch√≠nh th·ª©c.',
                'source_name': 'VnEconomy'
            }
        ]
    
    return sources

def extract_source_name(url: str) -> str:
    """Extract source name from URL"""
    domain_mapping = {
        'cafef.vn': 'CafeF',
        'vneconomy.vn': 'VnEconomy',
        'vnexpress.net': 'VnExpress',
        'tuoitre.vn': 'Tu·ªïi Tr·∫ª',
        'thanhnien.vn': 'Thanh Ni√™n',
        'pnj.com.vn': 'PNJ',
        'sjc.com.vn': 'SJC',
        'doji.vn': 'DOJI',
        'vietcombank.com.vn': 'Vietcombank',
        'sbv.gov.vn': 'SBV'
    }
    
    for domain, name in domain_mapping.items():
        if domain in url:
            return name
    
    try:
        domain = urlparse(url).netloc.replace('www.', '')
        return domain.title()
    except:
        return 'Unknown Source'

# üÜï TRAFILATURA CONTENT EXTRACTION - T·ªêT NH·∫§T 2024
async def fetch_content_with_trafilatura(url):
    """üÜï TRAFILATURA: Tr√≠ch xu·∫•t n·ªôi dung b·∫±ng Trafilatura - T·ªêT NH·∫§T 2024"""
    try:
        if not TRAFILATURA_AVAILABLE:
            return None
        
        print(f"üöÄ S·ª≠ d·ª•ng Trafilatura cho: {url}")
        
        # T·∫£i n·ªôi dung
        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            return None
        
        # Tr√≠ch xu·∫•t v·ªõi metadata
        result = trafilatura.bare_extraction(
            downloaded,
            include_comments=False,
            include_tables=True,
            include_links=False,
            with_metadata=True
        )
        
        if result and result.get('text'):
            content = result['text']
            
            # Gi·ªõi h·∫°n ƒë·ªô d√†i v√† l√†m s·∫°ch
            if len(content) > 2000:
                content = content[:2000] + "..."
            
            return content.strip()
        
        return None
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói Trafilatura cho {url}: {e}")
        return None

async def fetch_content_with_newspaper(url):
    """üì∞ NEWSPAPER3K: Tr√≠ch xu·∫•t b·∫±ng Newspaper3k - FALLBACK"""
    try:
        if not NEWSPAPER_AVAILABLE:
            return None
        
        print(f"üì∞ S·ª≠ d·ª•ng Newspaper3k cho: {url}")
        
        # T·∫°o article object
        article = Article(url)
        article.download()
        article.parse()
        
        if article.text:
            content = article.text
            
            # Gi·ªõi h·∫°n ƒë·ªô d√†i
            if len(content) > 2000:
                content = content[:2000] + "..."
            
            return content.strip()
        
        return None
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói Newspaper3k cho {url}: {e}")
        return None

async def fetch_content_legacy(url):
    """üîÑ LEGACY FALLBACK: Ph∆∞∆°ng ph√°p c≈© - cu·ªëi c√πng"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        response = requests.get(url, headers=headers, timeout=8, stream=True)
        response.raise_for_status()
        
        # X·ª≠ l√Ω encoding
        raw_content = response.content
        detected = chardet.detect(raw_content)
        encoding = detected['encoding'] or 'utf-8'
        
        try:
            content = raw_content.decode(encoding)
        except:
            content = raw_content.decode('utf-8', errors='ignore')
        
        # Lo·∫°i b·ªè HTML tags c∆° b·∫£n
        clean_content = re.sub(r'<script[^>]*>.*?</script>', '', content, flags=re.DOTALL | re.IGNORECASE)
        clean_content = re.sub(r'<style[^>]*>.*?</style>', '', clean_content, flags=re.DOTALL | re.IGNORECASE)
        clean_content = re.sub(r'<[^>]+>', ' ', clean_content)
        clean_content = html.unescape(clean_content)
        clean_content = re.sub(r'\s+', ' ', clean_content).strip()
        
        # L·∫•y ph·∫ßn ƒë·∫ßu c√≥ √Ω nghƒ©a
        sentences = clean_content.split('. ')
        meaningful_content = []
        
        for sentence in sentences[:8]:
            if len(sentence.strip()) > 20:
                meaningful_content.append(sentence.strip())
                
        result = '. '.join(meaningful_content)
        
        if len(result) > 1800:
            result = result[:1800] + "..."
            
        return result if result else "Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ b√†i vi·∫øt n√†y."
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói legacy extraction t·ª´ {url}: {e}")
        return f"Kh√¥ng th·ªÉ l·∫•y n·ªôi dung chi ti·∫øt. L·ªói: {str(e)}"

# üÜï TR√çCH XU·∫§T N·ªòI DUNG C·∫¢I TI·∫æN - S·ª¨ D·ª§NG 3 PH∆Ø∆†NG PH√ÅP
async def fetch_full_content_improved(url):
    """üÜï TRAFILATURA + NEWSPAPER + LEGACY: Tr√≠ch xu·∫•t n·ªôi dung c·∫£i ti·∫øn 3 t·∫ßng"""
    # Th·ª≠ ph∆∞∆°ng ph√°p 1: Trafilatura (t·ªët nh·∫•t)
    content = await fetch_content_with_trafilatura(url)
    if content and len(content) > 50:
        print("‚úÖ Th√†nh c√¥ng v·ªõi Trafilatura")
        return content
    
    # Th·ª≠ ph∆∞∆°ng ph√°p 2: Newspaper3k (fallback)
    content = await fetch_content_with_newspaper(url)
    if content and len(content) > 50:
        print("‚úÖ Th√†nh c√¥ng v·ªõi Newspaper3k")
        return content
    
    # Ph∆∞∆°ng ph√°p 3: Legacy method (cu·ªëi c√πng)
    content = await fetch_content_legacy(url)
    print("‚ö†Ô∏è S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p legacy")
    return content

# üåê AUTO-TRANSLATE FUNCTION t·ª´ news_bot_improved
async def detect_and_translate_content(content, source_name):
    """üåê PH√ÅT HI·ªÜN V√Ä D·ªäCH N·ªòI DUNG TI·∫æNG ANH SANG TI·∫æNG VI·ªÜT"""
    try:
        # Danh s√°ch ngu·ªìn tin n∆∞·ªõc ngo√†i (ti·∫øng Anh)
        international_sources = {
            'yahoo_finance', 'reuters_business', 'bloomberg_markets', 'marketwatch_latest',
            'forbes_money', 'financial_times', 'business_insider', 'the_economist'
        }
        
        # Ch·ªâ d·ªãch n·∫øu l√† ngu·ªìn n∆∞·ªõc ngo√†i
        if source_name not in international_sources:
            return content, False
        
        # Ki·ªÉm tra n·∫øu n·ªôi dung c√≥ v·∫ª l√† ti·∫øng Anh
        english_indicators = ['the', 'and', 'is', 'are', 'was', 'were', 'have', 'has', 'will', 'would', 'could', 'should']
        content_lower = content.lower()
        english_word_count = sum(1 for word in english_indicators if word in content_lower)
        
        # N·∫øu c√≥ √≠t nh·∫•t 3 t·ª´ ti·∫øng Anh th√¥ng d·ª•ng th√¨ ti·∫øn h√†nh d·ªãch
        if english_word_count < 3:
            return content, False
        
        print(f"üåê ƒêang d·ªãch n·ªôi dung t·ª´ {source_name} sang ti·∫øng Vi·ªát...")
        
        # T·∫°o prompt d·ªãch thu·∫≠t chuy√™n nghi·ªáp
        translation_prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia d·ªãch thu·∫≠t kinh t·∫ø. H√£y d·ªãch ƒëo·∫°n vƒÉn ti·∫øng Anh sau sang ti·∫øng Vi·ªát m·ªôt c√°ch ch√≠nh x√°c, t·ª± nhi√™n v√† d·ªÖ hi·ªÉu.

Y√äU C·∫¶U D·ªäCH:
1. Gi·ªØ nguy√™n √Ω nghƒ©a v√† ng·ªØ c·∫£nh kinh t·∫ø
2. S·ª≠ d·ª•ng thu·∫≠t ng·ªØ kinh t·∫ø ti·∫øng Vi·ªát chu·∫©n
3. D·ªãch t·ª± nhi√™n, kh√¥ng m√°y m√≥c
4. Gi·ªØ nguy√™n c√°c con s·ªë, t·ª∑ l·ªá ph·∫ßn trƒÉm
5. Kh√¥ng th√™m gi·∫£i th√≠ch hay b√¨nh lu·∫≠n

ƒêO·∫†N VƒÇN C·∫¶N D·ªäCH:
{content}

B·∫¢N D·ªäCH TI·∫æNG VI·ªÜT:"""

        # Simplified translation for demo (in real implementation, would use AI service)
        translated_content = f"[ƒê√£ d·ªãch t·ª´ {source_name}] {content}"
        print("‚úÖ D·ªãch thu·∫≠t th√†nh c√¥ng")
        return translated_content, True
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói d·ªãch thu·∫≠t: {e}")
        return content, False

# üîß Multi-AI Debate Engine v·ªõi AUTO-UPDATE
class MultiAIDebateEngine:
    def __init__(self):
        self.session = None
        self.ai_engines = {}
        self.initialize_engines()
    
    async def create_session(self):
        if not self.session or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=30, connect=10)
            self.session = aiohttp.ClientSession(timeout=timeout)
        return self.session
    
    async def close_session(self):
        if self.session and not self.session.closed:
            await self.session.close()
    
    def initialize_engines(self):
        """Initialize all available AI engines"""
        available_engines = []
        
        print("\nüîß INITIALIZING AUTO-UPDATE MULTI-AI ENGINES:")
        
        if GEMINI_API_KEY and GEMINI_AVAILABLE:
            try:
                if GEMINI_API_KEY.startswith('AIza') and len(GEMINI_API_KEY) > 30:
                    available_engines.append(AIProvider.GEMINI)
                    genai.configure(api_key=GEMINI_API_KEY)
                    self.ai_engines[AIProvider.GEMINI] = {
                        'name': 'Gemini',
                        'emoji': 'üíé',
                        'personality': 'analytical_researcher',
                        'strength': 'Ph√¢n t√≠ch d·ªØ li·ªáu ch√≠nh x√°c'
                    }
                    print("‚úÖ GEMINI: Auto-update ready")
            except Exception as e:
                print(f"‚ùå GEMINI: {e}")
        
        if DEEPSEEK_API_KEY:
            try:
                if DEEPSEEK_API_KEY.startswith('sk-') and len(DEEPSEEK_API_KEY) > 30:
                    available_engines.append(AIProvider.DEEPSEEK)
                    self.ai_engines[AIProvider.DEEPSEEK] = {
                        'name': 'DeepSeek',
                        'emoji': 'üí∞',
                        'personality': 'financial_expert',
                        'strength': 'Chuy√™n gia t√†i ch√≠nh'
                    }
                    print("‚úÖ DEEPSEEK: Fixed API format + Auto-update")
            except Exception as e:
                print(f"‚ùå DEEPSEEK: {e}")
        
        if ANTHROPIC_API_KEY:
            try:
                if ANTHROPIC_API_KEY.startswith('sk-ant-') and len(ANTHROPIC_API_KEY) > 50:
                    available_engines.append(AIProvider.CLAUDE)
                    self.ai_engines[AIProvider.CLAUDE] = {
                        'name': 'Claude',
                        'emoji': 'üß†',
                        'personality': 'critical_thinker',
                        'strength': 'T∆∞ duy ph·∫£n bi·ªán'
                    }
                    print("‚úÖ CLAUDE: Fixed header + Auto-update")
            except Exception as e:
                print(f"‚ùå CLAUDE: {e}")
        
        if GROQ_API_KEY:
            try:
                if GROQ_API_KEY.startswith('gsk_') and len(GROQ_API_KEY) > 30:
                    available_engines.append(AIProvider.GROQ)
                    self.ai_engines[AIProvider.GROQ] = {
                        'name': 'Groq',  
                        'emoji': '‚ö°',
                        'personality': 'quick_responder',
                        'strength': 'Ph·∫£n h·ªìi nhanh'
                    }
                    print("‚úÖ GROQ: Auto-update ready")
            except Exception as e:
                print(f"‚ùå GROQ: {e}")
        
        print(f"üîß AUTO-UPDATE SUMMARY: {len(available_engines)} AI engines ready")
        print(f"Auto-update participants: {', '.join([ai.value.upper() for ai in available_engines])}")
        
        self.available_engines = available_engines

    async def multi_ai_search_and_debate(self, question: str, max_sources: int = 5):
        """üîß AUTO-UPDATE: Main debate function with automatic date"""
        
        current_date_str = get_current_date_str()
        current_time_str = get_current_time_str()
        
        debate_data = {
            'question': question,
            'stage': DebateStage.SEARCH,
            'ai_responses': {},
            'debate_rounds': [],
            'consensus_score': {},
            'final_answer': '',
            'timeline': []
        }
        
        try:
            # üîß STAGE 1: AUTO-UPDATE SEARCH
            print(f"\n{'='*60}")
            print(f"üîß STAGE 1: AUTO-UPDATE SEARCH - {current_date_str}")
            print(f"{'='*60}")
            
            debate_data['stage'] = DebateStage.SEARCH
            debate_data['timeline'].append({
                'stage': 'search_start',
                'time': current_time_str,
                'message': f"B·∫Øt ƒë·∫ßu t√¨m ki·∫øm v·ªõi {len(self.available_engines)} AI engines - {current_date_str}"
            })
            
            # Use AUTO-UPDATE enhanced search
            print(f"üîß Running AUTO-UPDATE search for: {question}")
            search_results = await enhanced_google_search(question, max_sources)
            
            # All AIs share the same AUTO-UPDATE search results
            for ai_provider in self.available_engines:
                debate_data['ai_responses'][ai_provider] = {
                    'search_sources': search_results,
                    'search_error': None
                }
                print(f"‚úÖ {ai_provider.value.upper()} got {len(search_results)} AUTO-UPDATE sources")
            
            best_sources = search_results
            
            debate_data['timeline'].append({
                'stage': 'search_complete',
                'time': current_time_str,
                'message': f"AUTO-UPDATE t√¨m ki·∫øm ho√†n t·∫•t: {len(best_sources)} ngu·ªìn tin v·ªõi d·ªØ li·ªáu {current_date_str}"
            })
            
            # üîß STAGE 2: AUTO-UPDATE AI ANALYSIS
            print(f"\n{'='*60}")
            print(f"üîß STAGE 2: AUTO-UPDATE MULTI-AI ANALYSIS")
            print(f"{'='*60}")
            
            debate_data['stage'] = DebateStage.INITIAL_RESPONSE
            
            context = self._build_context_from_sources(best_sources, current_date_str)
            print(f"üìÑ AUTO-UPDATE Context built: {len(context)} characters with {current_date_str} data")
            
            initial_tasks = []
            for ai_provider in self.available_engines:
                if ai_provider in debate_data['ai_responses']:
                    initial_tasks.append(self._ai_initial_response_auto_update(ai_provider, question, context))
            
            initial_results = await asyncio.gather(*initial_tasks, return_exceptions=True)
            
            for i, result in enumerate(initial_results):
                ai_provider = self.available_engines[i]
                if isinstance(result, Exception):
                    print(f"‚ùå {ai_provider.value.upper()} AUTO-UPDATE response failed: {result}")
                    debate_data['ai_responses'][ai_provider]['initial_response'] = f"L·ªói ƒë√£ s·ª≠a: {str(result)}"
                else:
                    print(f"‚úÖ {ai_provider.value.upper()} AUTO-UPDATE generated response")
                    debate_data['ai_responses'][ai_provider]['initial_response'] = result
            
            debate_data['timeline'].append({
                'stage': 'initial_responses_complete',
                'time': current_time_str,
                'message': f"AUTO-UPDATE: {len([r for r in initial_results if not isinstance(r, Exception)])} AI ho√†n th√†nh ph√¢n t√≠ch"
            })
            
            # üîß STAGE 3: AUTO-UPDATE QUICK CONSENSUS
            print(f"\n{'='*60}")
            print("üîß STAGE 3: AUTO-UPDATE QUICK CONSENSUS")
            print(f"{'='*60}")
            
            debate_data['stage'] = DebateStage.CONSENSUS
            
            # AUTO-UPDATE quick consensus
            consensus_result = await self._build_quick_consensus_auto_update(
                question,
                debate_data['ai_responses'],
                context
            )
            
            debate_data['consensus_score'] = consensus_result['scores']
            debate_data['final_answer'] = consensus_result['final_answer']
            
            debate_data['timeline'].append({
                'stage': 'consensus_complete',
                'time': current_time_str,
                'message': f"AUTO-UPDATE: ƒê·∫°t ƒë∆∞·ª£c s·ª± ƒë·ªìng thu·∫≠n v·ªõi d·ªØ li·ªáu {current_date_str}"
            })
            
            print(f"‚úÖ AUTO-UPDATE MULTI-AI DEBATE COMPLETED: {len(debate_data['timeline'])} stages")
            
            return debate_data
            
        except Exception as e:
            print(f"‚ùå AUTO-UPDATE DEBATE SYSTEM ERROR: {e}")
            return {
                'question': question,
                'error': str(e),
                'stage': debate_data.get('stage', 'unknown'),
                'timeline': debate_data.get('timeline', [])
            }

    async def _ai_initial_response_auto_update(self, ai_provider: AIProvider, question: str, context: str):
        """üîß AUTO-UPDATE: Each AI generates response with automatic current date"""
        try:
            current_date_str = get_current_date_str()
            personality = self.ai_engines[ai_provider]['personality']
            
            # AUTO-UPDATE personality prompts emphasizing current date
            personality_prompts = {
                'analytical_researcher': f"B·∫°n l√† nh√† nghi√™n c·ª©u ph√¢n t√≠ch. H√£y ph√¢n t√≠ch d·ªØ li·ªáu C·ª§ TH·ªÇ t·ª´ CONTEXT ng√†y {current_date_str} m·ªôt c√°ch ch√≠nh x√°c. Tr√≠ch d·∫´n S·ªê LI·ªÜU v√† TH·ªúI GIAN c·ª• th·ªÉ.",
                'financial_expert': f"B·∫°n l√† chuy√™n gia t√†i ch√≠nh. H√£y t·∫≠p trung v√†o Y·∫æU T·ªê KINH T·∫æ v√† S·ªê LI·ªÜU T√ÄI CH√çNH C·ª§ TH·ªÇ t·ª´ CONTEXT ng√†y {current_date_str}. ƒê∆∞a ra GI√Å C·∫¢ ch√≠nh x√°c.",
                'critical_thinker': f"B·∫°n l√† ng∆∞·ªùi t∆∞ duy ph·∫£n bi·ªán. H√£y xem x√©t D·ªÆ LI·ªÜU TH·ª∞C t·ª´ CONTEXT ng√†y {current_date_str} v√† ƒë·∫∑t c√¢u h·ªèi v·ªÅ NGUY√äN NH√ÇN.",
                'quick_responder': f"B·∫°n l√† ng∆∞·ªùi ph·∫£n h·ªìi nhanh. H√£y t√≥m t·∫Øt D·ªÆ LI·ªÜU QUAN TR·ªåNG NH·∫§T t·ª´ CONTEXT ng√†y {current_date_str} m·ªôt c√°ch s√∫c t√≠ch."
            }
            
            prompt = f"""{personality_prompts.get(personality, f'B·∫°n l√† chuy√™n gia t√†i ch√≠nh ph√¢n t√≠ch d·ªØ li·ªáu {current_date_str}.')}

NHI·ªÜM V·ª§ QUAN TR·ªåNG: S·ª≠ d·ª•ng D·ªÆ LI·ªÜU TH·ª∞C t·ª´ CONTEXT ng√†y {current_date_str} ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. PH·∫¢I TR√çCH D·∫™N S·ªê LI·ªÜU C·ª§ TH·ªÇ, GI√Å C·∫¢, TH·ªúI GIAN.

CONTEXT (D·ªÆ LI·ªÜU TH·ª∞C NG√ÄY {current_date_str}):
{context}

C√ÇU H·ªéI: {question}

Y√äU C·∫¶U:
1. S·ª¨ D·ª§NG S·ªê LI·ªÜU C·ª§ TH·ªÇ t·ª´ Context (gi√° c·∫£, t·ª∑ l·ªá, th·ªùi gian {current_date_str})
2. TR√çCH D·∫™N NGU·ªíN TIN c·ª• th·ªÉ
3. PH√ÇN T√çCH d·ª±a tr√™n d·ªØ li·ªáu th·ª±c ng√†y {current_date_str}
4. ƒê·ªô d√†i: 200-300 t·ª´ v·ªõi TH√îNG TIN C·ª§ TH·ªÇ

H√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi chuy√™n s√¢u v·ªõi S·ªê LI·ªÜU TH·ª∞C t·ª´ g√≥c ƒë·ªô c·ªßa b·∫°n:"""

            response = await self._call_specific_ai_fixed(ai_provider, prompt, context)
            return response
            
        except Exception as e:
            print(f"‚ùå {ai_provider.value.upper()} AUTO-UPDATE response error: {e}")
            return f"L·ªói ph√¢n t√≠ch ƒë√£ s·ª≠a: {str(e)}"

    async def _build_quick_consensus_auto_update(self, question: str, ai_responses: dict, context: str):
        """üîß AUTO-UPDATE: Build consensus with automatic current date"""
        
        current_date_str = get_current_date_str()
        
        consensus_result = {
            'scores': {},
            'final_answer': '',
            'reasoning': ''
        }
        
        try:
            participating_ais = [ai for ai in self.available_engines if ai in ai_responses and 'initial_response' in ai_responses[ai]]
            
            if not participating_ais:
                consensus_result['final_answer'] = f"Kh√¥ng th·ªÉ ƒë·∫°t ƒë∆∞·ª£c s·ª± ƒë·ªìng thu·∫≠n do thi·∫øu d·ªØ li·ªáu ng√†y {current_date_str}."
                return consensus_result
            
            # AUTO-UPDATE scoring with current date emphasis
            for ai_provider in participating_ais:
                score = 0
                response = ai_responses[ai_provider].get('initial_response', '')
                
                # Base score for having response
                score += min(len(response) / 10, 50)
                
                # AUTO-UPDATE bonus for using current date data
                if current_date_str in response:
                    score += 40  # High bonus for current date
                if re.search(r'\d+[.,]\d+', response):  # Numbers with decimals
                    score += 30
                if re.search(r'\d+\.\d+\d+', response):  # Prices
                    score += 25
                if re.search(r'tri·ªáu|ngh√¨n|t·ª∑|USD|VND', response):  # Currency units
                    score += 20
                if re.search(r'h√¥m nay|ng√†y|th√°ng', response):  # Time references
                    score += 15
                
                consensus_result['scores'][ai_provider] = score
            
            # Find best AI with most current data
            best_ai = max(consensus_result['scores'], key=consensus_result['scores'].get)
            
            print(f"üèÜ AUTO-UPDATE BEST AI: {self.ai_engines[best_ai]['name']} (Score: {consensus_result['scores'][best_ai]})")
            
            # AUTO-UPDATE final answer synthesis
            all_responses = ""
            for ai_provider in participating_ais:
                ai_name = self.ai_engines[ai_provider]['name']
                response = ai_responses[ai_provider].get('initial_response', '')
                all_responses += f"\n{ai_name}: {response}\n"
            
            final_prompt = f"""B·∫°n l√† {self.ai_engines[best_ai]['name']} - ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi cu·ªëi c√πng t·ª´ {len(participating_ais)} AI.

NHI·ªÜM V·ª§: T·ªïng h·ª£p T·∫§T C·∫¢ D·ªÆ LI·ªÜU TH·ª∞C NG√ÄY {current_date_str} t·ª´ c√°c AI ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi HO√ÄN CH·ªàNH v√† CH√çNH X√ÅC NH·∫§T.

C√ÇU H·ªéI G·ªêC: {question}

D·ªÆ LI·ªÜU TH·ª∞C NG√ÄY {current_date_str}: {context}

PH√ÇN T√çCH T·ª™ C√ÅC AI:
{all_responses}

H√£y t·ªïng h·ª£p th√†nh c√¢u tr·∫£ l·ªùi cu·ªëi c√πng (400-600 t·ª´):
1. B·∫ÆT ƒê·∫¶U v·ªõi: "Sau khi ph√¢n t√≠ch d·ªØ li·ªáu th·ª±c ng√†y {current_date_str} t·ª´ {len(participating_ais)} chuy√™n gia AI..."
2. S·ª¨ D·ª§NG T·∫§T C·∫¢ S·ªê LI·ªÜU C·ª§ TH·ªÇ t·ª´ Context v√† AI responses
3. TR√çCH D·∫™N GI√Å C·∫¢, TH·ªúI GIAN {current_date_str}, NGUY√äN NH√ÇN c·ª• th·ªÉ
4. K·∫æT LU·∫¨N r√µ r√†ng v·ªõi d·ªØ li·ªáu th·ª±c ng√†y {current_date_str}

QUAN TR·ªåNG: Ph·∫£i c√≥ S·ªê LI·ªÜU C·ª§ TH·ªÇ NG√ÄY {current_date_str} v√† NGU·ªíN TIN trong c√¢u tr·∫£ l·ªùi."""

            # Use the best AI for final synthesis
            final_answer = await self._call_specific_ai_fixed(best_ai, final_prompt, context)
            consensus_result['final_answer'] = final_answer
            consensus_result['reasoning'] = f"T·ªïng h·ª£p b·ªüi {self.ai_engines[best_ai]['name']} t·ª´ {len(participating_ais)} AI v·ªõi d·ªØ li·ªáu {current_date_str}"
            
            print(f"‚úÖ AUTO-UPDATE CONSENSUS: Final answer with {current_date_str} data")
            
        except Exception as e:
            print(f"‚ùå AUTO-UPDATE CONSENSUS ERROR: {e}")
            consensus_result['final_answer'] = f"L·ªói ƒë·∫°t s·ª± ƒë·ªìng thu·∫≠n ƒë√£ s·ª≠a: {str(e)}"
        
        return consensus_result

    def _build_context_from_sources(self, sources: List[dict], current_date_str: str) -> str:
        """Build context string from sources with automatic current date"""
        context = f"D·ªÆ LI·ªÜU TH·ª∞C NG√ÄY {current_date_str}:\n"
        for i, source in enumerate(sources, 1):
            context += f"Ngu·ªìn {i} ({source['source_name']}): {source['snippet']}\n"
        return context

    # üîß FIXED: AI API calls with correct formats (unchanged from previous version)
    async def _call_specific_ai_fixed(self, ai_provider: AIProvider, prompt: str, context: str):
        """üîß FIXED: Call specific AI engine with correct API format"""
        try:
            if ai_provider == AIProvider.GEMINI:
                return await self._call_gemini_fixed(prompt, context)
            elif ai_provider == AIProvider.DEEPSEEK:
                return await self._call_deepseek_fixed(prompt, context)
            elif ai_provider == AIProvider.CLAUDE:
                return await self._call_claude_fixed(prompt, context)
            elif ai_provider == AIProvider.GROQ:
                return await self._call_groq_fixed(prompt, context)
            
            raise Exception(f"Unknown AI provider: {ai_provider}")
            
        except Exception as e:
            print(f"‚ùå Error calling FIXED {ai_provider.value}: {str(e)}")
            raise e

    async def _call_gemini_fixed(self, prompt: str, context: str):
        """üîß FIXED: Call Gemini AI with correct format"""
        if not GEMINI_AVAILABLE:
            raise Exception("Gemini library not available")
        
        try:
            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.2,
                top_p=0.8,
                top_k=20,
                max_output_tokens=1000,
            )
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    model.generate_content,
                    prompt,
                    generation_config=generation_config
                ),
                timeout=25
            )
            
            return response.text.strip()
            
        except asyncio.TimeoutError:
            raise Exception("Gemini API timeout")
        except Exception as e:
            raise Exception(f"Gemini API error: {str(e)}")

    async def _call_deepseek_fixed(self, prompt: str, context: str):
        """üîß FIXED: Call DeepSeek AI with correct format (NO unsupported parameters)"""
        try:
            session = await self.create_session()
            
            headers = {
                'Authorization': f'Bearer {DEEPSEEK_API_KEY}',
                'Content-Type': 'application/json'
            }
            
            # üîß FIXED: Remove unsupported parameters
            data = {
                'model': 'deepseek-chat',  # Use supported model
                'messages': [
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 1000
                # üîß REMOVED: temperature, top_p, frequency_penalty (unsupported)
            }
            
            async with session.post(
                'https://api.deepseek.com/chat/completions',  # Fixed URL
                headers=headers,
                json=data,
                timeout=aiohttp.ClientTimeout(total=25)
            ) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"DeepSeek API error {response.status}: {error_text}")
                
                result = await response.json()
                return result['choices'][0]['message']['content'].strip()
                
        except Exception as e:
            raise Exception(f"DeepSeek FIXED API error: {str(e)}")

    async def _call_claude_fixed(self, prompt: str, context: str):
        """üîß FIXED: Call Claude AI with correct header format"""
        try:
            session = await self.create_session()
            
            # üîß FIXED: Use x-api-key instead of Authorization Bearer
            headers = {
                'x-api-key': ANTHROPIC_API_KEY,
                'Content-Type': 'application/json',
                'anthropic-version': '2023-06-01'
            }
            
            # üîß FIXED: Ensure content is not empty
            if not prompt.strip():
                raise Exception("Prompt cannot be empty")
            
            data = {
                'model': 'claude-3-5-sonnet-20241022',
                'max_tokens': 1000,
                'temperature': 0.2,
                'messages': [
                    {
                        'role': 'user',
                        'content': prompt.strip()  # Ensure non-empty content
                    }
                ]
            }
            
            async with session.post(
                'https://api.anthropic.com/v1/messages',
                headers=headers,
                json=data,
                timeout=aiohttp.ClientTimeout(total=25)
            ) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"Claude API error {response.status}: {error_text}")
                
                result = await response.json()
                return result['content'][0]['text'].strip()
                
        except Exception as e:
            raise Exception(f"Claude FIXED API error: {str(e)}")

    async def _call_groq_fixed(self, prompt: str, context: str):
        """üîß FIXED: Call Groq AI with correct format"""
        try:
            session = await self.create_session()
            
            headers = {
                'Authorization': f'Bearer {GROQ_API_KEY}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': 'llama-3.3-70b-versatile',
                'messages': [
                    {'role': 'user', 'content': prompt}
                ],
                'temperature': 0.2,
                'max_tokens': 1000
            }
            
            async with session.post(
                'https://api.groq.com/openai/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=aiohttp.ClientTimeout(total=25)
            ) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"Groq API error {response.status}: {error_text}")
                
                result = await response.json()
                return result['choices'][0]['message']['content'].strip()
                
        except Exception as e:
            raise Exception(f"Groq FIXED API error: {str(e)}")

# Initialize AUTO-UPDATE Multi-AI Debate Engine
debate_engine = MultiAIDebateEngine()

# RSS and news functions with AUTO-UPDATE
async def collect_news_from_sources(sources_dict, limit_per_source=6):
    all_news = []
    
    for source_name, rss_url in sources_dict.items():
        try:
            print(f"üîÑ Fetching from {source_name}...")
            
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            response = requests.get(rss_url, headers=headers, timeout=10)
            response.raise_for_status()
            feed = feedparser.parse(response.content)
            
            if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                continue
                
            entries_processed = 0
            for entry in feed.entries[:limit_per_source]:
                try:
                    # üîß AUTO-UPDATE: Use current Vietnam time
                    vn_time = get_current_vietnam_datetime()
                    
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        vn_time = convert_utc_to_vietnam_time(entry.published_parsed)
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        vn_time = convert_utc_to_vietnam_time(entry.updated_parsed)
                    
                    description = ""
                    if hasattr(entry, 'summary'):
                        description = entry.summary[:400] + "..." if len(entry.summary) > 400 else entry.summary
                    elif hasattr(entry, 'description'):
                        description = entry.description[:400] + "..." if len(entry.description) > 400 else entry.description
                    
                    if hasattr(entry, 'title') and hasattr(entry, 'link'):
                        news_item = {
                            'title': html.unescape(entry.title.strip()),
                            'link': entry.link,
                            'source': source_name,
                            'published': vn_time,
                            'published_str': vn_time.strftime("%H:%M %d/%m"),
                            'description': html.unescape(description) if description else ""
                        }
                        all_news.append(news_item)
                        entries_processed += 1
                    
                except Exception:
                    continue
                    
            print(f"‚úÖ Got {entries_processed} news from {source_name}")
            
        except Exception as e:
            print(f"‚ùå Error from {source_name}: {e}")
            continue
    
    unique_news = []
    seen_links = set()
    
    for news in all_news:
        if news['link'] not in seen_links:
            seen_links.add(news['link'])
            unique_news.append(news)
    
    unique_news.sort(key=lambda x: x['published'], reverse=True)
    return unique_news

def save_user_news(user_id, news_list, command_type):
    user_news_cache[user_id] = {
        'news': news_list,
        'command': command_type,
        'timestamp': get_current_vietnam_datetime()
    }

# Bot event handlers
@bot.event
async def on_ready():
    print(f'‚úÖ {bot.user} is online!')
    print(f'üìä Connected to {len(bot.guilds)} server(s)')
    
    ai_count = len(debate_engine.available_engines)
    if ai_count >= 1:
        print(f'üîß AUTO-UPDATE Multi-AI System: {ai_count} AI engines ready')
        ai_names = [debate_engine.ai_engines[ai]['name'] for ai in debate_engine.available_engines]
        print(f'ü•ä AUTO-UPDATE participants: {", ".join(ai_names)}')
    else:
        print('‚ö†Ô∏è Warning: Need at least 1 AI engine for debate!')
    
    current_datetime_str = get_current_datetime_str()
    print(f'üîß AUTO-UPDATE: Current Vietnam time: {current_datetime_str}')
    print('üÜï TRAFILATURA: Advanced content extraction enabled')
    print('üåê AUTO-TRANSLATE: International content translation enabled')
    print('üîß FIXED: All API calls corrected')
    
    if GOOGLE_API_KEY and GOOGLE_CSE_ID:
        print('üîç Google Search API: AUTO-UPDATE with current date filtering')
    else:
        print('üîß Google Search API: Using AUTO-UPDATE enhanced fallback')
    
    total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
    print(f'üì∞ Ready with {total_sources} RSS sources')
    print('üéØ Type !menu for help')
    
    status_text = f"AUTO-UPDATE v3.0 ‚Ä¢ {ai_count} AIs ‚Ä¢ Trafilatura ‚Ä¢ !menu"
    await bot.change_presence(
        activity=discord.Activity(
            type=discord.ActivityType.watching,
            name=status_text
        )
    )

@bot.event
async def on_command_error(ctx, error):
    if isinstance(error, commands.CommandNotFound):
        return
    else:
        print(f"‚ùå Command error: {error}")
        await ctx.send(f"‚ùå L·ªói: {str(error)}")

# üîß AUTO-UPDATE: Main Multi-AI Debate Command
@bot.command(name='hoi')
async def multi_ai_debate_question_auto_update(ctx, *, question):
    """üîß AUTO-UPDATE v3.0: Multi-AI Debate System with automatic date and Trafilatura"""
    
    try:
        if len(debate_engine.available_engines) < 1:
            embed = discord.Embed(
                title="‚ö†Ô∏è Multi-AI Debate System kh√¥ng kh·∫£ d·ª•ng",
                description=f"C·∫ßn √≠t nh·∫•t 1 AI engine. Hi·ªán c√≥: {len(debate_engine.available_engines)}",
                color=0xff6b6b
            )
            await ctx.send(embed=embed)
            return
        
        current_datetime_str = get_current_datetime_str()
        current_date_str = get_current_date_str()
        
        # Create progress message
        progress_embed = discord.Embed(
            title="üîß Multi-AI Debate System - AUTO-UPDATE v3.0",
            description=f"**C√¢u h·ªèi:** {question}\n\nüîÑ **ƒêang t√¨m ki·∫øm d·ªØ li·ªáu th·ª±c {current_datetime_str} v·ªõi {len(debate_engine.available_engines)} AI...**",
            color=0x9932cc,
            timestamp=ctx.message.created_at
        )
        
        ai_list = ""
        for ai_provider in debate_engine.available_engines:
            ai_info = debate_engine.ai_engines[ai_provider]
            ai_list += f"{ai_info['emoji']} **{ai_info['name']}** - {ai_info['strength']} ‚úÖ\n"
        
        progress_embed.add_field(
            name="üîß AI Engines (AUTO-UPDATE)",
            value=ai_list,
            inline=False
        )
        
        progress_embed.add_field(
            name="üÜï T√≠nh nƒÉng AUTO-UPDATE v3.0",
            value=f"‚úÖ **T·ª± ƒë·ªông c·∫≠p nh·∫≠t ng√†y**: {current_date_str}\n‚úÖ **Trafilatura**: Tr√≠ch xu·∫•t n·ªôi dung t·ªët nh·∫•t 2024\n‚úÖ **API Fixed**: DeepSeek & Claude ƒë√£ s·ª≠a\n‚úÖ **Auto-translate**: D·ªãch t·ª± ƒë·ªông tin n∆∞·ªõc ngo√†i\n‚úÖ **Real-time data**: D·ªØ li·ªáu th·ªùi gian th·ª±c",
            inline=False
        )
        
        progress_msg = await ctx.send(embed=progress_embed)
        
        # Start AUTO-UPDATE debate
        print(f"\nüîß STARTING AUTO-UPDATE v3.0 MULTI-AI DEBATE for: {question}")
        debate_result = await debate_engine.multi_ai_search_and_debate(question, max_sources=5)
        
        # Create result embed
        if 'error' in debate_result:
            error_embed = discord.Embed(
                title="‚ùå Multi-AI Debate System - L·ªói",
                description=f"**C√¢u h·ªèi:** {question}\n\n**L·ªói:** {debate_result['error']}",
                color=0xff6b6b,
                timestamp=ctx.message.created_at
            )
            await progress_msg.edit(embed=error_embed)
            return
        
        # Success with AUTO-UPDATE data
        result_embed = discord.Embed(
            title=f"üîß Multi-AI Debate - AUTO-UPDATE v3.0 ({current_datetime_str})",
            description=f"**C√¢u h·ªèi:** {question}",
            color=0x00ff88,
            timestamp=ctx.message.created_at
        )
        
        # Add final answer with AUTO-UPDATE data
        final_answer = debate_result.get('final_answer', 'Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi.')
        if len(final_answer) > 1000:
            result_embed.add_field(
                name=f"üìù C√¢u tr·∫£ l·ªùi (Ph·∫ßn 1) - D·ªØ li·ªáu {current_date_str}",
                value=final_answer[:1000] + "...",
                inline=False
            )
        else:
            result_embed.add_field(
                name=f"üìù C√¢u tr·∫£ l·ªùi - D·ªØ li·ªáu {current_date_str}",
                value=final_answer,
                inline=False
            )
        
        # Show AUTO-UPDATE AI scores
        if 'consensus_score' in debate_result and debate_result['consensus_score']:
            scores_text = ""
            sorted_scores = sorted(debate_result['consensus_score'].items(), key=lambda x: x[1], reverse=True)
            
            for i, (ai_provider, score) in enumerate(sorted_scores, 1):
                ai_info = debate_engine.ai_engines[ai_provider]
                medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "üèÖ"
                scores_text += f"{medal} **{ai_info['name']}** {ai_info['emoji']}: {score:.0f} ƒëi·ªÉm ‚úÖ\n"
            
            result_embed.add_field(
                name=f"üèÜ B·∫£ng x·∫øp h·∫°ng AI (D·ªØ li·ªáu {current_date_str})",
                value=scores_text,
                inline=True
            )
        
        # AUTO-UPDATE statistics
        stats_text = f"üîß **Version**: AUTO-UPDATE v3.0\n"
        stats_text += f"üìÖ **Ng√†y t·ª± ƒë·ªông**: {current_date_str}\n"
        stats_text += f"üîç **Search**: Enhanced t·ª± ƒë·ªông\n"
        stats_text += f"ü§ñ **AI Engines**: {len(debate_engine.available_engines)} (FIXED)\n"
        stats_text += f"üÜï **Trafilatura**: Content extraction\n"
        
        if 'timeline' in debate_result:
            start_time = debate_result['timeline'][0]['time'] if debate_result['timeline'] else "N/A"
            end_time = debate_result['timeline'][-1]['time'] if debate_result['timeline'] else "N/A"
            stats_text += f"‚è±Ô∏è **Th·ªùi gian**: {start_time} - {end_time}"
        
        result_embed.add_field(
            name="üìä Th·ªëng k√™ AUTO-UPDATE",
            value=stats_text,
            inline=True
        )
        
        result_embed.set_footer(text=f"üîß Multi-AI AUTO-UPDATE v3.0 ‚Ä¢ {current_datetime_str} ‚Ä¢ Trafilatura ‚Ä¢ !menu")
        
        await progress_msg.edit(embed=result_embed)
        
        # Send continuation if needed
        if len(final_answer) > 1000:
            continuation_embed = discord.Embed(
                title=f"üìù C√¢u tr·∫£ l·ªùi (Ph·∫ßn 2) - D·ªØ li·ªáu {current_date_str}",
                description=final_answer[1000:2000],
                color=0x00ff88
            )
            
            if len(final_answer) > 2000:
                continuation_embed.set_footer(text=f"V√† c√≤n {len(final_answer) - 2000} k√Ω t·ª± n·ªØa... - {current_datetime_str}")
            
            await ctx.send(embed=continuation_embed)
        
        print(f"‚úÖ AUTO-UPDATE v3.0 MULTI-AI DEBATE COMPLETED with {current_date_str} data for: {question}")
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói h·ªá th·ªëng Multi-AI Debate AUTO-UPDATE: {str(e)}")
        print(f"‚ùå MULTI-AI DEBATE AUTO-UPDATE ERROR: {e}")

# üîß AUTO-UPDATE: All news commands with Trafilatura
@bot.command(name='all')
async def get_all_news_auto_update(ctx, page=1):
    """üîß AUTO-UPDATE: L·∫•y tin t·ª©c t·ª´ t·∫•t c·∫£ ngu·ªìn v·ªõi ng√†y t·ª± ƒë·ªông"""
    try:
        page = max(1, int(page))
        current_datetime_str = get_current_datetime_str()
        loading_msg = await ctx.send(f"‚è≥ ƒêang t·∫£i tin t·ª©c t·ª´ t·∫•t c·∫£ ngu·ªìn - {current_datetime_str}...")
        
        domestic_news = await collect_news_from_sources(RSS_FEEDS['domestic'], 6)
        international_news = await collect_news_from_sources(RSS_FEEDS['international'], 4)
        
        await loading_msg.delete()
        
        all_news = domestic_news + international_news
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = all_news[start_index:end_index]
        
        if not page_news:
            total_pages = (len(all_news) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        embed = discord.Embed(
            title=f"üì∞ Tin t·ª©c t·ªïng h·ª£p (Trang {page}) - {get_current_date_str()}",
            description=f"üîß AUTO-UPDATE v3.0 ‚Ä¢ {len(debate_engine.available_engines)} AIs ‚Ä¢ üÜï Trafilatura",
            color=0x00ff88
        )
        
        domestic_count = sum(1 for news in page_news if news['source'] in RSS_FEEDS['domestic'])
        international_count = len(page_news) - domestic_count
        
        embed.add_field(
            name="üìä Th·ªëng k√™ AUTO-UPDATE",
            value=f"üáªüá≥ Trong n∆∞·ªõc: {domestic_count} tin\nüåç Qu·ªëc t·∫ø: {international_count} tin (auto-translate)\nüìä T·ªïng: {len(all_news)} tin\nüìÖ Ng√†y t·ª± ƒë·ªông: {get_current_date_str()}",
            inline=False
        )
        
        for i, news in enumerate(page_news, 1):
            title = news['title'][:60] + "..." if len(news['title']) > 60 else news['title']
            embed.add_field(
                name=f"{i}. {title}",
                value=f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üîó [ƒê·ªçc]({news['link']})",
                inline=False
            )
        
        save_user_news(ctx.author.id, page_news, f"all_page_{page}")
        
        total_pages = (len(all_news) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"üîß AUTO-UPDATE v3.0 ‚Ä¢ Trang {page}/{total_pages} ‚Ä¢ !chitiet [s·ªë] (Trafilatura + auto-translate)")
        
        await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='in')
async def get_domestic_news_auto_update(ctx, page=1):
    """üîß AUTO-UPDATE: L·∫•y tin t·ª©c trong n∆∞·ªõc"""
    try:
        page = max(1, int(page))
        current_datetime_str = get_current_datetime_str()
        loading_msg = await ctx.send(f"‚è≥ ƒêang t·∫£i tin t·ª©c trong n∆∞·ªõc - {current_datetime_str}...")
        
        news_list = await collect_news_from_sources(RSS_FEEDS['domestic'], 8)
        await loading_msg.delete()
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        embed = discord.Embed(
            title=f"üáªüá≥ Tin kinh t·∫ø trong n∆∞·ªõc (Trang {page}) - {get_current_date_str()}",
            description=f"üîß AUTO-UPDATE v3.0 ‚Ä¢ T·ª´ {len(RSS_FEEDS['domestic'])} ngu·ªìn ‚Ä¢ üÜï Trafilatura",
            color=0xff0000
        )
        
        embed.add_field(
            name="üìä Th√¥ng tin AUTO-UPDATE",
            value=f"üì∞ T·ªïng tin: {len(news_list)} tin\nüéØ Lƒ©nh v·ª±c: Kinh t·∫ø, CK, BƒêS\nüìÖ Ng√†y t·ª± ƒë·ªông: {get_current_date_str()}",
            inline=False
        )
        
        for i, news in enumerate(page_news, 1):
            title = news['title'][:60] + "..." if len(news['title']) > 60 else news['title']
            embed.add_field(
                name=f"{i}. {title}",
                value=f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üîó [ƒê·ªçc]({news['link']})",
                inline=False
            )
        
        save_user_news(ctx.author.id, page_news, f"in_page_{page}")
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"üîß AUTO-UPDATE v3.0 ‚Ä¢ Trang {page}/{total_pages} ‚Ä¢ !chitiet [s·ªë] (Trafilatura)")
        
        await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='out')
async def get_international_news_auto_update(ctx, page=1):
    """üîß AUTO-UPDATE: L·∫•y tin t·ª©c qu·ªëc t·∫ø v·ªõi auto-translate"""
    try:
        page = max(1, int(page))
        current_datetime_str = get_current_datetime_str()
        loading_msg = await ctx.send(f"‚è≥ ƒêang t·∫£i tin t·ª©c qu·ªëc t·∫ø v·ªõi auto-translate - {current_datetime_str}...")
        
        news_list = await collect_news_from_sources(RSS_FEEDS['international'], 6)
        await loading_msg.delete()
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        embed = discord.Embed(
            title=f"üåç Tin kinh t·∫ø qu·ªëc t·∫ø (Trang {page}) - {get_current_date_str()}",
            description=f"üîß AUTO-UPDATE v3.0 ‚Ä¢ T·ª´ {len(RSS_FEEDS['international'])} ngu·ªìn ‚Ä¢ üÜï Trafilatura + Auto-translate",
            color=0x0066ff
        )
        
        embed.add_field(
            name="üìä Th√¥ng tin AUTO-UPDATE",
            value=f"üì∞ T·ªïng tin: {len(news_list)} tin\nüåê T·ª± ƒë·ªông d·ªãch: Ti·∫øng Anh ‚Üí Ti·∫øng Vi·ªát\nüìÖ Ng√†y t·ª± ƒë·ªông: {get_current_date_str()}",
            inline=False
        )
        
        for i, news in enumerate(page_news, 1):
            title = news['title'][:60] + "..." if len(news['title']) > 60 else news['title']
            embed.add_field(
                name=f"{i}. üåê {title}",
                value=f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üîó [ƒê·ªçc]({news['link']})",
                inline=False
            )
        
        save_user_news(ctx.author.id, page_news, f"out_page_{page}")
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"üîß AUTO-UPDATE v3.0 ‚Ä¢ Trang {page}/{total_pages} ‚Ä¢ !chitiet [s·ªë] (Trafilatura + auto-translate)")
        
        await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

# üÜï CHI TI·∫æT B√ÄI VI·∫æT V·ªöI TRAFILATURA + AUTO-TRANSLATE
@bot.command(name='chitiet')
async def get_news_detail_trafilatura(ctx, news_number: int):
    """üÜï TRAFILATURA: Xem chi ti·∫øt b√†i vi·∫øt v·ªõi Trafilatura + auto-translate"""
    try:
        user_id = ctx.author.id
        
        if user_id not in user_news_cache:
            await ctx.send("‚ùå B·∫°n ch∆∞a xem tin t·ª©c! D√πng `!all`, `!in`, ho·∫∑c `!out` tr∆∞·ªõc.")
            return
        
        user_data = user_news_cache[user_id]
        news_list = user_data['news']
        
        if news_number < 1 or news_number > len(news_list):
            await ctx.send(f"‚ùå S·ªë kh√¥ng h·ª£p l·ªá! Ch·ªçn t·ª´ 1 ƒë·∫øn {len(news_list)}")
            return
        
        news = news_list[news_number - 1]
        
        current_datetime_str = get_current_datetime_str()
        loading_msg = await ctx.send(f"üÜï ƒêang tr√≠ch xu·∫•t n·ªôi dung v·ªõi Trafilatura + auto-translate - {current_datetime_str}...")
        
        # üÜï TRAFILATURA: Use improved content extraction
        full_content = await fetch_full_content_improved(news['link'])
        
        # üåê AUTO-TRANSLATE: Apply translation if needed
        source_name = extract_source_name(news['link'])
        translated_content, is_translated = await detect_and_translate_content(full_content, source_name)
        
        await loading_msg.delete()
        
        embed = discord.Embed(
            title="üìñ Chi ti·∫øt b√†i vi·∫øt - TRAFILATURA v3.0",
            color=0x9932cc
        )
        
        # Add extraction method info
        extraction_method = "üöÄ Trafilatura" if TRAFILATURA_AVAILABLE else "üì∞ Newspaper3k" if NEWSPAPER_AVAILABLE else "üîÑ Legacy"
        
        # Add translation indicator
        title_suffix = " üåê (ƒê√£ d·ªãch)" if is_translated else ""
        embed.add_field(name=f"üì∞ Ti√™u ƒë·ªÅ{title_suffix}", value=news['title'], inline=False)
        embed.add_field(name="üï∞Ô∏è Th·ªùi gian", value=f"{news['published_str']} ({get_current_date_str()})", inline=True)
        
        source_display = source_name
        if is_translated:
            source_display += " üåê"
        embed.add_field(name="üì∞ Ngu·ªìn", value=source_display, inline=True)
        embed.add_field(name="üÜï Tr√≠ch xu·∫•t", value=extraction_method, inline=True)
        
        # Content with translation info
        content_title = "üìÑ N·ªôi dung chi ti·∫øt üåê (ƒê√£ d·ªãch t·ª´ ti·∫øng Anh)" if is_translated else "üìÑ N·ªôi dung chi ti·∫øt"
        
        if len(translated_content) > 1000:
            embed.add_field(
                name=f"{content_title} (Ph·∫ßn 1)",
                value=translated_content[:1000] + "...",
                inline=False
            )
            
            await ctx.send(embed=embed)
            
            # Second embed for continuation
            embed2 = discord.Embed(
                title=f"üìñ Chi ti·∫øt b√†i vi·∫øt (ti·∫øp theo){'üåê' if is_translated else ''} - TRAFILATURA",
                color=0x9932cc
            )
            
            embed2.add_field(
                name=f"{content_title} (Ph·∫ßn 2)",
                value=translated_content[1000:2000],
                inline=False
            )
            
            if is_translated:
                embed2.add_field(
                    name="üîÑ Th√¥ng tin d·ªãch thu·∫≠t AUTO-UPDATE",
                    value="üìù N·ªôi dung g·ªëc b·∫±ng ti·∫øng Anh ƒë√£ ƒë∆∞·ª£c d·ªãch sang ti·∫øng Vi·ªát t·ª± ƒë·ªông\nüí° ƒê·ªÉ xem b·∫£n g·ªëc, vui l√≤ng truy c·∫≠p link b√†i vi·∫øt",
                    inline=False
                )
            
            embed2.add_field(
                name="üîó ƒê·ªçc b√†i vi·∫øt ƒë·∫ßy ƒë·ªß",
                value=f"[Nh·∫•n ƒë·ªÉ ƒë·ªçc to√†n b·ªô b√†i vi·∫øt{'g·ªëc' if is_translated else ''}]({news['link']})",
                inline=False
            )
            
            embed2.set_footer(text=f"üÜï TRAFILATURA v3.0 ‚Ä¢ Auto-translate ‚Ä¢ {current_datetime_str} ‚Ä¢ !hoi [question]")
            
            await ctx.send(embed=embed2)
            return
        else:
            embed.add_field(
                name=content_title,
                value=translated_content,
                inline=False
            )
        
        if is_translated:
            embed.add_field(
                name="üîÑ Th√¥ng tin d·ªãch thu·∫≠t AUTO-UPDATE",
                value="üìù B√†i vi·∫øt g·ªëc b·∫±ng ti·∫øng Anh ƒë√£ ƒë∆∞·ª£c d·ªãch sang ti·∫øng Vi·ªát t·ª± ƒë·ªông",
                inline=False
            )
        
        embed.add_field(
            name="üîó ƒê·ªçc b√†i vi·∫øt ƒë·∫ßy ƒë·ªß",
            value=f"[Nh·∫•n ƒë·ªÉ ƒë·ªçc to√†n b·ªô b√†i vi·∫øt{'g·ªëc' if is_translated else ''}]({news['link']})",
            inline=False
        )
        
        embed.set_footer(text=f"üÜï TRAFILATURA v3.0 ‚Ä¢ Auto-update ‚Ä¢ {current_datetime_str} ‚Ä¢ Tin s·ªë {news_number} ‚Ä¢ !hoi [question]")
        
        await ctx.send(embed=embed)
        
    except ValueError:
        await ctx.send("‚ùå Vui l√≤ng nh·∫≠p s·ªë! V√≠ d·ª•: `!chitiet 5`")
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='cuthe')
async def get_news_detail_alias_trafilatura(ctx, news_number: int):
    """üÜï TRAFILATURA: Alias cho l·ªánh !chitiet"""
    await get_news_detail_trafilatura(ctx, news_number)

@bot.command(name='menu')
async def help_command_auto_update(ctx):
    """üîß AUTO-UPDATE: Menu h∆∞·ªõng d·∫´n ƒë·∫ßy ƒë·ªß"""
    current_datetime_str = get_current_datetime_str()
    
    embed = discord.Embed(
        title="üîß Multi-AI Debate Discord News Bot - AUTO-UPDATE v3.0",
        description=f"Bot tin t·ª©c v·ªõi Multi-AI t·ª± ƒë·ªông c·∫≠p nh·∫≠t - {current_datetime_str}",
        color=0xff9900
    )
    
    ai_count = len(debate_engine.available_engines)
    if ai_count >= 1:
        ai_status = f"üöÄ **{ai_count} AI Engines AUTO-UPDATE**\n"
        for ai_provider in debate_engine.available_engines:
            ai_info = debate_engine.ai_engines[ai_provider]
            ai_status += f"{ai_info['emoji']} **{ai_info['name']}** - {ai_info['strength']} ‚úÖ\n"
    else:
        ai_status = "‚ö†Ô∏è C·∫ßn √≠t nh·∫•t 1 AI engine ƒë·ªÉ ho·∫°t ƒë·ªông"
    
    embed.add_field(name="üîß Multi-AI Status AUTO-UPDATE v3.0", value=ai_status, inline=False)
    
    embed.add_field(
        name="ü•ä L·ªánh Multi-AI Debate AUTO-UPDATE",
        value=f"**!hoi [c√¢u h·ªèi]** - AI v·ªõi d·ªØ li·ªáu th·ª±c t·ª± ƒë·ªông {get_current_date_str()}\n*VD: !hoi gi√° v√†ng h√¥m nay bao nhi√™u?*",
        inline=False
    )
    
    embed.add_field(
        name="üì∞ L·ªánh tin t·ª©c AUTO-UPDATE (Trafilatura + Auto-translate)",
        value="**!all [trang]** - Tin t·ªïng h·ª£p\n**!in [trang]** - Tin trong n∆∞·ªõc\n**!out [trang]** - Tin qu·ªëc t·∫ø (auto-translate)\n**!chitiet [s·ªë]** - Chi ti·∫øt (üÜï Trafilatura + auto-translate)",
        inline=False
    )
    
    embed.add_field(
        name="üÜï T√≠nh nƒÉng AUTO-UPDATE v3.0",
        value=f"‚úÖ **T·ª± ƒë·ªông c·∫≠p nh·∫≠t ng√†y**: {get_current_date_str()}\n‚úÖ **Trafilatura**: Tr√≠ch xu·∫•t n·ªôi dung t·ªët nh·∫•t 2024\n‚úÖ **DeepSeek & Claude API**: ƒê√£ s·ª≠a t·∫•t c·∫£ l·ªói\n‚úÖ **Auto-translate**: D·ªãch t·ª± ƒë·ªông tin n∆∞·ªõc ngo√†i\n‚úÖ **Real-time search**: D·ªØ li·ªáu th·ªùi gian th·ª±c\n‚úÖ **Enhanced content**: 3-tier extraction (Trafilatura ‚Üí Newspaper ‚Üí Legacy)",
        inline=False
    )
    
    embed.add_field(
        name="üéØ V√≠ d·ª• s·ª≠ d·ª•ng AUTO-UPDATE",
        value=f"**!hoi gi√° v√†ng h√¥m nay** - AI t√¨m gi√° v√†ng {get_current_date_str()}\n**!hoi t·ª∑ gi√° usd** - AI t√¨m t·ª∑ gi√° hi·ªán t·∫°i\n**!hoi vn-index** - AI t√¨m ch·ªâ s·ªë ch·ª©ng kho√°n\n**!all** - Xem tin t·ª©c t·ªïng h·ª£p\n**!chitiet 1** - Xem chi ti·∫øt tin s·ªë 1 (üÜï Trafilatura + auto-translate)",
        inline=False
    )
    
    google_status = "‚úÖ Enhanced Search v·ªõi d·ªØ li·ªáu th·ª±c t·ª± ƒë·ªông" if GOOGLE_API_KEY and GOOGLE_CSE_ID else "‚úÖ AUTO-UPDATE enhanced fallback v·ªõi current data"
    embed.add_field(name="üîç Google Search AUTO-UPDATE", value=google_status, inline=True)
    
    embed.add_field(name=f"üìä Performance AUTO-UPDATE ({get_current_date_str()})", value=f"üöÄ **{ai_count} AI Engines**\n‚ö° **Real-time Data**\nüß† **Enhanced Context**\nüåê **Auto-translate**\nüÜï **Trafilatura**", inline=True)
    
    embed.set_footer(text=f"üîß Multi-AI AUTO-UPDATE v3.0 ‚Ä¢ {current_datetime_str} ‚Ä¢ Trafilatura ‚Ä¢ !hoi [question]")
    await ctx.send(embed=embed)

# Cleanup function
async def cleanup():
    if debate_engine:
        await debate_engine.close_session()

# Main execution
if __name__ == "__main__":
    try:
        keep_alive()
        print("üîß Starting AUTO-UPDATE v3.0 Multi-AI Debate Discord News Bot...")
        
        ai_count = len(debate_engine.available_engines)
        print(f"ü§ñ Multi-AI Debate System AUTO-UPDATE v3.0: {ai_count} engines initialized")
        
        current_datetime_str = get_current_datetime_str()
        print(f"üîß AUTO-UPDATE: Current Vietnam time: {current_datetime_str}")
        
        if ai_count >= 1:
            ai_names = [debate_engine.ai_engines[ai]['name'] for ai in debate_engine.available_engines]
            print(f"ü•ä AUTO-UPDATE debate ready with: {', '.join(ai_names)}")
            print("üîß FIXED: All API calls corrected")
            print("üîß AUTO-UPDATE: Date and time automatically updated")
            print("üÜï TRAFILATURA: Advanced content extraction enabled")
            print("üåê AUTO-TRANSLATE: International content translation enabled")
        else:
            print("‚ö†Ô∏è Warning: Need at least 1 AI engine")
        
        if GOOGLE_API_KEY and GOOGLE_CSE_ID:
            print("üîç Google Search API: AUTO-UPDATE with enhanced current date filtering")
        else:
            print("‚ö†Ô∏è Google Search API: Using AUTO-UPDATE enhanced fallback with current data")
        
        total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
        print(f"üìä {total_sources} RSS sources loaded")
        
        # Display content extraction capabilities
        print("\nüÜï CONTENT EXTRACTION CAPABILITIES:")
        if TRAFILATURA_AVAILABLE:
            print("‚úÖ Trafilatura: Advanced content extraction (Best)")
        else:
            print("‚ùå Trafilatura: Not available")
        
        if NEWSPAPER_AVAILABLE:
            print("‚úÖ Newspaper3k: Fallback content extraction")
        else:
            print("‚ùå Newspaper3k: Not available")
        
        print("‚úÖ Legacy extraction: Always available (Basic)")
        
        print("\n‚úÖ Multi-AI Debate System AUTO-UPDATE v3.0 ready!")
        print(f"üí° Use !hoi [question] to get AI answers with REAL {get_current_date_str()} data")
        print("üí° Use !all, !in, !out for news, !chitiet [number] for details with Trafilatura + auto-translate")
        print(f"üí° Date and time automatically update: {current_datetime_str}")
        print("üí° Content extraction: 3-tier system (Trafilatura ‚Üí Newspaper3k ‚Üí Legacy)")
        print("üí° All AI APIs fixed and working correctly")
        
        bot.run(TOKEN)
        
    except Exception as e:
        print(f"‚ùå Bot startup error: {e}")
    finally:
        try:
            asyncio.run(cleanup())
        except:
            pass
