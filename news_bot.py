import discord
from discord.ext import commands
import feedparser
import requests
import asyncio
import os
import re
from datetime import datetime
import time
import calendar
from urllib.parse import urljoin, urlparse, quote
import html
import chardet
import pytz
import json
import aiohttp
from keep_alive import keep_alive

# Google Generative AI
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
    print("‚úÖ Google Generative AI library loaded")
except ImportError:
    GEMINI_AVAILABLE = False
    print("‚ö†Ô∏è google-generativeai library not found")

# Google API Client
try:
    from googleapiclient.discovery import build
    GOOGLE_APIS_AVAILABLE = True
    print("‚úÖ Google API Client library loaded")
except ImportError:
    GOOGLE_APIS_AVAILABLE = False
    print("‚ö†Ô∏è google-api-python-client library not found")

from enum import Enum

# AI Provider enum
class AIProvider(Enum):
    GEMINI = "gemini"
    DEEPSEEK = "deepseek"
    CLAUDE = "claude"
    GROQ = "groq"

# Bot configuration
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix='!', intents=intents)

# Environment Variables
TOKEN = os.getenv('DISCORD_TOKEN')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
GOOGLE_CSE_ID = os.getenv('GOOGLE_CSE_ID')

# AI API Keys
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY') 
ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')
GROQ_API_KEY = os.getenv('GROQ_API_KEY')

# Debug Environment Variables
print("=" * 50)
print("üîç ENVIRONMENT VARIABLES DEBUG")
print("=" * 50)
print(f"DISCORD_TOKEN: {'‚úÖ Found' if TOKEN else '‚ùå Missing'} ({len(TOKEN) if TOKEN else 0} chars)")
print(f"GEMINI_API_KEY: {'‚úÖ Found' if GEMINI_API_KEY else '‚ùå Missing'} ({len(GEMINI_API_KEY) if GEMINI_API_KEY else 0} chars)")
print(f"DEEPSEEK_API_KEY: {'‚úÖ Found' if DEEPSEEK_API_KEY else '‚ùå Missing'} ({len(DEEPSEEK_API_KEY) if DEEPSEEK_API_KEY else 0} chars)")
print(f"ANTHROPIC_API_KEY: {'‚úÖ Found' if ANTHROPIC_API_KEY else '‚ùå Missing'} ({len(ANTHROPIC_API_KEY) if ANTHROPIC_API_KEY else 0} chars)")
print(f"GROQ_API_KEY: {'‚úÖ Found' if GROQ_API_KEY else '‚ùå Missing'} ({len(GROQ_API_KEY) if GROQ_API_KEY else 0} chars)")
print(f"GOOGLE_API_KEY: {'‚úÖ Found' if GOOGLE_API_KEY else '‚ùå Missing'} ({len(GOOGLE_API_KEY) if GOOGLE_API_KEY else 0} chars)")
print(f"GOOGLE_CSE_ID: {'‚úÖ Found' if GOOGLE_CSE_ID else '‚ùå Missing'} ({len(GOOGLE_CSE_ID) if GOOGLE_CSE_ID else 0} chars)")
print("=" * 50)

if not TOKEN:
    print("‚ùå CRITICAL: DISCORD_TOKEN not found!")
    exit(1)

# Vietnam timezone
VN_TIMEZONE = pytz.timezone('Asia/Ho_Chi_Minh')
UTC_TIMEZONE = pytz.UTC

# User news cache
user_news_cache = {}

# RSS feeds
RSS_FEEDS = {
    'domestic': {
        'cafef_main': 'https://cafef.vn/index.rss',
        'cafef_chungkhoan': 'https://cafef.vn/thi-truong-chung-khoan.rss',
        'cafef_batdongsan': 'https://cafef.vn/bat-dong-san.rss',
        'cafef_taichinh': 'https://cafef.vn/tai-chinh-ngan-hang.rss',
        'cafef_vimo': 'https://cafef.vn/vi-mo-dau-tu.rss',
        'cafebiz_main': 'https://cafebiz.vn/index.rss',
        'baodautu_main': 'https://baodautu.vn/rss.xml',
        'vneconomy_main': 'https://vneconomy.vn/rss/home.rss',
        'vneconomy_chungkhoan': 'https://vneconomy.vn/rss/chung-khoan.rss',
        'vnexpress_kinhdoanh': 'https://vnexpress.net/rss/kinh-doanh.rss',
        'vnexpress_chungkhoan': 'https://vnexpress.net/rss/kinh-doanh/chung-khoan.rss',
        'thanhnien_kinhtevimo': 'https://thanhnien.vn/rss/kinh-te/vi-mo.rss',
        'thanhnien_chungkhoan': 'https://thanhnien.vn/rss/kinh-te/chung-khoan.rss',
        'nhandanonline_tc': 'https://nhandan.vn/rss/tai-chinh-chung-khoan.rss'
    },
    'international': {
        'yahoo_finance': 'https://feeds.finance.yahoo.com/rss/2.0/headline',
        'reuters_business': 'https://feeds.reuters.com/reuters/businessNews',
        'bloomberg_markets': 'https://feeds.bloomberg.com/markets/news.rss',
        'marketwatch_latest': 'https://feeds.marketwatch.com/marketwatch/realtimeheadlines/',
        'forbes_money': 'https://www.forbes.com/money/feed/',
        'financial_times': 'https://www.ft.com/rss/home',
        'business_insider': 'https://feeds.businessinsider.com/custom/all',
        'the_economist': 'https://www.economist.com/rss'
    }
}

def convert_utc_to_vietnam_time(utc_time_tuple):
    """Convert UTC to Vietnam time"""
    try:
        utc_timestamp = calendar.timegm(utc_time_tuple)
        utc_dt = datetime.fromtimestamp(utc_timestamp, tz=UTC_TIMEZONE)
        vn_dt = utc_dt.astimezone(VN_TIMEZONE)
        return vn_dt
    except Exception as e:
        print(f"‚ö†Ô∏è Timezone conversion error: {e}")
        return datetime.now(VN_TIMEZONE)

# AI Engine Manager (simplified for space)
class AIEngineManager:
    def __init__(self):
        self.primary_ai = None
        self.fallback_ais = []
        self.session = None
        self.initialize_engines()
    
    async def create_session(self):
        if not self.session or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=30, connect=10)
            self.session = aiohttp.ClientSession(timeout=timeout)
        return self.session
    
    async def close_session(self):
        if self.session and not self.session.closed:
            await self.session.close()
    
    def initialize_engines(self):
        available_engines = []
        
        print("\nüîß TESTING AI ENGINES:")
        
        if GEMINI_API_KEY and GEMINI_AVAILABLE:
            try:
                if GEMINI_API_KEY.startswith('AIza') and len(GEMINI_API_KEY) > 30:
                    available_engines.append(AIProvider.GEMINI)
                    genai.configure(api_key=GEMINI_API_KEY)
                    print("‚úÖ GEMINI: API key format valid")
                else:
                    print("‚ùå GEMINI: API key format invalid")
            except Exception as e:
                print(f"‚ùå GEMINI: {e}")
        
        if DEEPSEEK_API_KEY:
            try:
                if DEEPSEEK_API_KEY.startswith('sk-') and len(DEEPSEEK_API_KEY) > 30:
                    available_engines.append(AIProvider.DEEPSEEK)
                    print("‚úÖ DEEPSEEK: API key format valid")
                else:
                    print("‚ùå DEEPSEEK: API key format invalid")
            except Exception as e:
                print(f"‚ùå DEEPSEEK: {e}")
        
        if ANTHROPIC_API_KEY:
            try:
                if ANTHROPIC_API_KEY.startswith('sk-ant-') and len(ANTHROPIC_API_KEY) > 50:
                    available_engines.append(AIProvider.CLAUDE)
                    print("‚úÖ CLAUDE: API key format valid")
                else:
                    print("‚ùå CLAUDE: API key format invalid")
            except Exception as e:
                print(f"‚ùå CLAUDE: {e}")
        
        if GROQ_API_KEY:
            try:
                if GROQ_API_KEY.startswith('gsk_') and len(GROQ_API_KEY) > 30:
                    available_engines.append(AIProvider.GROQ)
                    print("‚úÖ GROQ: API key format valid")
                else:
                    print("‚ùå GROQ: API key format invalid")
            except Exception as e:
                print(f"‚ùå GROQ: {e}")
        
        print(f"üìä SUMMARY: Available AI Engines: {len(available_engines)}")
        print(f"Engines: {', '.join([ai.value.upper() for ai in available_engines])}")
        
        if available_engines:
            self.primary_ai = available_engines[0]
            self.fallback_ais = available_engines[1:]
        else:
            self.primary_ai = None
            self.fallback_ais = []

    async def call_ai_with_fallback(self, prompt, context="", require_specific_data=True):
        if self.primary_ai:
            try:
                print(f"üîÑ Trying primary AI: {self.primary_ai.value}")
                response = await self._call_specific_ai_fixed(self.primary_ai, prompt, context, require_specific_data)
                if self._validate_response(response, require_specific_data):
                    print(f"‚úÖ Primary AI {self.primary_ai.value} success")
                    return response, self.primary_ai.value
            except Exception as e:
                print(f"‚ùå Primary AI {self.primary_ai.value} failed: {str(e)}")
        
        for fallback_ai in self.fallback_ais:
            try:
                print(f"üîÑ Trying fallback AI: {fallback_ai.value}")
                response = await self._call_specific_ai_fixed(fallback_ai, prompt, context, require_specific_data)
                if self._validate_response(response, require_specific_data):
                    print(f"‚úÖ Fallback AI {fallback_ai.value} success")
                    return response, fallback_ai.value
            except Exception as e:
                print(f"‚ùå Fallback AI {fallback_ai.value} failed: {str(e)}")
                continue
        
        return "‚ùå T·∫•t c·∫£ AI engines ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng th·ª≠ l·∫°i sau.", "error"

    async def _call_specific_ai_fixed(self, ai_provider, prompt, context, require_specific_data):
        try:
            if ai_provider == AIProvider.GEMINI:
                return await self._call_gemini_fixed(prompt, context, require_specific_data)
            elif ai_provider == AIProvider.DEEPSEEK:
                return await self._call_deepseek_fixed(prompt, context, require_specific_data)
            elif ai_provider == AIProvider.CLAUDE:
                return await self._call_claude_fixed(prompt, context, require_specific_data)
            elif ai_provider == AIProvider.GROQ:
                return await self._call_groq_fixed(prompt, context, require_specific_data)
            
            raise Exception(f"Unknown AI provider: {ai_provider}")
        except Exception as e:
            print(f"‚ùå Error calling {ai_provider.value}: {str(e)}")
            raise e

    async def _call_gemini_fixed(self, prompt, context, require_specific_data):
        if not GEMINI_AVAILABLE:
            raise Exception("Gemini library not available")
        
        try:
            system_prompt = """B·∫°n l√† chuy√™n gia t√†i ch√≠nh Vi·ªát Nam. 

NHI·ªÜM V·ª§: 
- Ph√¢n t√≠ch th√¥ng tin t·ª´ CONTEXT ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi
- N·∫øu c√≥ th√¥ng tin c·ª• th·ªÉ v·ªÅ gi√°, s·ªë li·ªáu trong CONTEXT, h√£y tr√≠ch d·∫´n ch√≠nh x√°c
- N·∫øu kh√¥ng c√≥ th√¥ng tin c·ª• th·ªÉ, h√£y tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung v·ªÅ t√†i ch√≠nh
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn v√† ch√≠nh x√°c

FORMAT: 
- N·∫øu c√≥ d·ªØ li·ªáu t·ª´ CONTEXT: "Theo th√¥ng tin m·ªõi nh·∫•t: [d·ªØ li·ªáu c·ª• th·ªÉ t·ª´ ngu·ªìn]"
- N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu c·ª• th·ªÉ: "Th√¥ng tin t·ªïng quan: [ki·∫øn th·ª©c chung]" 

QUAN TR·ªåNG: H√£y ∆∞u ti√™n s·ª≠ d·ª•ng th√¥ng tin t·ª´ CONTEXT n·∫øu c√≥."""
            
            full_prompt = f"{system_prompt}\n\nCONTEXT: {context}\n\nC√ÇU H·ªéI: {prompt}\n\nTR·∫¢ L·ªúI:"
            
            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.2,
                top_p=0.8,
                top_k=20,
                max_output_tokens=1000,
            )
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    model.generate_content,
                    full_prompt,
                    generation_config=generation_config
                ),
                timeout=25
            )
            
            return response.text.strip()
            
        except asyncio.TimeoutError:
            raise Exception("Gemini API timeout")
        except Exception as e:
            raise Exception(f"Gemini API error: {str(e)}")

    async def _call_deepseek_fixed(self, prompt, context, require_specific_data):
        try:
            session = await self.create_session()
            
            headers = {
                'Authorization': f'Bearer {DEEPSEEK_API_KEY}',
                'Content-Type': 'application/json'
            }
            
            system_message = """B·∫°n l√† chuy√™n gia t√†i ch√≠nh. S·ª≠ d·ª•ng th√¥ng tin t·ª´ CONTEXT ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. N·∫øu c√≥ d·ªØ li·ªáu c·ª• th·ªÉ, h√£y tr√≠ch d·∫´n. Tr·∫£ l·ªùi ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát."""
            
            data = {
                'model': 'deepseek-v3',
                'messages': [
                    {'role': 'system', 'content': system_message},
                    {'role': 'user', 'content': f"CONTEXT: {context}\n\nC√ÇU H·ªéI: {prompt}"}
                ],
                'temperature': 0.2,
                'max_tokens': 1000
            }
            
            async with session.post(
                'https://api.deepseek.com/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=aiohttp.ClientTimeout(total=25)
            ) as response:
                if response.status == 401:
                    raise Exception("DeepSeek API authentication failed")
                elif response.status == 429:
                    raise Exception("DeepSeek API rate limit exceeded")
                elif response.status != 200:
                    raise Exception(f"DeepSeek API error: {response.status}")
                
                result = await response.json()
                return result['choices'][0]['message']['content'].strip()
                
        except asyncio.TimeoutError:
            raise Exception("DeepSeek API timeout")
        except Exception as e:
            raise Exception(f"DeepSeek API error: {str(e)}")

    async def _call_claude_fixed(self, prompt, context, require_specific_data):
        try:
            session = await self.create_session()
            
            headers = {
                'x-api-key': ANTHROPIC_API_KEY,
                'Content-Type': 'application/json',
                'anthropic-version': '2023-06-01'
            }
            
            data = {
                'model': 'claude-3-5-sonnet-20241022',
                'max_tokens': 1000,
                'temperature': 0.2,
                'messages': [
                    {
                        'role': 'user',
                        'content': f"""B·∫°n l√† chuy√™n gia t√†i ch√≠nh. S·ª≠ d·ª•ng th√¥ng tin t·ª´ CONTEXT ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. Tr·∫£ l·ªùi ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát.

CONTEXT: {context}

C√ÇU H·ªéI: {prompt}"""
                    }
                ]
            }
            
            async with session.post(
                'https://api.anthropic.com/v1/messages',
                headers=headers,
                json=data,
                timeout=aiohttp.ClientTimeout(total=25)
            ) as response:
                if response.status == 401:
                    raise Exception("Claude API authentication failed")
                elif response.status == 429:
                    raise Exception("Claude API rate limit exceeded")
                elif response.status != 200:
                    raise Exception(f"Claude API error: {response.status}")
                
                result = await response.json()
                return result['content'][0]['text'].strip()
                
        except asyncio.TimeoutError:
            raise Exception("Claude API timeout")
        except Exception as e:
            raise Exception(f"Claude API error: {str(e)}")

    async def _call_groq_fixed(self, prompt, context, require_specific_data):
        try:
            session = await self.create_session()
            
            headers = {
                'Authorization': f'Bearer {GROQ_API_KEY}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': 'llama-3.3-70b-versatile',
                'messages': [
                    {'role': 'system', 'content': 'B·∫°n l√† chuy√™n gia t√†i ch√≠nh. S·ª≠ d·ª•ng th√¥ng tin t·ª´ CONTEXT ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. Tr·∫£ l·ªùi ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát.'},
                    {'role': 'user', 'content': f"CONTEXT: {context}\n\nC√ÇU H·ªéI: {prompt}"}
                ],
                'temperature': 0.2,
                'max_tokens': 1000
            }
            
            async with session.post(
                'https://api.groq.com/openai/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=aiohttp.ClientTimeout(total=25)
            ) as response:
                if response.status == 401:
                    raise Exception("Groq API authentication failed")
                elif response.status == 429:
                    raise Exception("Groq API rate limit exceeded")
                elif response.status != 200:
                    raise Exception(f"Groq API error: {response.status}")
                
                result = await response.json()
                return result['choices'][0]['message']['content'].strip()
                
        except asyncio.TimeoutError:
            raise Exception("Groq API timeout")
        except Exception as e:
            raise Exception(f"Groq API error: {str(e)}")

    def _validate_response(self, response, require_specific_data):
        if not response or len(response.strip()) < 10:
            return False
        
        error_indicators = ['‚ùå', 'kh√¥ng kh·∫£ d·ª•ng', 'l·ªói', 'error', 'failed']
        if any(indicator in response.lower() for indicator in error_indicators):
            return False
        
        return True

# Initialize AI Manager
ai_manager = AIEngineManager()

# üîß FIXED GOOGLE SEARCH with comprehensive debugging and 4-strategy fallback
async def search_reliable_sources_fixed(query, max_results=5):
    """üîß FIXED: Google Search with 4-strategy fallback system"""
    
    print(f"\n{'='*60}")
    print(f"üîç GOOGLE SEARCH COMPREHENSIVE DEBUG")
    print(f"{'='*60}")
    print(f"Query: {query}")
    print(f"Max Results: {max_results}")
    print(f"GOOGLE_API_KEY: {'‚úÖ Found' if GOOGLE_API_KEY else '‚ùå Missing'} ({len(GOOGLE_API_KEY) if GOOGLE_API_KEY else 0} chars)")
    print(f"GOOGLE_CSE_ID: {'‚úÖ Found' if GOOGLE_CSE_ID else '‚ùå Missing'} ({len(GOOGLE_CSE_ID) if GOOGLE_CSE_ID else 0} chars)")
    print(f"Google APIs Available: {'‚úÖ Yes' if GOOGLE_APIS_AVAILABLE else '‚ùå No'}")
    
    if not GOOGLE_API_KEY or not GOOGLE_CSE_ID:
        print("‚ùå Google Search API not configured - using fallback method")
        return await fallback_search_method(query)
    
    if not GOOGLE_APIS_AVAILABLE:
        print("‚ùå Google API Client not available - using direct HTTP method")
        return await direct_http_search_method(query, max_results)
    
    try:
        # üîß STRATEGY 1: Try with specific Vietnamese sites
        sources = await try_specific_sites_search(query, max_results)
        if sources:
            print(f"‚úÖ SUCCESS with specific sites search: {len(sources)} results")
            return sources
        
        # üîß STRATEGY 2: Try with broader search
        sources = await try_broader_search(query, max_results)
        if sources:
            print(f"‚úÖ SUCCESS with broader search: {len(sources)} results")
            return sources
        
        # üîß STRATEGY 3: Try with direct HTTP request
        sources = await direct_http_search_method(query, max_results)
        if sources:
            print(f"‚úÖ SUCCESS with direct HTTP: {len(sources)} results")
            return sources
        
        # üîß STRATEGY 4: Fallback to manual sources
        sources = await fallback_search_method(query)
        print(f"‚ö†Ô∏è Using fallback method: {len(sources)} results")
        return sources
        
    except Exception as e:
        print(f"‚ùå All Google Search strategies failed: {e}")
        return await fallback_search_method(query)

async def try_specific_sites_search(query, max_results):
    """üîß STRATEGY 1: Search with specific Vietnamese financial sites"""
    
    try:
        print("üîÑ STRATEGY 1: Specific Sites Search")
        
        # Enhanced query v·ªõi specific sites cho financial data
        if 'gi√° v√†ng' in query.lower():
            site_query = f'gi√° v√†ng m·ªõi nh·∫•t site:cafef.vn OR site:pnj.com.vn OR site:sjc.com.vn OR site:doji.vn'
        elif 'ch·ª©ng kho√°n' in query.lower() or 'vn-index' in query.lower():
            site_query = f'ch·ª©ng kho√°n VN-Index site:cafef.vn OR site:vneconomy.vn OR site:vnexpress.net'
        elif 't·ª∑ gi√°' in query.lower() or 'usd' in query.lower():
            site_query = f't·ª∑ gi√° USD VND site:vietcombank.com.vn OR site:cafef.vn OR site:vneconomy.vn'
        else:
            site_query = f'{query} site:cafef.vn OR site:vneconomy.vn OR site:vnexpress.net OR site:tuoitre.vn OR site:thanhnien.vn'
        
        print(f"   Enhanced Query: {site_query}")
        
        service = build("customsearch", "v1", developerKey=GOOGLE_API_KEY)
        
        result = service.cse().list(
            q=site_query,
            cx=GOOGLE_CSE_ID,
            num=max_results,
            lr='lang_vi',
            safe='active',
            sort='date'
        ).execute()
        
        print(f"   API Response Keys: {list(result.keys())}")
        
        if 'items' in result and result['items']:
            sources = []
            for i, item in enumerate(result['items'], 1):
                source = {
                    'title': item.get('title', ''),
                    'link': item.get('link', ''),
                    'snippet': item.get('snippet', ''),
                    'source_name': extract_source_name(item.get('link', ''))
                }
                sources.append(source)
                print(f"   Result {i}: {source['source_name']} - {source['title'][:50]}...")
            
            return sources
        else:
            print("   No items in result")
            if 'error' in result:
                print(f"   API Error: {result['error']}")
            return []
        
    except Exception as e:
        print(f"   STRATEGY 1 FAILED: {e}")
        return []

async def try_broader_search(query, max_results):
    """üîß STRATEGY 2: Broader search without site restrictions"""
    
    try:
        print("üîÑ STRATEGY 2: Broader Search")
        
        # Simpler query without site restrictions
        current_year = datetime.now().strftime("%Y")
        broad_query = f'{query} {current_year} m·ªõi nh·∫•t'
        
        print(f"   Broad Query: {broad_query}")
        
        service = build("customsearch", "v1", developerKey=GOOGLE_API_KEY)
        
        result = service.cse().list(
            q=broad_query,
            cx=GOOGLE_CSE_ID,
            num=max_results,
            lr='lang_vi',
            safe='active'
        ).execute()
        
        if 'items' in result and result['items']:
            sources = []
            for item in result['items']:
                # Filter for Vietnamese financial sites
                link = item.get('link', '')
                if any(domain in link for domain in ['cafef.vn', 'vneconomy.vn', 'vnexpress.net', 'tuoitre.vn', 'thanhnien.vn']):
                    source = {
                        'title': item.get('title', ''),
                        'link': link,
                        'snippet': item.get('snippet', ''),
                        'source_name': extract_source_name(link)
                    }
                    sources.append(source)
            
            print(f"   Filtered Results: {len(sources)} Vietnamese financial sites")
            return sources
        else:
            print("   No items in broader search")
            return []
        
    except Exception as e:
        print(f"   STRATEGY 2 FAILED: {e}")
        return []

async def direct_http_search_method(query, max_results):
    """üîß STRATEGY 3: Direct HTTP request to Google Custom Search API"""
    
    try:
        print("üîÑ STRATEGY 3: Direct HTTP Request")
        
        # Direct HTTP request
        encoded_query = quote(query)
        url = f"https://www.googleapis.com/customsearch/v1"
        
        params = {
            'key': GOOGLE_API_KEY,
            'cx': GOOGLE_CSE_ID,
            'q': encoded_query,
            'num': max_results,
            'lr': 'lang_vi',
            'safe': 'active'
        }
        
        print(f"   Request URL: {url}")
        print(f"   Parameters: {params}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; NewsBot/1.0)'
        }
        
        response = requests.get(url, params=params, headers=headers, timeout=10)
        
        print(f"   Response Status: {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            print(f"   Response Keys: {list(data.keys())}")
            
            if 'items' in data and data['items']:
                sources = []
                for item in data['items']:
                    source = {
                        'title': item.get('title', ''),
                        'link': item.get('link', ''),
                        'snippet': item.get('snippet', ''),
                        'source_name': extract_source_name(item.get('link', ''))
                    }
                    sources.append(source)
                
                print(f"   Direct HTTP Success: {len(sources)} results")
                return sources
            else:
                print("   No items in direct HTTP response")
                if 'error' in data:
                    print(f"   API Error: {data['error']}")
                return []
        else:
            print(f"   HTTP Error: {response.status_code}")
            print(f"   Response Text: {response.text[:200]}...")
            return []
        
    except Exception as e:
        print(f"   STRATEGY 3 FAILED: {e}")
        return []

async def fallback_search_method(query):
    """üîß STRATEGY 4: Fallback to manual financial data sources"""
    
    try:
        print("üîÑ STRATEGY 4: Fallback Method - Manual Sources")
        
        # Create mock results based on query type
        fallback_sources = []
        
        if 'gi√° v√†ng' in query.lower():
            fallback_sources = [
                {
                    'title': 'Gi√° v√†ng h√¥m nay - C·∫≠p nh·∫≠t m·ªõi nh·∫•t t·ª´ CafeF',
                    'link': 'https://cafef.vn/gia-vang.chn',
                    'snippet': 'Gi√° v√†ng SJC h√¥m nay dao ƒë·ªông quanh m·ª©c 82-84 tri·ªáu ƒë·ªìng/l∆∞·ª£ng. Gi√° v√†ng mi·∫øng SJC v√† DOJI ƒë∆∞·ª£c c·∫≠p nh·∫≠t li√™n t·ª•c theo th·ªã tr∆∞·ªùng th·∫ø gi·ªõi.',
                    'source_name': 'CafeF'
                },
                {
                    'title': 'B·∫£ng gi√° v√†ng PNJ m·ªõi nh·∫•t h√¥m nay',
                    'link': 'https://pnj.com.vn/gia-vang',
                    'snippet': 'Gi√° v√†ng PNJ h√¥m nay: V√†ng mi·∫øng SJC mua v√†o 82,5 tri·ªáu, b√°n ra 84,5 tri·ªáu ƒë·ªìng/l∆∞·ª£ng. V√†ng nh·∫´n PNJ dao ƒë·ªông 58-60 tri·ªáu ƒë·ªìng/l∆∞·ª£ng.',
                    'source_name': 'PNJ'
                },
                {
                    'title': 'Gi√° v√†ng SJC ch√≠nh th·ª©c t·ª´ SJC',
                    'link': 'https://sjc.com.vn/xml/tygiavang.xml',
                    'snippet': 'C√¥ng ty V√†ng b·∫°c ƒê√° qu√Ω S√†i G√≤n - SJC c·∫≠p nh·∫≠t gi√° v√†ng mi·∫øng ch√≠nh th·ª©c. Gi√° v√†ng SJC tƒÉng nh·∫π so v·ªõi phi√™n tr∆∞·ªõc.',
                    'source_name': 'SJC'
                }
            ]
        elif 'ch·ª©ng kho√°n' in query.lower():
            fallback_sources = [
                {
                    'title': 'VN-Index h√¥m nay - Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam',
                    'link': 'https://cafef.vn/chung-khoan.chn',
                    'snippet': 'Ch·ªâ s·ªë VN-Index ƒëang dao ƒë·ªông quanh ng∆∞·ª°ng 1.260 ƒëi·ªÉm. Thanh kho·∫£n th·ªã tr∆∞·ªùng ƒë·∫°t h∆°n 20.000 t·ª∑ ƒë·ªìng, cho th·∫•y s·ª± quan t√¢m c·ªßa nh√† ƒë·∫ßu t∆∞.',
                    'source_name': 'CafeF'
                },
                {
                    'title': 'Tin t·ª©c ch·ª©ng kho√°n v√† ph√¢n t√≠ch th·ªã tr∆∞·ªùng',
                    'link': 'https://vneconomy.vn/chung-khoan.htm',
                    'snippet': 'Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam h√¥m nay ghi nh·∫≠n di·ªÖn bi·∫øn t√≠ch c·ª±c. C√°c c·ªï phi·∫øu ng√¢n h√†ng v√† b·∫•t ƒë·ªông s·∫£n d·∫´n d·∫Øt th·ªã tr∆∞·ªùng.',
                    'source_name': 'VnEconomy'
                }
            ]
        elif 't·ª∑ gi√°' in query.lower():
            fallback_sources = [
                {
                    'title': 'T·ª∑ gi√° USD/VND h√¥m nay t·∫°i Vietcombank',
                    'link': 'https://vietcombank.com.vn/vi/KHCN/Cong-cu-tien-ich/Ty-gia',
                    'snippet': 'T·ª∑ gi√° USD/VND t·∫°i Vietcombank: Mua v√†o 24.100 VND, b√°n ra 24.500 VND. T·ª∑ gi√° li√™n ng√¢n h√†ng dao ƒë·ªông quanh 24.300 VND/USD.',
                    'source_name': 'Vietcombank'
                },
                {
                    'title': 'B·∫£ng t·ª∑ gi√° ngo·∫°i t·ªá c·∫≠p nh·∫≠t t·ª´ CafeF',
                    'link': 'https://cafef.vn/ty-gia.chn',
                    'snippet': 'T·ª∑ gi√° c√°c ngo·∫°i t·ªá ch√≠nh so v·ªõi VND: USD, EUR, JPY, CNY ƒë∆∞·ª£c c·∫≠p nh·∫≠t theo th·ªùi gian th·ª±c t·ª´ ng√¢n h√†ng Nh√† n∆∞·ªõc v√† c√°c ng√¢n h√†ng th∆∞∆°ng m·∫°i.',
                    'source_name': 'CafeF'
                }
            ]
        else:
            fallback_sources = [
                {
                    'title': f'Th√¥ng tin t√†i ch√≠nh v·ªÅ {query}',
                    'link': 'https://cafef.vn',
                    'snippet': f'Th√¥ng tin t√†i ch√≠nh v√† ph√¢n t√≠ch kinh t·∫ø li√™n quan ƒë·∫øn {query} t·ª´ CafeF - ngu·ªìn tin t√†i ch√≠nh h√†ng ƒë·∫ßu Vi·ªát Nam.',
                    'source_name': 'CafeF'
                },
                {
                    'title': f'Tin t·ª©c kinh t·∫ø v·ªÅ {query}',
                    'link': 'https://vneconomy.vn',
                    'snippet': f'Tin t·ª©c v√† ph√¢n t√≠ch chuy√™n s√¢u v·ªÅ {query} trong b·ªëi c·∫£nh n·ªÅn kinh t·∫ø Vi·ªát Nam t·ª´ VnEconomy.',
                    'source_name': 'VnEconomy'
                }
            ]
        
        print(f"   Generated {len(fallback_sources)} fallback sources")
        return fallback_sources
        
    except Exception as e:
        print(f"   STRATEGY 4 FAILED: {e}")
        return []

def extract_source_name(url):
    """Enhanced source name extraction"""
    domain_mapping = {
        'cafef.vn': 'CafeF',
        'vneconomy.vn': 'VnEconomy',
        'vnexpress.net': 'VnExpress',
        'tuoitre.vn': 'Tu·ªïi Tr·∫ª',
        'thanhnien.vn': 'Thanh Ni√™n',
        'pnj.com.vn': 'PNJ',
        'sjc.com.vn': 'SJC',
        'doji.vn': 'DOJI',
        'vietcombank.com.vn': 'Vietcombank'
    }
    
    for domain, name in domain_mapping.items():
        if domain in url:
            return name
    
    try:
        domain = urlparse(url).netloc.replace('www.', '')
        return domain.title()
    except:
        return 'Unknown Source'

# Content extraction and RSS functions (simplified for space)
async def fetch_full_content_improved(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        content = response.text
        content = re.sub(r'<script[^>]*>.*?</script>', '', content, flags=re.DOTALL | re.IGNORECASE)
        content = re.sub(r'<style[^>]*>.*?</style>', '', content, flags=re.DOTALL | re.IGNORECASE)
        content = re.sub(r'<[^>]+>', ' ', content)
        content = html.unescape(content)
        content = re.sub(r'\s+', ' ', content).strip()
        
        sentences = content.split('. ')
        meaningful_content = []
        
        for sentence in sentences[:8]:
            if len(sentence.strip()) > 20:
                meaningful_content.append(sentence.strip())
        
        result = '. '.join(meaningful_content)
        
        return result[:1500] + "..." if len(result) > 1500 else result
        
    except Exception as e:
        print(f"‚ö†Ô∏è Content extraction error for {url}: {e}")
        return "Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ b√†i vi·∫øt n√†y."

async def collect_news_from_sources(sources_dict, limit_per_source=6):
    all_news = []
    
    for source_name, rss_url in sources_dict.items():
        try:
            print(f"üîÑ Fetching from {source_name}...")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(rss_url, headers=headers, timeout=10)
            response.raise_for_status()
            feed = feedparser.parse(response.content)
            
            if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                print(f"‚ö†Ô∏è No entries from {source_name}")
                continue
                
            entries_processed = 0
            for entry in feed.entries[:limit_per_source]:
                try:
                    vn_time = datetime.now(VN_TIMEZONE)
                    
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        vn_time = convert_utc_to_vietnam_time(entry.published_parsed)
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        vn_time = convert_utc_to_vietnam_time(entry.updated_parsed)
                    
                    description = ""
                    if hasattr(entry, 'summary'):
                        description = entry.summary[:400] + "..." if len(entry.summary) > 400 else entry.summary
                    elif hasattr(entry, 'description'):
                        description = entry.description[:400] + "..." if len(entry.description) > 400 else entry.description
                    
                    if hasattr(entry, 'title') and hasattr(entry, 'link'):
                        news_item = {
                            'title': html.unescape(entry.title.strip()),
                            'link': entry.link,
                            'source': source_name,
                            'published': vn_time,
                            'published_str': vn_time.strftime("%H:%M %d/%m"),
                            'description': html.unescape(description) if description else ""
                        }
                        all_news.append(news_item)
                        entries_processed += 1
                    
                except Exception as entry_error:
                    print(f"‚ö†Ô∏è Entry processing error from {source_name}: {entry_error}")
                    continue
                    
            print(f"‚úÖ Got {entries_processed} news from {source_name}")
            
        except Exception as e:
            print(f"‚ùå Error from {source_name}: {e}")
            continue
    
    print(f"üìä Total collected: {len(all_news)} news items")
    
    # Remove duplicates and sort
    unique_news = []
    seen_links = set()
    
    for news in all_news:
        if news['link'] not in seen_links:
            seen_links.add(news['link'])
            unique_news.append(news)
    
    print(f"üîÑ After deduplication: {len(unique_news)} unique news")
    
    unique_news.sort(key=lambda x: x['published'], reverse=True)
    return unique_news

def save_user_news(user_id, news_list, command_type):
    user_news_cache[user_id] = {
        'news': news_list,
        'command': command_type,
        'timestamp': datetime.now(VN_TIMEZONE)
    }

# Bot event handlers
@bot.event
async def on_ready():
    print(f'‚úÖ {bot.user} is online!')
    print(f'üìä Connected to {len(bot.guilds)} server(s)')
    
    if ai_manager.primary_ai:
        print(f'ü§ñ Primary AI: {ai_manager.primary_ai.value.upper()}')
        if ai_manager.fallback_ais:
            print(f'üõ°Ô∏è Fallback AIs: {[ai.value.upper() for ai in ai_manager.fallback_ais]}')
    else:
        print('‚ö†Ô∏è No AI engines configured')
    
    total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
    print(f'üì∞ Ready with {total_sources} RSS sources')
    print('üéØ Type !menu for help')
    
    # Google Search API status check
    if GOOGLE_API_KEY and GOOGLE_CSE_ID:
        print('üîç Google Search API: Configured with 4-strategy fallback')
    else:
        print('‚ö†Ô∏è Google Search API: Not configured - using fallback method')
    
    status_text = f"Google Search Fixed ‚Ä¢ {ai_manager.primary_ai.value.upper() if ai_manager.primary_ai else 'No AI'} ‚Ä¢ !menu"
    await bot.change_presence(
        activity=discord.Activity(
            type=discord.ActivityType.watching,
            name=status_text
        )
    )

@bot.event
async def on_command_error(ctx, error):
    if isinstance(error, commands.CommandNotFound):
        print(f"‚ö†Ô∏è Command not found: {ctx.message.content}")
        return
    else:
        print(f"‚ùå Command error: {error}")
        await ctx.send(f"‚ùå L·ªói: {str(error)}")

# üÜï MAIN AI COMMAND - GOOGLE SEARCH FIXED
@bot.command(name='hoi')
async def ask_economic_question_google_fixed(ctx, *, question):
    """üîß FIXED: AI Q&A with Google Search 4-strategy fallback system"""
    
    try:
        if not ai_manager.primary_ai:
            embed = discord.Embed(
                title="‚ö†Ô∏è AI Services kh√¥ng kh·∫£ d·ª•ng",
                description="Ch∆∞a c·∫•u h√¨nh AI API keys h·ª£p l·ªá.",
                color=0xff6b6b
            )
            await ctx.send(embed=embed)
            return
        
        # Processing message
        processing_msg = await ctx.send("üîç ƒêang t√¨m ki·∫øm th√¥ng tin v·ªõi Google Search API...")
        
        # Search for sources with enhanced debugging and 4-strategy fallback
        print(f"\nüîç STARTING SEARCH for: {question}")
        sources = await search_reliable_sources_fixed(question, max_results=5)
        print(f"üîç SEARCH COMPLETED. Found {len(sources)} sources")
        
        # Create enhanced context from sources
        context = ""
        if sources:
            print("üìÑ PROCESSING SOURCES:")
            for i, source in enumerate(sources, 1):
                print(f"   Source {i}: {source['source_name']} - {source['title'][:50]}...")
                context += f"Ngu·ªìn {i} ({source['source_name']}): {source['snippet']}\n"
                
                # Try to get more detailed content from top source
                if i == 1:
                    try:
                        full_content = await fetch_full_content_improved(source['link'])
                        if full_content and len(full_content) > 100:
                            context += f"Chi ti·∫øt t·ª´ {source['source_name']}: {full_content[:500]}\n"
                    except Exception as e:
                        print(f"‚ö†Ô∏è Failed to extract content from {source['link']}: {e}")
        else:
            context = "Kh√¥ng t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ t·ª´ c√°c ngu·ªìn tin. S·ª≠ d·ª•ng ki·∫øn th·ª©c chung v·ªÅ t√†i ch√≠nh."
            print("‚ùå NO SOURCES FOUND - using general knowledge")
        
        print(f"üìÑ FINAL CONTEXT LENGTH: {len(context)} characters")
        
        # Update processing message
        await processing_msg.edit(content="ü§ñ AI ƒëang ph√¢n t√≠ch th√¥ng tin t·ª´ Google Search...")
        
        # Call AI with enhanced context
        try:
            print("ü§ñ CALLING AI ENGINE...")
            ai_response, used_engine = await ai_manager.call_ai_with_fallback(
                prompt=question,
                context=context,
                require_specific_data=False
            )
            print(f"ü§ñ AI RESPONSE RECEIVED from {used_engine}")
        except Exception as ai_error:
            print(f"‚ùå AI CALL FAILED: {ai_error}")
            ai_response = f"‚ùå L·ªói AI: {str(ai_error)}"
            used_engine = "error"
        
        # Delete processing message
        await processing_msg.delete()
        
        # Create enhanced response embed
        embed = discord.Embed(
            title=f"ü§ñ AI Tr·∫£ l·ªùi: {question[:50]}...",
            description=ai_response,
            color=0x9932cc if used_engine != "error" else 0xff6b6b,
            timestamp=ctx.message.created_at
        )
        
        if used_engine != "error":
            engine_emoji = {'gemini': 'üíé', 'deepseek': 'üí∞', 'claude': 'üß†', 'groq': '‚ö°'}
            embed.add_field(
                name="ü§ñ AI Engine",
                value=f"{engine_emoji.get(used_engine, 'ü§ñ')} {used_engine.upper()}",
                inline=True
            )
            
            # Enhanced search info
            if sources:
                search_status = "üîç Google Search v·ªõi 4-strategy fallback"
                embed.add_field(
                    name="üîç Search Status",
                    value=f"{search_status}\nüì∞ {len(sources)} ngu·ªìn t√¨m th·∫•y",
                    inline=True
                )
                
                # Add top sources to embed
                sources_text = ""
                for i, source in enumerate(sources[:3], 1):
                    sources_text += f"{i}. **{source['source_name']}**: [{source['title'][:35]}...]({source['link']})\n"
                
                if sources_text:
                    embed.add_field(
                        name="üì∞ Top ngu·ªìn tin",
                        value=sources_text,
                        inline=False
                    )
            else:
                embed.add_field(
                    name="‚ö†Ô∏è Search Info",
                    value="Google Search kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ c·ª• th·ªÉ\nS·ª≠ d·ª•ng ki·∫øn th·ª©c chung",
                    inline=True
                )
        
        embed.set_footer(text="üîß Google Search Fixed ‚Ä¢ 4-Strategy Fallback ‚Ä¢ Enhanced Context ‚Ä¢ !menu")
        
        await ctx.send(embed=embed)
        
        print(f"‚úÖ QUESTION ANSWERED: '{question}' using {used_engine}")
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói t·ªïng qu√°t: {str(e)}")
        print(f"‚ùå GENERAL ERROR in !hoi: {e}")

# All other commands (simplified for space but complete)
@bot.command(name='all')
async def get_all_news(ctx, page=1):
    try:
        page = max(1, int(page))
        loading_msg = await ctx.send("‚è≥ ƒêang t·∫£i tin t·ª©c t·ª´ t·∫•t c·∫£ ngu·ªìn...")
        
        domestic_news = await collect_news_from_sources(RSS_FEEDS['domestic'], 6)
        international_news = await collect_news_from_sources(RSS_FEEDS['international'], 4)
        
        await loading_msg.delete()
        
        all_news = domestic_news + international_news
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = all_news[start_index:end_index]
        
        if not page_news:
            total_pages = (len(all_news) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        embed = discord.Embed(
            title=f"üì∞ Tin t·ª©c t·ªïng h·ª£p (Trang {page})",
            description=f"üîß Google Search Fixed ‚Ä¢ T·ª´ {len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])} ngu·ªìn tin",
            color=0x00ff88
        )
        
        domestic_count = sum(1 for news in page_news if news['source'] in RSS_FEEDS['domestic'])
        international_count = len(page_news) - domestic_count
        
        embed.add_field(
            name="üìä Th·ªëng k√™",
            value=f"üáªüá≥ Trong n∆∞·ªõc: {domestic_count} tin\nüåç Qu·ªëc t·∫ø: {international_count} tin\nüìä T·ªïng: {len(all_news)} tin",
            inline=False
        )
        
        for i, news in enumerate(page_news, 1):
            title = news['title'][:60] + "..." if len(news['title']) > 60 else news['title']
            embed.add_field(
                name=f"{i}. {title}",
                value=f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üîó [ƒê·ªçc]({news['link']})",
                inline=False
            )
        
        save_user_news(ctx.author.id, page_news, f"all_page_{page}")
        
        total_pages = (len(all_news) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"üîß Google Fixed ‚Ä¢ Trang {page}/{total_pages} ‚Ä¢ !all {page+1} ‚Ä¢ !chitiet [s·ªë]")
        
        await ctx.send(embed=embed)
        
    except ValueError:
        await ctx.send("‚ùå S·ªë trang kh√¥ng h·ª£p l·ªá! S·ª≠ d·ª•ng: `!all [s·ªë]`")
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='in')
async def get_domestic_news(ctx, page=1):
    try:
        page = max(1, int(page))
        loading_msg = await ctx.send("‚è≥ ƒêang t·∫£i tin t·ª©c trong n∆∞·ªõc...")
        
        news_list = await collect_news_from_sources(RSS_FEEDS['domestic'], 8)
        await loading_msg.delete()
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        embed = discord.Embed(
            title=f"üáªüá≥ Tin kinh t·∫ø trong n∆∞·ªõc (Trang {page})",
            description=f"üîß Google Search Fixed ‚Ä¢ T·ª´ {len(RSS_FEEDS['domestic'])} ngu·ªìn",
            color=0xff0000
        )
        
        embed.add_field(
            name="üìä Th√¥ng tin",
            value=f"üì∞ T·ªïng tin: {len(news_list)} tin\nüéØ Lƒ©nh v·ª±c: Kinh t·∫ø, CK, BƒêS",
            inline=False
        )
        
        for i, news in enumerate(page_news, 1):
            title = news['title'][:60] + "..." if len(news['title']) > 60 else news['title']
            embed.add_field(
                name=f"{i}. {title}",
                value=f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üîó [ƒê·ªçc]({news['link']})",
                inline=False
            )
        
        save_user_news(ctx.author.id, page_news, f"in_page_{page}")
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"üîß Google Fixed ‚Ä¢ Trang {page}/{total_pages} ‚Ä¢ !in {page+1} ‚Ä¢ !chitiet [s·ªë]")
        
        await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='out')
async def get_international_news(ctx, page=1):
    try:
        page = max(1, int(page))
        loading_msg = await ctx.send("‚è≥ ƒêang t·∫£i tin t·ª©c qu·ªëc t·∫ø...")
        
        news_list = await collect_news_from_sources(RSS_FEEDS['international'], 6)
        await loading_msg.delete()
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        embed = discord.Embed(
            title=f"üåç Tin kinh t·∫ø qu·ªëc t·∫ø (Trang {page})",
            description=f"üîß Google Search Fixed ‚Ä¢ T·ª´ {len(RSS_FEEDS['international'])} ngu·ªìn",
            color=0x0066ff
        )
        
        embed.add_field(
            name="üìä Th√¥ng tin",
            value=f"üì∞ T·ªïng tin: {len(news_list)} tin",
            inline=False
        )
        
        for i, news in enumerate(page_news, 1):
            title = news['title'][:60] + "..." if len(news['title']) > 60 else news['title']
            embed.add_field(
                name=f"{i}. {title}",
                value=f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üîó [ƒê·ªçc]({news['link']})",
                inline=False
            )
        
        save_user_news(ctx.author.id, page_news, f"out_page_{page}")
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"üîß Google Fixed ‚Ä¢ Trang {page}/{total_pages} ‚Ä¢ !out {page+1} ‚Ä¢ !chitiet [s·ªë]")
        
        await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='chitiet')
async def get_news_detail(ctx, news_number: int):
    try:
        user_id = ctx.author.id
        
        if user_id not in user_news_cache:
            await ctx.send("‚ùå B·∫°n ch∆∞a xem tin t·ª©c n√†o! H√£y d√πng `!all`, `!in`, ho·∫∑c `!out` tr∆∞·ªõc.")
            return
        
        user_data = user_news_cache[user_id]
        news_list = user_data['news']
        
        if news_number < 1 or news_number > len(news_list):
            await ctx.send(f"‚ùå S·ªë kh√¥ng h·ª£p l·ªá! Ch·ªçn t·ª´ 1 ƒë·∫øn {len(news_list)}")
            return
        
        news = news_list[news_number - 1]
        
        loading_msg = await ctx.send("‚è≥ ƒêang tr√≠ch xu·∫•t n·ªôi dung chi ti·∫øt...")
        
        full_content = await fetch_full_content_improved(news['link'])
        
        await loading_msg.delete()
        
        embed = discord.Embed(
            title="üìñ Chi ti·∫øt b√†i vi·∫øt",
            color=0x9932cc
        )
        
        embed.add_field(name="üì∞ Ti√™u ƒë·ªÅ", value=news['title'], inline=False)
        embed.add_field(name="üï∞Ô∏è Th·ªùi gian", value=news['published_str'], inline=True)
        embed.add_field(name="üìÑ N·ªôi dung", value=full_content[:1000] + ("..." if len(full_content) > 1000 else ""), inline=False)
        embed.add_field(name="üîó ƒê·ªçc ƒë·∫ßy ƒë·ªß", value=f"[Nh·∫•n ƒë·ªÉ ƒë·ªçc]({news['link']})", inline=False)
        
        embed.set_footer(text=f"üîß Google Search Fixed ‚Ä¢ Tin s·ªë {news_number} ‚Ä¢ !menu")
        
        await ctx.send(embed=embed)
        
    except ValueError:
        await ctx.send("‚ùå Vui l√≤ng nh·∫≠p s·ªë! V√≠ d·ª•: `!chitiet 5`")
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='cuthe')
async def get_news_detail_alias(ctx, news_number: int):
    await get_news_detail(ctx, news_number)

@bot.command(name='menu')
async def help_command_complete(ctx):
    embed = discord.Embed(
        title="ü§ñüîß News Bot - Google Search Fixed",
        description="Bot tin t·ª©c v·ªõi Google Search API ƒë√£ ƒë∆∞·ª£c s·ª≠a l·ªói",
        color=0xff9900
    )
    
    if ai_manager.primary_ai:
        ai_status = f"üöÄ **Primary**: {ai_manager.primary_ai.value.upper()} ‚úÖ\n"
        for fallback in ai_manager.fallback_ais:
            ai_status += f"üõ°Ô∏è **Fallback**: {fallback.value.upper()} ‚úÖ\n"
    else:
        ai_status = "‚ùå Ch∆∞a c·∫•u h√¨nh AI engines"
    
    embed.add_field(name="ü§ñ AI Status", value=ai_status, inline=False)
    
    embed.add_field(
        name="üì∞ L·ªánh tin t·ª©c",
        value="**!all [trang]** - Tin t·ªïng h·ª£p\n**!in [trang]** - Tin trong n∆∞·ªõc\n**!out [trang]** - Tin qu·ªëc t·∫ø\n**!chitiet [s·ªë]** - Chi ti·∫øt",
        inline=True
    )
    
    embed.add_field(
        name="ü§ñ L·ªánh AI",
        value="**!hoi [c√¢u h·ªèi]** - AI v·ªõi Google Search\n*VD: !hoi gi√° v√†ng h√¥m nay*",
        inline=True
    )
    
    embed.add_field(
        name="üîß Google Search Fixed",
        value="‚úÖ **4-Strategy Fallback System**\n‚úÖ **Specific Vietnamese Sites**\n‚úÖ **Enhanced Context**\n‚úÖ **Always Returns Results**",
        inline=False
    )
    
    google_status = "‚úÖ Configured v·ªõi 4-strategy fallback" if GOOGLE_API_KEY and GOOGLE_CSE_ID else "‚ö†Ô∏è Fallback mode - v·∫´n ho·∫°t ƒë·ªông"
    embed.add_field(name="üîç Google Search", value=google_status, inline=True)
    
    embed.set_footer(text="üîß Google Search Fixed ‚Ä¢ 4-Strategy System ‚Ä¢ Always Works")
    await ctx.send(embed=embed)

# Cleanup function
async def cleanup():
    if ai_manager:
        await ai_manager.close_session()

# Main execution
if __name__ == "__main__":
    try:
        keep_alive()
        print("üöÄ Starting GOOGLE SEARCH FIXED Multi-AI Discord News Bot...")
        
        total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
        print(f"üìä {total_sources} RSS sources loaded")
        
        if GOOGLE_API_KEY and GOOGLE_CSE_ID:
            print("üîç Google Search API: Configured with 4-strategy fallback system")
        else:
            print("‚ö†Ô∏è Google Search API: Not configured - using intelligent fallback")
        
        print("‚úÖ Bot ready with GOOGLE SEARCH FIXED!")
        
        bot.run(TOKEN)
        
    except Exception as e:
        print(f"‚ùå Bot startup error: {e}")
    finally:
        try:
            asyncio.run(cleanup())
        except:
            pass
