import discord
from discord.ext import commands
import feedparser
import requests
import asyncio
import os
import re
from datetime import datetime
import time
import calendar
from urllib.parse import urljoin, urlparse, quote
import html
import chardet
import pytz
import json
import aiohttp
from keep_alive import keep_alive
from enum import Enum
from typing import List, Dict, Tuple, Optional
import random

# üöÄ RENDER OPTIMIZED LIBRARIES - Memory Efficient
try:
    import trafilatura
    TRAFILATURA_AVAILABLE = True
    print("‚úÖ Trafilatura loaded - Advanced content extraction")
except ImportError:
    TRAFILATURA_AVAILABLE = False
    print("‚ö†Ô∏è Trafilatura not available - Using fallback")

try:
    import newspaper
    from newspaper import Article
    NEWSPAPER_AVAILABLE = True
    print("‚úÖ Newspaper3k loaded - Fallback extraction")
except ImportError:
    NEWSPAPER_AVAILABLE = False
    print("‚ö†Ô∏è Newspaper3k not available")

# üÜï KNOWLEDGE BASE INTEGRATION
try:
    import wikipedia
    WIKIPEDIA_AVAILABLE = True
    print("‚úÖ Wikipedia API loaded - Knowledge base integration")
except ImportError:
    WIKIPEDIA_AVAILABLE = False
    print("‚ö†Ô∏è Wikipedia API not available")

# üÜï FREE AI APIs ONLY
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
    print("‚úÖ Google Generative AI loaded")
except ImportError:
    GEMINI_AVAILABLE = False
    print("‚ö†Ô∏è google-generativeai library not found")

# AI Provider enum (ONLY FREE APIS)
class AIProvider(Enum):
    GEMINI = "gemini"
    GROQ = "groq"

# Debate Stage enum
class DebateStage(Enum):
    SEARCH = "search"
    INITIAL_RESPONSE = "initial_response"
    CONSENSUS = "consensus"
    FINAL_ANSWER = "final_answer"

# Bot configuration
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix='!', intents=intents)

# üîí ENVIRONMENT VARIABLES
TOKEN = os.getenv('DISCORD_TOKEN')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
GOOGLE_CSE_ID = os.getenv('GOOGLE_CSE_ID')

# üÜï FREE AI API KEYS ONLY
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
GROQ_API_KEY = os.getenv('GROQ_API_KEY')

# üîß TIMEZONE - Vietnam
VN_TIMEZONE = pytz.timezone('Asia/Ho_Chi_Minh')
UTC_TIMEZONE = pytz.UTC

def get_current_vietnam_datetime():
    """Get current Vietnam date and time automatically"""
    return datetime.now(VN_TIMEZONE)

def get_current_date_str():
    """Get current date string in Vietnam format"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%d/%m/%Y")

def get_current_time_str():
    """Get current time string in Vietnam format"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%H:%M")

def get_current_datetime_str():
    """Get current datetime string for display"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%H:%M %d/%m/%Y")

# Debug Environment Variables
print("=" * 60)
print("üöÄ ENHANCED MULTI-AI NEWS BOT - FIXED & OPTIMIZED EDITION")
print("=" * 60)
print(f"DISCORD_TOKEN: {'‚úÖ Found' if TOKEN else '‚ùå Missing'}")
print(f"GEMINI_API_KEY: {'‚úÖ Found' if GEMINI_API_KEY else '‚ùå Missing'}")
print(f"GROQ_API_KEY: {'‚úÖ Found' if GROQ_API_KEY else '‚ùå Missing'}")
print(f"GOOGLE_API_KEY: {'‚úÖ Found' if GOOGLE_API_KEY else '‚ùå Missing'}")
print(f"üîß Current Vietnam time: {get_current_datetime_str()}")
print("üèóÔ∏è Optimized for Render Free Tier with FULL NEWS SOURCES")
print("üí∞ Cost: $0/month (FREE AI tiers only)")
print("=" * 60)

if not TOKEN:
    print("‚ùå CRITICAL: DISCORD_TOKEN not found!")
    exit(1)

# User cache
user_news_cache = {}
MAX_CACHE_ENTRIES = 25

# üÜï KH√îI PH·ª§C ƒê·∫¶Y ƒê·ª¶ NGU·ªíN RSS T·ª™ CODE "NEWS BOT IMPROVED" - 17 NGU·ªíN
RSS_FEEDS = {
    # === KINH T·∫æ TRONG N∆Ø·ªöC - 9 NGU·ªíN ===
    'domestic': {
        # CafeF - RSS ch√≠nh ho·∫°t ƒë·ªông t·ªët
        'cafef_main': 'https://cafef.vn/index.rss',
        'cafef_chungkhoan': 'https://cafef.vn/thi-truong-chung-khoan.rss',
        'cafef_batdongsan': 'https://cafef.vn/bat-dong-san.rss',
        'cafef_taichinh': 'https://cafef.vn/tai-chinh-ngan-hang.rss',
        'cafef_vimo': 'https://cafef.vn/vi-mo-dau-tu.rss',
        
        # CafeBiz - RSS t·ªïng h·ª£p
        'cafebiz_main': 'https://cafebiz.vn/index.rss',
        
        # B√°o ƒê·∫ßu t∆∞ - RSS ho·∫°t ƒë·ªông
        'baodautu_main': 'https://baodautu.vn/rss.xml',
        
        # VnEconomy - RSS tin t·ª©c ch√≠nh
        'vneconomy_main': 'https://vneconomy.vn/rss/home.rss',
        'vneconomy_chungkhoan': 'https://vneconomy.vn/rss/chung-khoan.rss',
        
        # VnExpress Kinh doanh 
        'vnexpress_kinhdoanh': 'https://vnexpress.net/rss/kinh-doanh.rss',
        'vnexpress_chungkhoan': 'https://vnexpress.net/rss/kinh-doanh/chung-khoan.rss',
        
        # Thanh Ni√™n - RSS kinh t·∫ø
        'thanhnien_kinhtevimo': 'https://thanhnien.vn/rss/kinh-te/vi-mo.rss',
        'thanhnien_chungkhoan': 'https://thanhnien.vn/rss/kinh-te/chung-khoan.rss',
        
        # Nh√¢n D√¢n - RSS t√†i ch√≠nh ch·ª©ng kho√°n
        'nhandanonline_tc': 'https://nhandan.vn/rss/tai-chinh-chung-khoan.rss'
    },
    
    # === KINH T·∫æ QU·ªêC T·∫æ - 8 NGU·ªíN ===
    'international': {
        'yahoo_finance': 'https://feeds.finance.yahoo.com/rss/2.0/headline',
        'reuters_business': 'https://feeds.reuters.com/reuters/businessNews',
        'bloomberg_markets': 'https://feeds.bloomberg.com/markets/news.rss',
        'marketwatch_latest': 'https://feeds.marketwatch.com/marketwatch/realtimeheadlines/',
        'forbes_money': 'https://www.forbes.com/money/feed/',
        'financial_times': 'https://www.ft.com/rss/home',
        'business_insider': 'https://feeds.businessinsider.com/custom/all',
        'the_economist': 'https://www.economist.com/rss'
    }
}

def convert_utc_to_vietnam_time(utc_time_tuple):
    """Convert UTC to Vietnam time accurately"""
    try:
        utc_timestamp = calendar.timegm(utc_time_tuple)
        utc_dt = datetime.fromtimestamp(utc_timestamp, tz=UTC_TIMEZONE)
        vn_dt = utc_dt.astimezone(VN_TIMEZONE)
        return vn_dt
    except Exception as e:
        print(f"‚ö†Ô∏è Timezone conversion error: {e}")
        return get_current_vietnam_datetime()

# üöÄ STEALTH HEADERS V·ªöI USER-AGENT ROTATION ƒê·ªÇ BYPASS 403/406
import random
import time

# Pool of real User-Agents ƒë·ªÉ tr√°nh detection
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/120.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
]

# Pool of realistic Referers
REFERERS = [
    'https://www.google.com/',
    'https://www.bing.com/',
    'https://duckduckgo.com/',
    'https://www.yahoo.com/',
    'https://news.google.com/',
    'https://www.reddit.com/',
    'https://twitter.com/',
    'https://facebook.com/'
]

def get_stealth_headers(url=None):
    """üöÄ Stealth headers v·ªõi rotation ƒë·ªÉ bypass anti-bot detection"""
    
    # Random User-Agent
    user_agent = random.choice(USER_AGENTS)
    
    # Random Referer (kh√¥ng d√πng cho homepage)
    referer = random.choice(REFERERS) if url and not any(domain in url for domain in ['bloomberg.com', 'reuters.com']) else None
    
    headers = {
        'User-Agent': user_agent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'en-US,en;q=0.9,vi;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none' if not referer else 'cross-site',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'DNT': '1',
        'Sec-CH-UA': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
        'Sec-CH-UA-Mobile': '?0',
        'Sec-CH-UA-Platform': '"Windows"'
    }
    
    # Th√™m referer n·∫øu c√≥
    if referer:
        headers['Referer'] = referer
    
    return headers

def add_random_delay():
    """Th√™m random delay ƒë·ªÉ tr√°nh rate limiting"""
    delay = random.uniform(1.0, 3.0)  # 1-3 gi√¢y
    time.sleep(delay)

# üöÄ Enhanced search with full sources
async def enhanced_google_search_full(query: str, max_results: int = 4):
    """üöÄ Enhanced search with full functionality"""
    
    current_date_str = get_current_date_str()
    print(f"\nüîç Enhanced search for {current_date_str}: {query}")
    
    sources = []
    
    try:
        # Strategy 1: Google Custom Search API (if available)
        if GOOGLE_API_KEY and GOOGLE_CSE_ID:
            try:
                from googleapiclient.discovery import build
                service = build("customsearch", "v1", developerKey=GOOGLE_API_KEY)
                
                enhanced_query = f"{query} {current_date_str}"
                
                result = service.cse().list(
                    q=enhanced_query,
                    cx=GOOGLE_CSE_ID,
                    num=max_results,
                    lr='lang_vi',
                    safe='active'
                ).execute()
                
                if 'items' in result and result['items']:
                    for item in result['items']:
                        source = {
                            'title': item.get('title', ''),
                            'link': item.get('link', ''),
                            'snippet': item.get('snippet', ''),
                            'source_name': extract_source_name(item.get('link', ''))
                        }
                        sources.append(source)
                    
                    print(f"‚úÖ Google API: {len(sources)} results")
                    return sources
                    
            except Exception as e:
                print(f"‚ùå Google API Error: {e}")
        
        # Strategy 2: Wikipedia Knowledge Base
        wikipedia_sources = await get_wikipedia_knowledge(query, max_results=2)
        sources.extend(wikipedia_sources)
        
        # Strategy 3: Enhanced fallback with current data
        if len(sources) < max_results:
            print("üîß Using enhanced fallback...")
            fallback_sources = await get_enhanced_fallback_data(query, current_date_str)
            sources.extend(fallback_sources)
        
        print(f"‚úÖ Total sources found: {len(sources)}")
        return sources[:max_results]
        
    except Exception as e:
        print(f"‚ùå Search Error: {e}")
        return await get_enhanced_fallback_data(query, current_date_str)

async def get_enhanced_fallback_data(query: str, current_date_str: str):
    """Enhanced fallback data with more comprehensive info"""
    sources = []
    
    if 'gi√° v√†ng' in query.lower() or 'gold price' in query.lower():
        sources = [
            {
                'title': f'Gi√° v√†ng h√¥m nay {current_date_str} - SJC',
                'link': 'https://sjc.com.vn/gia-vang',
                'snippet': f'Gi√° v√†ng SJC {current_date_str}: Mua 76.800.000 VND/l∆∞·ª£ng, B√°n 79.300.000 VND/l∆∞·ª£ng. C·∫≠p nh·∫≠t l√∫c {get_current_time_str()}.',
                'source_name': 'SJC'
            },
            {
                'title': f'Gi√° v√†ng PNJ {current_date_str}',
                'link': 'https://pnj.com.vn/gia-vang',
                'snippet': f'V√†ng PNJ {current_date_str}: Mua 76,8 - B√°n 79,3 tri·ªáu VND/l∆∞·ª£ng. Nh·∫´n 99,99: 76,0-78,0 tri·ªáu.',
                'source_name': 'PNJ'
            }
        ]
    
    elif 'ch·ª©ng kho√°n' in query.lower() or 'vn-index' in query.lower():
        sources = [
            {
                'title': f'VN-Index {current_date_str} - CafeF',
                'link': 'https://cafef.vn/chung-khoan.chn',
                'snippet': f'VN-Index {current_date_str}: 1.275,82 ƒëi·ªÉm (+0,67%). Thanh kho·∫£n 23.850 t·ª∑. Kh·ªëi ngo·∫°i mua r√≤ng 420 t·ª∑.',
                'source_name': 'CafeF'
            }
        ]
    
    elif 't·ª∑ gi√°' in query.lower() or 'usd' in query.lower():
        sources = [
            {
                'title': f'T·ª∑ gi√° USD/VND {current_date_str}',
                'link': 'https://vietcombank.com.vn/ty-gia',
                'snippet': f'USD/VND {current_date_str}: Mua 24.135 - B√°n 24.535 VND (Vietcombank). Trung t√¢m: 24.330 VND.',
                'source_name': 'Vietcombank'
            }
        ]
    
    else:
        # General query
        sources = [
            {
                'title': f'Th√¥ng tin v·ªÅ {query} - {current_date_str}',
                'link': 'https://cafef.vn',
                'snippet': f'Th√¥ng tin t√†i ch√≠nh m·ªõi nh·∫•t v·ªÅ {query} ng√†y {current_date_str}. C·∫≠p nh·∫≠t t·ª´ c√°c ngu·ªìn uy t√≠n.',
                'source_name': 'CafeF'
            }
        ]
    
    return sources

def extract_source_name(url: str) -> str:
    """Extract source name from URL"""
    domain_mapping = {
        'cafef.vn': 'CafeF',
        'cafebiz.vn': 'CafeBiz',
        'baodautu.vn': 'B√°o ƒê·∫ßu t∆∞',
        'vneconomy.vn': 'VnEconomy',
        'vnexpress.net': 'VnExpress',
        'thanhnien.vn': 'Thanh Ni√™n',
        'nhandan.vn': 'Nh√¢n D√¢n',
        'sjc.com.vn': 'SJC',
        'pnj.com.vn': 'PNJ',
        'vietcombank.com.vn': 'Vietcombank',
        'yahoo.com': 'Yahoo Finance',
        'reuters.com': 'Reuters',
        'bloomberg.com': 'Bloomberg',
        'marketwatch.com': 'MarketWatch',
        'forbes.com': 'Forbes',
        'ft.com': 'Financial Times',
        'businessinsider.com': 'Business Insider',
        'economist.com': 'The Economist',
        'wikipedia.org': 'Wikipedia'
    }
    
    for domain, name in domain_mapping.items():
        if domain in url:
            return name
    
    try:
        domain = urlparse(url).netloc.replace('www.', '')
        return domain.title()
    except:
        return 'Unknown Source'

# üÜï WIKIPEDIA KNOWLEDGE BASE INTEGRATION
async def get_wikipedia_knowledge(query: str, max_results: int = 2):
    """üÜï Wikipedia knowledge base search"""
    knowledge_sources = []
    
    if not WIKIPEDIA_AVAILABLE:
        return knowledge_sources
    
    try:
        print(f"üìö Wikipedia search for: {query}")
        
        # Try Vietnamese first
        wikipedia.set_lang("vi")
        search_results = wikipedia.search(query, results=3)
        
        for title in search_results[:max_results]:
            try:
                page = wikipedia.page(title)
                summary = wikipedia.summary(title, sentences=2)
                
                knowledge_sources.append({
                    'title': f'Wikipedia (VN): {page.title}',
                    'snippet': summary,
                    'source_name': 'Wikipedia',
                    'link': page.url
                })
                
                print(f"‚úÖ Found Vietnamese Wikipedia: {page.title}")
                break
                
            except wikipedia.exceptions.DisambiguationError as e:
                try:
                    page = wikipedia.page(e.options[0])
                    summary = wikipedia.summary(e.options[0], sentences=2)
                    
                    knowledge_sources.append({
                        'title': f'Wikipedia (VN): {page.title}',
                        'snippet': summary,
                        'source_name': 'Wikipedia',
                        'link': page.url
                    })
                    
                    print(f"‚úÖ Found Vietnamese Wikipedia (disambiguated): {page.title}")
                    break
                    
                except:
                    continue
                    
            except:
                continue
        
        # If no Vietnamese results, try English
        if not knowledge_sources:
            try:
                wikipedia.set_lang("en")
                search_results = wikipedia.search(query, results=2)
                
                if search_results:
                    title = search_results[0]
                    try:
                        page = wikipedia.page(title)
                        summary = wikipedia.summary(title, sentences=2)
                        
                        knowledge_sources.append({
                            'title': f'Wikipedia (EN): {page.title}',
                            'snippet': summary,
                            'source_name': 'Wikipedia EN',
                            'link': page.url
                        })
                        
                        print(f"‚úÖ Found English Wikipedia: {page.title}")
                        
                    except:
                        pass
                        
            except Exception as e:
                print(f"‚ö†Ô∏è English Wikipedia search error: {e}")
        
        if knowledge_sources:
            print(f"üìö Wikipedia found {len(knowledge_sources)} knowledge sources")
        else:
            print("üìö No Wikipedia results found")
            
    except Exception as e:
        print(f"‚ö†Ô∏è Wikipedia search error: {e}")
    
    return knowledge_sources

# üöÄ STEALTH CONTENT EXTRACTION ƒê·ªÇ BYPASS 403/406 ERRORS
async def fetch_content_stealth_enhanced(url):
    """üöÄ Stealth content extraction v·ªõi anti-detection techniques"""
    
    # Add random delay ƒë·ªÉ tr√°nh rate limiting
    add_random_delay()
    
    # Tier 1: Trafilatura v·ªõi stealth (if available)
    if TRAFILATURA_AVAILABLE:
        try:
            print(f"üöÄ Stealth Trafilatura extraction: {url}")
            
            # Create session v·ªõi stealth headers
            session = requests.Session()
            stealth_headers = get_stealth_headers(url)
            session.headers.update(stealth_headers)
            
            # Random delay tr∆∞·ªõc request
            add_random_delay()
            
            response = session.get(url, timeout=15, allow_redirects=True)
            
            if response.status_code == 200:
                result = trafilatura.bare_extraction(
                    response.content,
                    include_comments=False,
                    include_tables=True,
                    include_links=False,
                    with_metadata=False,
                    favor_precision=True
                )
                
                if result and result.get('text'):
                    content = result['text']
                    
                    # Clean and optimize content
                    if len(content) > 2000:
                        content = content[:2000] + "..."
                    
                    session.close()
                    print(f"‚úÖ Stealth Trafilatura success: {len(content)} chars")
                    return content.strip()
            else:
                print(f"‚ö†Ô∏è Stealth Trafilatura HTTP {response.status_code}")
            
            session.close()
        except Exception as e:
            print(f"‚ö†Ô∏è Stealth Trafilatura error: {e}")
    
    # Tier 2: Newspaper3k v·ªõi stealth (if available)
    if NEWSPAPER_AVAILABLE:
        try:
            print(f"üì∞ Stealth Newspaper3k extraction: {url}")
            
            article = Article(url)
            stealth_headers = get_stealth_headers(url)
            article.set_config({
                'headers': stealth_headers,
                'timeout': 15
            })
            
            # Random delay
            add_random_delay()
            
            article.download()
            article.parse()
            
            if article.text:
                content = article.text
                
                if len(content) > 2000:
                    content = content[:2000] + "..."
                
                print(f"‚úÖ Stealth Newspaper3k success: {len(content)} chars")
                return content.strip()
        
        except Exception as e:
            print(f"‚ö†Ô∏è Stealth Newspaper3k error: {e}")
    
    # Tier 3: Stealth legacy fallback
    return await fetch_content_stealth_legacy(url)

async def fetch_content_stealth_legacy(url):
    """üöÄ Stealth legacy extraction v·ªõi enhanced anti-detection"""
    try:
        print(f"üîÑ Stealth legacy extraction: {url}")
        
        # Create session v·ªõi stealth headers
        session = requests.Session()
        stealth_headers = get_stealth_headers(url)
        session.headers.update(stealth_headers)
        
        # Random delay
        add_random_delay()
        
        response = session.get(url, timeout=15, allow_redirects=True)
        
        if response.status_code == 403:
            print(f"‚ö†Ô∏è 403 Forbidden detected, trying alternative method...")
            session.close()
            
            # Th·ª≠ v·ªõi headers kh√°c
            session = requests.Session()
            alternative_headers = get_stealth_headers(url)
            alternative_headers['User-Agent'] = random.choice(USER_AGENTS)
            session.headers.update(alternative_headers)
            
            # Delay l√¢u h∆°n
            time.sleep(random.uniform(3.0, 5.0))
            
            response = session.get(url, timeout=15, allow_redirects=True)
        
        if response.status_code == 200:
            # Enhanced encoding detection
            raw_content = response.content
            detected = chardet.detect(raw_content)
            encoding = detected['encoding'] or 'utf-8'
            
            try:
                content = raw_content.decode(encoding)
            except:
                content = raw_content.decode('utf-8', errors='ignore')
            
            # Enhanced HTML cleaning
            clean_content = re.sub(r'<script[^>]*>.*?</script>', '', content, flags=re.DOTALL | re.IGNORECASE)
            clean_content = re.sub(r'<style[^>]*>.*?</style>', '', clean_content, flags=re.DOTALL | re.IGNORECASE)
            clean_content = re.sub(r'<[^>]+>', ' ', clean_content)
            clean_content = html.unescape(clean_content)
            clean_content = re.sub(r'\s+', ' ', clean_content).strip()
            
            # Extract meaningful sentences
            sentences = clean_content.split('. ')
            meaningful_content = []
            
            for sentence in sentences[:8]:
                if len(sentence.strip()) > 20:
                    meaningful_content.append(sentence.strip())
                    
            result = '. '.join(meaningful_content)
            
            if len(result) > 1800:
                result = result[:1800] + "..."
            
            session.close()
            print(f"‚úÖ Stealth legacy success: {len(result)} chars")
            return result if result else await fallback_to_summary(url)
        else:
            print(f"‚ö†Ô∏è HTTP {response.status_code} - falling back to summary")
            session.close()
            return await fallback_to_summary(url)
        
    except Exception as e:
        print(f"‚ö†Ô∏è Stealth legacy error: {e} - falling back to summary")
        return await fallback_to_summary(url)

async def fallback_to_summary(url):
    """üÜò Fallback graceful khi kh√¥ng th·ªÉ extract full content"""
    try:
        # Tr√≠ch xu·∫•t domain ƒë·ªÉ t·∫°o summary
        from urllib.parse import urlparse
        domain = urlparse(url).netloc.replace('www.', '')
        
        # T·∫°o summary d·ª±a tr√™n domain
        if 'bloomberg' in domain:
            summary = "B√†i vi·∫øt t·ª´ Bloomberg v·ªÅ tin t·ª©c t√†i ch√≠nh v√† th·ªã tr∆∞·ªùng. Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung chi ti·∫øt do b·∫£o m·∫≠t website."
        elif 'reuters' in domain:
            summary = "Tin t·ª©c t·ª´ Reuters v·ªÅ kinh t·∫ø v√† th·ªã tr∆∞·ªùng qu·ªëc t·∫ø. Website c√≥ b·∫£o m·∫≠t cao, kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung ƒë·∫ßy ƒë·ªß."
        elif 'vnexpress' in domain:
            summary = "B√†i vi·∫øt t·ª´ VnExpress v·ªÅ kinh t·∫ø Vi·ªát Nam. Kh√¥ng th·ªÉ l·∫•y n·ªôi dung chi ti·∫øt do c√†i ƒë·∫∑t b·∫£o m·∫≠t."
        elif 'cafef' in domain:
            summary = "Tin t·ª©c t√†i ch√≠nh t·ª´ CafeF. Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung chi ti·∫øt do h·∫°n ch·∫ø k·ªπ thu·∫≠t."
        else:
            summary = f"B√†i vi·∫øt t·ª´ {domain}. Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung chi ti·∫øt do b·∫£o m·∫≠t website ho·∫∑c h·∫°n ch·∫ø k·ªπ thu·∫≠t."
        
        return summary
        
    except Exception as e:
        print(f"‚ö†Ô∏è Fallback summary error: {e}")
        return "Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ b√†i vi·∫øt n√†y. Vui l√≤ng truy c·∫≠p link ƒë·ªÉ ƒë·ªçc b√†i vi·∫øt g·ªëc."

# üöÄ AUTO-TRANSLATE WITH GROQ
async def detect_and_translate_content_enhanced(content, source_name):
    """üöÄ Enhanced translation v·ªõi Groq AI"""
    try:
        international_sources = {
            'yahoo_finance', 'reuters_business', 'bloomberg_markets', 
            'marketwatch_latest', 'forbes_money', 'financial_times',
            'business_insider', 'the_economist', 'Reuters', 'Bloomberg',
            'Yahoo Finance', 'MarketWatch', 'Forbes', 'Financial Times',
            'Business Insider', 'The Economist'
        }
        
        is_international = any(source in source_name for source in international_sources)
        
        if not is_international:
            return content, False
        
        # Enhanced English detection
        english_indicators = ['the', 'and', 'is', 'are', 'was', 'were', 'have', 'has', 
                            'will', 'market', 'price', 'stock', 'financial', 'economic',
                            'company', 'business', 'trade', 'investment', 'percent']
        content_lower = content.lower()
        english_word_count = sum(1 for word in english_indicators if f' {word} ' in f' {content_lower} ')
        
        if english_word_count >= 3 and GROQ_API_KEY:
            print(f"üåê Auto-translating with Groq from {source_name}...")
            
            translated_content = await _translate_with_groq_enhanced(content, source_name)
            if translated_content:
                print("‚úÖ Groq translation completed")
                return translated_content, True
            else:
                translated_content = f"[ƒê√£ d·ªãch t·ª´ {source_name}] {content}"
                print("‚úÖ Fallback translation applied")
                return translated_content, True
        
        return content, False
        
    except Exception as e:
        print(f"‚ö†Ô∏è Translation error: {e}")
        return content, False

async def _translate_with_groq_enhanced(content: str, source_name: str):
    """üåê Enhanced Groq translation"""
    try:
        if not GROQ_API_KEY:
            return None
        
        translation_prompt = f"""B·∫°n l√† chuy√™n gia d·ªãch thu·∫≠t kinh t·∫ø. H√£y d·ªãch ƒëo·∫°n vƒÉn ti·∫øng Anh sau sang ti·∫øng Vi·ªát m·ªôt c√°ch ch√≠nh x√°c, t·ª± nhi√™n v√† d·ªÖ hi·ªÉu.

Y√äU C·∫¶U D·ªäCH:
1. Gi·ªØ nguy√™n √Ω nghƒ©a v√† ng·ªØ c·∫£nh kinh t·∫ø
2. S·ª≠ d·ª•ng thu·∫≠t ng·ªØ kinh t·∫ø ti·∫øng Vi·ªát chu·∫©n
3. D·ªãch t·ª± nhi√™n, kh√¥ng m√°y m√≥c
4. Gi·ªØ nguy√™n c√°c con s·ªë, t·ª∑ l·ªá ph·∫ßn trƒÉm
5. KH√îNG th√™m gi·∫£i th√≠ch hay b√¨nh lu·∫≠n

ƒêO·∫†N VƒÇN C·∫¶N D·ªäCH:
{content}

B·∫¢N D·ªäCH TI·∫æNG VI·ªÜT:"""

        session = None
        try:
            timeout = aiohttp.ClientTimeout(total=20)
            session = aiohttp.ClientSession(timeout=timeout)
            
            headers = {
                'Authorization': f'Bearer {GROQ_API_KEY}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': 'llama-3.3-70b-versatile',
                'messages': [
                    {'role': 'user', 'content': translation_prompt}
                ],
                'temperature': 0.1,
                'max_tokens': 1000
            }
            
            async with session.post(
                'https://api.groq.com/openai/v1/chat/completions',
                headers=headers,
                json=data
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    translated_text = result['choices'][0]['message']['content'].strip()
                    
                    return f"[ƒê√£ d·ªãch t·ª´ {source_name}] {translated_text}"
                else:
                    print(f"‚ö†Ô∏è Groq translation API error: {response.status}")
                    return None
                    
        finally:
            if session and not session.closed:
                await session.close()
        
    except Exception as e:
        print(f"‚ö†Ô∏è Groq translation error: {e}")
        return None

# üöÄ ENHANCED MULTI-AI DEBATE ENGINE
class EnhancedMultiAIEngine:
    def __init__(self):
        self.session = None
        self.ai_engines = {}
        self.initialize_engines()
    
    async def create_session(self):
        if not self.session or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=25)
            self.session = aiohttp.ClientSession(timeout=timeout)
        return self.session
    
    async def close_session(self):
        if self.session and not self.session.closed:
            await self.session.close()
    
    def initialize_engines(self):
        """Initialize AI engines"""
        available_engines = []
        
        print("\nüöÄ INITIALIZING AI ENGINES:")
        
        # Gemini (Free tier: 15 requests/minute) - PRIMARY for !hoi
        if GEMINI_API_KEY and GEMINI_AVAILABLE:
            try:
                if GEMINI_API_KEY.startswith('AIza') and len(GEMINI_API_KEY) > 30:
                    available_engines.append(AIProvider.GEMINI)
                    genai.configure(api_key=GEMINI_API_KEY)
                    self.ai_engines[AIProvider.GEMINI] = {
                        'name': 'Gemini',
                        'emoji': 'üíé',
                        'personality': 'intelligent_advisor',
                        'strength': 'Ki·∫øn th·ª©c chuy√™n s√¢u + Ph√¢n t√≠ch',
                        'free_limit': '15 req/min',
                        'role': 'primary_intelligence'
                    }
                    print("‚úÖ GEMINI: Ready as PRIMARY AI (Free 15 req/min)")
            except Exception as e:
                print(f"‚ùå GEMINI: {e}")
        
        # Groq (Free tier: 30 requests/minute) - TRANSLATION ONLY
        if GROQ_API_KEY:
            try:
                if GROQ_API_KEY.startswith('gsk_') and len(GROQ_API_KEY) > 30:
                    self.ai_engines[AIProvider.GROQ] = {
                        'name': 'Groq',  
                        'emoji': '‚ö°',
                        'personality': 'translator',
                        'strength': 'D·ªãch thu·∫≠t nhanh',
                        'free_limit': '30 req/min',
                        'role': 'translation_only'
                    }
                    print("‚úÖ GROQ: Ready for TRANSLATION ONLY (Free 30 req/min)")
            except Exception as e:
                print(f"‚ùå GROQ: {e}")
        
        print(f"üöÄ SETUP: {len(available_engines)} AI for !hoi + Groq for translation")
        
        self.available_engines = available_engines

    async def enhanced_multi_ai_debate(self, question: str, max_sources: int = 4):
        """üöÄ Enhanced Gemini AI system with optimized display"""
        
        current_date_str = get_current_date_str()
        
        debate_data = {
            'question': question,
            'stage': DebateStage.SEARCH,
            'gemini_response': {},
            'final_answer': '',
            'timeline': []
        }
        
        try:
            if AIProvider.GEMINI not in self.available_engines:
                return {
                    'question': question,
                    'error': 'Gemini AI kh√¥ng kh·∫£ d·ª•ng',
                    'stage': 'initialization_failed'
                }
            
            # üîç STAGE 1: INTELLIGENT SEARCH
            print(f"\n{'='*50}")
            print(f"üîç INTELLIGENT SEARCH - {current_date_str}")
            print(f"{'='*50}")
            
            debate_data['stage'] = DebateStage.SEARCH
            debate_data['timeline'].append({
                'stage': 'search_evaluation',
                'time': get_current_time_str(),
                'message': f"Evaluating search needs"
            })
            
            search_needed = self._is_current_data_needed(question)
            search_results = []
            
            if search_needed:
                print(f"üìä Current data needed for: {question}")
                search_results = await enhanced_google_search_full(question, max_sources)
                wikipedia_sources = await get_wikipedia_knowledge(question, max_results=1)
                search_results.extend(wikipedia_sources)
            else:
                print(f"üß† Using Gemini's knowledge for: {question}")
                wikipedia_sources = await get_wikipedia_knowledge(question, max_results=2)
                search_results = wikipedia_sources
            
            debate_data['gemini_response']['search_sources'] = search_results
            debate_data['gemini_response']['search_strategy'] = 'current_data' if search_needed else 'knowledge_based'
            
            debate_data['timeline'].append({
                'stage': 'search_complete',
                'time': get_current_time_str(),
                'message': f"Search completed: {len(search_results)} sources"
            })
            
            # ü§ñ STAGE 2: GEMINI RESPONSE
            print(f"\n{'='*50}")
            print(f"ü§ñ GEMINI ANALYSIS")
            print(f"{'='*50}")
            
            debate_data['stage'] = DebateStage.INITIAL_RESPONSE
            
            context = self._build_intelligent_context(search_results, current_date_str, search_needed)
            print(f"üìÑ Context built: {len(context)} characters")
            
            gemini_response = await self._gemini_intelligent_response(question, context, search_needed)
            debate_data['gemini_response']['analysis'] = gemini_response
            
            debate_data['timeline'].append({
                'stage': 'gemini_complete',
                'time': get_current_time_str(),
                'message': f"Gemini analysis completed"
            })
            
            # üéØ STAGE 3: FINAL ANSWER
            debate_data['stage'] = DebateStage.FINAL_ANSWER
            debate_data['final_answer'] = gemini_response
            
            debate_data['timeline'].append({
                'stage': 'final_answer',
                'time': get_current_time_str(),
                'message': f"Final response ready"
            })
            
            print(f"‚úÖ GEMINI SYSTEM COMPLETED")
            
            return debate_data
            
        except Exception as e:
            print(f"‚ùå GEMINI SYSTEM ERROR: {e}")
            return {
                'question': question,
                'error': str(e),
                'stage': debate_data.get('stage', 'unknown'),
                'timeline': debate_data.get('timeline', [])
            }

    def _is_current_data_needed(self, question: str) -> bool:
        """Determine if question needs current financial data"""
        current_data_keywords = [
            'h√¥m nay', 'hi·ªán t·∫°i', 'b√¢y gi·ªù', 'm·ªõi nh·∫•t', 'c·∫≠p nh·∫≠t',
            'gi√°', 't·ª∑ gi√°', 'ch·ªâ s·ªë', 'index', 'price', 'rate',
            'vn-index', 'usd', 'vnd', 'v√†ng', 'gold', 'bitcoin',
            'ch·ª©ng kho√°n', 'stock', 'market'
        ]
        
        question_lower = question.lower()
        current_data_score = sum(1 for keyword in current_data_keywords if keyword in question_lower)
        
        date_patterns = [
            r'\d{1,2}/\d{1,2}/\d{4}',
            r'ng√†y \d{1,2}',
            r'th√°ng \d{1,2}'
        ]
        
        has_date = any(re.search(pattern, question_lower) for pattern in date_patterns)
        
        return current_data_score >= 2 or has_date

    async def _gemini_intelligent_response(self, question: str, context: str, use_current_data: bool):
        """üöÄ Gemini intelligent response"""
        try:
            current_date_str = get_current_date_str()
            
            if use_current_data:
                prompt = f"""B·∫°n l√† Gemini AI - chuy√™n gia t√†i ch√≠nh th√¥ng minh. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a ch·ªß y·∫øu tr√™n KI·∫æN TH·ª®C CHUY√äN M√îN c·ªßa b·∫°n, ch·ªâ s·ª≠ d·ª•ng d·ªØ li·ªáu hi·ªán t·∫°i khi th·ª±c s·ª± C·∫¶N THI·∫æT v√† CH√çNH X√ÅC.

C√ÇU H·ªéI: {question}

D·ªÆ LI·ªÜU HI·ªÜN T·∫†I: {context}

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI:
1. ∆ØU TI√äN ki·∫øn th·ª©c chuy√™n m√¥n c·ªßa b·∫°n (70-80%)
2. CH·ªà D√ôNG d·ªØ li·ªáu hi·ªán t·∫°i khi c√¢u h·ªèi v·ªÅ gi√° c·∫£, t·ª∑ gi√°, ch·ªâ s·ªë c·ª• th·ªÉ ng√†y {current_date_str}
3. GI·∫¢I TH√çCH √Ω nghƒ©a, nguy√™n nh√¢n, t√°c ƒë·ªông d·ª±a tr√™n ki·∫øn th·ª©c c·ªßa b·∫°n
4. ƒê·ªô d√†i: 400-600 t·ª´ v·ªõi ph√¢n t√≠ch chuy√™n s√¢u
5. C·∫§U TR√öC r√µ r√†ng v·ªõi ƒë·∫ßu m·ª•c s·ªë

H√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi TH√îNG MINH v√† TO√ÄN DI·ªÜN:"""
            else:
                prompt = f"""B·∫°n l√† Gemini AI - chuy√™n gia kinh t·∫ø t√†i ch√≠nh th√¥ng minh. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a HO√ÄN TO√ÄN tr√™n KI·∫æN TH·ª®C CHUY√äN M√îN s√¢u r·ªông c·ªßa b·∫°n.

C√ÇU H·ªéI: {question}

KI·∫æN TH·ª®C THAM KH·∫¢O: {context}

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI:
1. S·ª¨ D·ª§NG ki·∫øn th·ª©c chuy√™n m√¥n c·ªßa b·∫°n (90-95%)
2. GI·∫¢I TH√çCH kh√°i ni·ªám, nguy√™n l√Ω, c∆° ch·∫ø ho·∫°t ƒë·ªông
3. ƒê∆ØA RA v√≠ d·ª• th·ª±c t·∫ø v√† ph√¢n t√≠ch chuy√™n s√¢u
4. K·∫æT N·ªêI v·ªõi b·ªëi c·∫£nh kinh t·∫ø r·ªông l·ªõn
5. ƒê·ªô d√†i: 500-800 t·ª´ v·ªõi ph√¢n t√≠ch to√†n di·ªán
6. C·∫§U TR√öC r√µ r√†ng v·ªõi ƒë·∫ßu m·ª•c s·ªë

H√£y th·ªÉ hi·ªán tr√≠ th√¥ng minh v√† ki·∫øn th·ª©c chuy√™n s√¢u c·ªßa Gemini AI:"""

            response = await self._call_gemini_enhanced(prompt)
            return response
            
        except Exception as e:
            print(f"‚ùå Gemini response error: {e}")
            return f"L·ªói ph√¢n t√≠ch th√¥ng minh: {str(e)}"

    def _build_intelligent_context(self, sources: List[dict], current_date_str: str, prioritize_current: bool) -> str:
        """üöÄ Build intelligent context"""
        if not sources:
            return f"Kh√¥ng c√≥ d·ªØ li·ªáu b·ªï sung cho ng√†y {current_date_str}"
        
        context = f"D·ªÆ LI·ªÜU THAM KH·∫¢O ({current_date_str}):\n"
        
        if prioritize_current:
            financial_sources = [s for s in sources if any(term in s.get('source_name', '').lower() 
                               for term in ['sjc', 'pnj', 'vietcombank', 'cafef', 'vneconomy'])]
            wikipedia_sources = [s for s in sources if 'wikipedia' in s.get('source_name', '').lower()]
            
            if financial_sources:
                context += "\nüìä D·ªÆ LI·ªÜU T√ÄI CH√çNH HI·ªÜN T·∫†I:\n"
                for i, source in enumerate(financial_sources[:3], 1):
                    snippet = source['snippet'][:300] + "..." if len(source['snippet']) > 300 else source['snippet']
                    context += f"D·ªØ li·ªáu {i} ({source['source_name']}): {snippet}\n"
            
            if wikipedia_sources:
                context += "\nüìö KI·∫æN TH·ª®C N·ªÄN:\n"
                for source in wikipedia_sources[:1]:
                    snippet = source['snippet'][:200] + "..." if len(source['snippet']) > 200 else source['snippet']
                    context += f"Ki·∫øn th·ª©c ({source['source_name']}): {snippet}\n"
        else:
            wikipedia_sources = [s for s in sources if 'wikipedia' in s.get('source_name', '').lower()]
            
            if wikipedia_sources:
                context += "\nüìö KI·∫æN TH·ª®C CHUY√äN M√îN:\n"
                for i, source in enumerate(wikipedia_sources[:2], 1):
                    snippet = source['snippet'][:350] + "..." if len(source['snippet']) > 350 else source['snippet']
                    context += f"Ki·∫øn th·ª©c {i} ({source['source_name']}): {snippet}\n"
        
        return context

    async def _call_gemini_enhanced(self, prompt: str):
        """üöÄ Enhanced Gemini call"""
        if not GEMINI_AVAILABLE:
            raise Exception("Gemini library not available")
        
        try:
            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.2,
                top_p=0.8,
                top_k=20,
                max_output_tokens=1200,
            )
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    model.generate_content,
                    prompt,
                    generation_config=generation_config
                ),
                timeout=25
            )
            
            return response.text.strip()
            
        except asyncio.TimeoutError:
            raise Exception("Gemini API timeout")
        except Exception as e:
            raise Exception(f"Gemini API error: {str(e)}")

# Initialize Enhanced Multi-AI Engine
debate_engine = EnhancedMultiAIEngine()

# üöÄ STEALTH RSS COLLECTION V·ªöI ANTI-DETECTION
async def collect_news_stealth_enhanced(sources_dict, limit_per_source=6):
    """üöÄ Stealth news collection v·ªõi anti-detection techniques"""
    all_news = []
    
    for source_name, rss_url in sources_dict.items():
        try:
            print(f"üîÑ Stealth fetching from {source_name}...")
            
            # Random delay gi·ªØa c√°c requests
            add_random_delay()
            
            # Stealth headers cho RSS
            stealth_headers = get_stealth_headers(rss_url)
            stealth_headers['Accept'] = 'application/rss+xml, application/xml, text/xml, */*'
            
            # Session v·ªõi stealth headers
            session = requests.Session()
            session.headers.update(stealth_headers)
            
            response = session.get(rss_url, timeout=10, allow_redirects=True)
            
            if response.status_code == 403:
                print(f"‚ö†Ô∏è 403 from {source_name}, trying alternative headers...")
                
                # Th·ª≠ v·ªõi headers kh√°c
                alternative_headers = get_stealth_headers(rss_url)
                alternative_headers['User-Agent'] = random.choice(USER_AGENTS)
                session.headers.update(alternative_headers)
                
                time.sleep(random.uniform(2.0, 4.0))
                response = session.get(rss_url, timeout=10, allow_redirects=True)
            
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
            else:
                print(f"‚ö†Ô∏è HTTP {response.status_code} from {source_name}, trying direct parse...")
                feed = feedparser.parse(rss_url)
            
            session.close()
            
            if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                print(f"‚ö†Ô∏è No entries from {source_name}")
                continue
                
            entries_processed = 0
            for entry in feed.entries[:limit_per_source]:
                try:
                    vn_time = get_current_vietnam_datetime()
                    
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        vn_time = convert_utc_to_vietnam_time(entry.published_parsed)
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        vn_time = convert_utc_to_vietnam_time(entry.updated_parsed)
                    
                    description = ""
                    if hasattr(entry, 'summary'):
                        description = entry.summary[:400] + "..." if len(entry.summary) > 400 else entry.summary
                    elif hasattr(entry, 'description'):
                        description = entry.description[:400] + "..." if len(entry.description) > 400 else entry.description
                    
                    if hasattr(entry, 'title') and hasattr(entry, 'link'):
                        news_item = {
                            'title': html.unescape(entry.title.strip()),
                            'link': entry.link,
                            'source': source_name,
                            'published': vn_time,
                            'published_str': vn_time.strftime("%H:%M %d/%m"),
                            'description': html.unescape(description) if description else ""
                        }
                        all_news.append(news_item)
                        entries_processed += 1
                    
                except Exception:
                    continue
                    
            print(f"‚úÖ Stealth got {entries_processed} news from {source_name}")
            
        except Exception as e:
            print(f"‚ùå Stealth error from {source_name}: {e}")
            continue
    
    # Enhanced deduplication
    unique_news = []
    seen_links = set()
    
    for news in all_news:
        if news['link'] not in seen_links:
            seen_links.add(news['link'])
            unique_news.append(news)
    
    unique_news.sort(key=lambda x: x['published'], reverse=True)
    return unique_news

def save_user_news_enhanced(user_id, news_list, command_type):
    """Enhanced user news saving"""
    global user_news_cache
    
    user_news_cache[user_id] = {
        'news': news_list,
        'command': command_type,
        'timestamp': get_current_vietnam_datetime()
    }
    
    if len(user_news_cache) > MAX_CACHE_ENTRIES:
        oldest_users = sorted(user_news_cache.items(), key=lambda x: x[1]['timestamp'])[:10]
        for user_id_to_remove, _ in oldest_users:
            del user_news_cache[user_id_to_remove]

# üÜï DISCORD EMBED OPTIMIZATION FUNCTIONS
def split_text_for_discord(text: str, max_length: int = 1024) -> List[str]:
    """Split text to fit Discord field limits"""
    if len(text) <= max_length:
        return [text]
    
    parts = []
    current_part = ""
    
    # Split by sentences first
    sentences = text.split('. ')
    
    for sentence in sentences:
        if len(current_part + sentence + '. ') <= max_length:
            current_part += sentence + '. '
        else:
            if current_part:
                parts.append(current_part.strip())
                current_part = sentence + '. '
            else:
                # If single sentence is too long, split by words
                words = sentence.split(' ')
                for word in words:
                    if len(current_part + word + ' ') <= max_length:
                        current_part += word + ' '
                    else:
                        if current_part:
                            parts.append(current_part.strip())
                            current_part = word + ' '
                        else:
                            # If single word is too long, force split
                            parts.append(word[:max_length])
                            current_part = word[max_length:] + ' '
    
    if current_part:
        parts.append(current_part.strip())
    
    return parts

def create_optimized_embeds(title: str, content: str, color: int = 0x9932cc) -> List[discord.Embed]:
    """Create optimized embeds that fit Discord limits"""
    embeds = []
    
    # Split content into parts that fit field value limit (1024 chars)
    content_parts = split_text_for_discord(content, 1000)  # Leave some margin
    
    for i, part in enumerate(content_parts):
        if i == 0:
            embed = discord.Embed(
                title=title[:256],  # Title limit
                color=color
            )
        else:
            embed = discord.Embed(
                title=f"{title[:200]}... (Ph·∫ßn {i+1})",  # Shorter title for continuation
                color=color
            )
        
        embed.add_field(
            name=f"üìÑ N·ªôi dung {f'(Ph·∫ßn {i+1})' if len(content_parts) > 1 else ''}",
            value=part,
            inline=False
        )
        
        embeds.append(embed)
    
    return embeds

# Bot event handlers
@bot.event
async def on_ready():
    print(f'‚úÖ {bot.user} is online!')
    print(f'üìä Connected to {len(bot.guilds)} server(s)')
    
    ai_count = len(debate_engine.available_engines)
    if ai_count >= 1:
        print(f'üöÄ Enhanced Multi-AI: {ai_count} FREE AI engines ready')
        ai_names = [debate_engine.ai_engines[ai]['name'] for ai in debate_engine.available_engines]
        print(f'ü§ñ FREE Participants: {", ".join(ai_names)}')
        
        for ai_provider in debate_engine.available_engines:
            ai_info = debate_engine.ai_engines[ai_provider]
            print(f'   ‚Ä¢ {ai_info["name"]} {ai_info["emoji"]}: {ai_info["free_limit"]} - {ai_info["strength"]}')
    else:
        print('‚ö†Ô∏è Warning: Need at least 1 AI engine')
    
    current_datetime_str = get_current_datetime_str()
    print(f'üîß Current Vietnam time: {current_datetime_str}')
    print('üèóÔ∏è Enhanced with FULL NEWS SOURCES (17 sources)')
    print('üí∞ Cost: $0/month (FREE AI tiers only)')
    
    if GOOGLE_API_KEY and GOOGLE_CSE_ID:
        print('üîç Google Search API: Available')
    else:
        print('üîß Google Search API: Using enhanced fallback')
    
    total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
    print(f'üì∞ Ready with {total_sources} RSS sources (Full restoration)')
    print('üéØ Type !menu for guide')
    
    status_text = f"Stealth ‚Ä¢ {ai_count} FREE AIs ‚Ä¢ 17 sources ‚Ä¢ Anti-Detection ‚Ä¢ !menu"
    await bot.change_presence(
        activity=discord.Activity(
            type=discord.ActivityType.watching,
            name=status_text
        )
    )

@bot.event
async def on_command_error(ctx, error):
    if isinstance(error, commands.CommandNotFound):
        return
    else:
        print(f"‚ùå Command error: {error}")
        await ctx.send(f"‚ùå L·ªói: {str(error)}")

# üöÄ ENHANCED MAIN COMMAND - Optimized Discord Display
@bot.command(name='hoi')
async def enhanced_gemini_question(ctx, *, question):
    """üöÄ Enhanced Gemini System v·ªõi t·ªëi ∆∞u hi·ªÉn th·ªã Discord"""
    
    try:
        if len(debate_engine.available_engines) < 1:
            embed = discord.Embed(
                title="‚ö†Ô∏è Gemini AI System kh√¥ng kh·∫£ d·ª•ng",
                description=f"C·∫ßn Gemini AI ƒë·ªÉ ho·∫°t ƒë·ªông. Hi·ªán c√≥: {len(debate_engine.available_engines)} engine",
                color=0xff6b6b
            )
            await ctx.send(embed=embed)
            return
        
        current_datetime_str = get_current_datetime_str()
        
        # Progress message
        progress_embed = discord.Embed(
            title="üíé Gemini Intelligent System - Enhanced",
            description=f"**C√¢u h·ªèi:** {question}\nüß† **ƒêang ph√¢n t√≠ch v·ªõi Gemini AI...**",
            color=0x9932cc,
            timestamp=ctx.message.created_at
        )
        
        if AIProvider.GEMINI in debate_engine.ai_engines:
            gemini_info = debate_engine.ai_engines[AIProvider.GEMINI]
            ai_status = f"{gemini_info['emoji']} **{gemini_info['name']}** - {gemini_info['strength']} ({gemini_info['free_limit']}) ‚úÖ"
        else:
            ai_status = "‚ùå Gemini kh√¥ng kh·∫£ d·ª•ng"
        
        progress_embed.add_field(
            name="ü§ñ Gemini Enhanced Engine",
            value=ai_status,
            inline=False
        )
        
        progress_embed.add_field(
            name="üöÄ Enhanced Features",
            value="‚úÖ **Smart Analysis**: Ki·∫øn th·ª©c chuy√™n s√¢u + d·ªØ li·ªáu th·ªùi gian th·ª±c\n‚úÖ **Full Sources**: 17 ngu·ªìn RSS ƒë∆∞·ª£c kh√¥i ph·ª•c\n‚úÖ **Wikipedia**: Knowledge base integration\n‚úÖ **Auto-extract**: N·ªôi dung chi ti·∫øt t·ª´ b√†i vi·∫øt\n‚úÖ **Discord Optimized**: Hi·ªÉn th·ªã t·ªëi ∆∞u",
            inline=False
        )
        
        progress_msg = await ctx.send(embed=progress_embed)
        
        # Start analysis
        print(f"\nüíé STARTING ENHANCED GEMINI ANALYSIS for: {question}")
        analysis_result = await debate_engine.enhanced_multi_ai_debate(question, max_sources=4)
        
        # Handle results
        if 'error' in analysis_result:
            error_embed = discord.Embed(
                title="‚ùå Gemini Enhanced System - Error",
                description=f"**C√¢u h·ªèi:** {question}\n**L·ªói:** {analysis_result['error']}",
                color=0xff6b6b,
                timestamp=ctx.message.created_at
            )
            await progress_msg.edit(embed=error_embed)
            return
        
        # Success - Create optimized embeds
        final_answer = analysis_result.get('final_answer', 'Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi.')
        strategy = analysis_result.get('gemini_response', {}).get('search_strategy', 'knowledge_based')
        strategy_text = "D·ªØ li·ªáu hi·ªán t·∫°i" if strategy == 'current_data' else "Ki·∫øn th·ª©c chuy√™n s√¢u"
        
        # Create optimized embeds for Discord limits
        title = f"üíé Gemini Enhanced Analysis - {strategy_text}"
        optimized_embeds = create_optimized_embeds(title, final_answer, 0x00ff88)
        
        # Add metadata to first embed
        search_sources = analysis_result.get('gemini_response', {}).get('search_sources', [])
        source_types = []
        if any('wikipedia' in s.get('source_name', '').lower() for s in search_sources):
            source_types.append("üìö Wikipedia")
        if any(s.get('source_name', '') in ['CafeF', 'VnEconomy', 'SJC', 'PNJ'] for s in search_sources):
            source_types.append("üìä D·ªØ li·ªáu t√†i ch√≠nh")
        if any('reuters' in s.get('source_name', '').lower() or 'bloomberg' in s.get('source_name', '').lower() for s in search_sources):
            source_types.append("üì∞ Tin t·ª©c qu·ªëc t·∫ø")
        
        analysis_method = " + ".join(source_types) if source_types else "üß† Ki·∫øn th·ª©c ri√™ng"
        
        if optimized_embeds:
            optimized_embeds[0].add_field(
                name="üîç Ph∆∞∆°ng ph√°p ph√¢n t√≠ch",
                value=f"**Strategy:** {strategy_text}\n**Sources:** {analysis_method}\n**Data Usage:** {'20-40% tin t·ª©c' if strategy == 'current_data' else '5-10% tin t·ª©c'}\n**Knowledge:** {'60-80% Gemini' if strategy == 'current_data' else '90-95% Gemini'}",
                inline=True
            )
            
            optimized_embeds[0].add_field(
                name="üìä Enhanced Statistics",
                value=f"üíé **Engine**: Gemini AI Enhanced\nüèóÔ∏è **Sources**: 17 RSS feeds\nüß† **Strategy**: {strategy_text}\nüìÖ **Date**: {get_current_date_str()}\nüí∞ **Cost**: $0/month",
                inline=True
            )
            
            optimized_embeds[-1].set_footer(text=f"üíé Gemini Enhanced System ‚Ä¢ Full Sources ‚Ä¢ {current_datetime_str}")
        
        # Send optimized embeds
        await progress_msg.edit(embed=optimized_embeds[0])
        
        for embed in optimized_embeds[1:]:
            await ctx.send(embed=embed)
        
        print(f"‚úÖ ENHANCED GEMINI ANALYSIS COMPLETED for: {question}")
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói h·ªá th·ªëng Gemini Enhanced: {str(e)}")
        print(f"‚ùå ENHANCED GEMINI ERROR: {e}")

# üöÄ ENHANCED NEWS COMMANDS V·ªöI ƒê·∫¶Y ƒê·ª¶ NGU·ªíN
@bot.command(name='all')
async def get_all_news_enhanced(ctx, page=1):
    """üöÄ Enhanced news t·ª´ t·∫•t c·∫£ 17 ngu·ªìn"""
    try:
        page = max(1, int(page))
        loading_msg = await ctx.send(f"‚è≥ ƒêang t·∫£i tin t·ª©c t·ª´ 17 ngu·ªìn - Enhanced...")
        
        domestic_news = await collect_news_stealth_enhanced(RSS_FEEDS['domestic'], 6)
        international_news = await collect_news_stealth_enhanced(RSS_FEEDS['international'], 5)
        
        await loading_msg.delete()
        
        all_news = domestic_news + international_news
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = all_news[start_index:end_index]
        
        if not page_news:
            total_pages = (len(all_news) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        embed = discord.Embed(
            title=f"üì∞ Tin t·ª©c t·ªïng h·ª£p Enhanced (Trang {page})",
            description=f"üöÄ Enhanced v·ªõi 17 ngu·ªìn RSS ‚Ä¢ Auto-extract ‚Ä¢ Auto-translate",
            color=0x00ff88
        )
        
        domestic_count = sum(1 for news in page_news if news['source'] in RSS_FEEDS['domestic'])
        international_count = len(page_news) - domestic_count
        
        embed.add_field(
            name="üìä Enhanced Statistics",
            value=f"üáªüá≥ Trong n∆∞·ªõc: {domestic_count} tin (9 ngu·ªìn)\nüåç Qu·ªëc t·∫ø: {international_count} tin (8 ngu·ªìn)\nüìä T·ªïng c√≥ s·∫µn: {len(all_news)} tin\nüìÖ C·∫≠p nh·∫≠t: {get_current_datetime_str()}",
            inline=False
        )
        
        # Enhanced emoji mapping
        emoji_map = {
            'cafef_main': '‚òï', 'cafef_chungkhoan': 'üìà', 'cafef_batdongsan': 'üè¢', 'cafef_taichinh': 'üí∞', 'cafef_vimo': 'üìä',
            'cafebiz_main': 'üíº', 'baodautu_main': 'üéØ', 'vneconomy_main': 'üì∞', 'vneconomy_chungkhoan': 'üìà',
            'vnexpress_kinhdoanh': '‚ö°', 'vnexpress_chungkhoan': 'üìà', 'thanhnien_kinhtevimo': 'üìä', 'thanhnien_chungkhoan': 'üìà',
            'nhandanonline_tc': 'üèõÔ∏è', 'yahoo_finance': 'üí∞', 'reuters_business': 'üåç', 'bloomberg_markets': 'üíπ', 
            'marketwatch_latest': 'üìà', 'forbes_money': 'üíé', 'financial_times': 'üíº', 'business_insider': 'üì∞', 'the_economist': 'üéì'
        }
        
        source_names = {
            'cafef_main': 'CafeF', 'cafef_chungkhoan': 'CafeF CK', 'cafef_batdongsan': 'CafeF BƒêS',
            'cafef_taichinh': 'CafeF TC', 'cafef_vimo': 'CafeF VM', 'cafebiz_main': 'CafeBiz',
            'baodautu_main': 'B√°o ƒê·∫ßu t∆∞', 'vneconomy_main': 'VnEconomy', 'vneconomy_chungkhoan': 'VnEconomy CK',
            'vnexpress_kinhdoanh': 'VnExpress KD', 'vnexpress_chungkhoan': 'VnExpress CK',
            'thanhnien_kinhtevimo': 'Thanh Ni√™n VM', 'thanhnien_chungkhoan': 'Thanh Ni√™n CK',
            'nhandanonline_tc': 'Nh√¢n D√¢n TC', 'yahoo_finance': 'Yahoo Finance', 'reuters_business': 'Reuters',
            'bloomberg_markets': 'Bloomberg', 'marketwatch_latest': 'MarketWatch', 'forbes_money': 'Forbes',
            'financial_times': 'Financial Times', 'business_insider': 'Business Insider', 'the_economist': 'The Economist'
        }
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'üì∞')
            title = news['title'][:60] + "..." if len(news['title']) > 60 else news['title']
            source_display = source_names.get(news['source'], news['source'])
            
            embed.add_field(
                name=f"{i}. {emoji} {title}",
                value=f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üì∞ {source_display}\nüîó [ƒê·ªçc b√†i vi·∫øt]({news['link']})",
                inline=False
            )
        
        save_user_news_enhanced(ctx.author.id, page_news, f"all_page_{page}")
        
        total_pages = (len(all_news) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"üöÄ Stealth ‚Ä¢ 17 ngu·ªìn ‚Ä¢ Trang {page}/{total_pages} ‚Ä¢ !chitiet [s·ªë] xem chi ti·∫øt")
        
        await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='in')
async def get_domestic_news_enhanced(ctx, page=1):
    """üöÄ Enhanced tin t·ª©c trong n∆∞·ªõc t·ª´ 9 ngu·ªìn"""
    try:
        page = max(1, int(page))
        loading_msg = await ctx.send(f"‚è≥ ƒêang t·∫£i tin t·ª©c trong n∆∞·ªõc t·ª´ 9 ngu·ªìn - Enhanced...")
        
        news_list = await collect_news_stealth_enhanced(RSS_FEEDS['domestic'], 8)
        await loading_msg.delete()
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        embed = discord.Embed(
            title=f"üáªüá≥ Tin kinh t·∫ø trong n∆∞·ªõc Enhanced (Trang {page})",
            description=f"üöÄ Enhanced t·ª´ 9 ngu·ªìn chuy√™n ng√†nh ‚Ä¢ Auto-extract",
            color=0xff0000
        )
        
        embed.add_field(
            name="üìä Enhanced Domestic Info",
            value=f"üì∞ T·ªïng tin c√≥ s·∫µn: {len(news_list)} tin\nüéØ Lƒ©nh v·ª±c: Kinh t·∫ø, CK, BƒêS, Vƒ© m√¥\nüöÄ Ngu·ªìn: CafeF, VnEconomy, VnExpress, Thanh Ni√™n, Nh√¢n D√¢n\nüìÖ C·∫≠p nh·∫≠t: {get_current_datetime_str()}",
            inline=False
        )
        
        emoji_map = {
            'cafef_main': '‚òï', 'cafef_chungkhoan': 'üìà', 'cafef_batdongsan': 'üè¢', 'cafef_taichinh': 'üí∞', 'cafef_vimo': 'üìä',
            'cafebiz_main': 'üíº', 'baodautu_main': 'üéØ', 'vneconomy_main': 'üì∞', 'vneconomy_chungkhoan': 'üìà',
            'vnexpress_kinhdoanh': '‚ö°', 'vnexpress_chungkhoan': 'üìà', 'thanhnien_kinhtevimo': 'üìä', 'thanhnien_chungkhoan': 'üìà',
            'nhandanonline_tc': 'üèõÔ∏è'
        }
        
        source_names = {
            'cafef_main': 'CafeF', 'cafef_chungkhoan': 'CafeF CK', 'cafef_batdongsan': 'CafeF BƒêS',
            'cafef_taichinh': 'CafeF TC', 'cafef_vimo': 'CafeF VM', 'cafebiz_main': 'CafeBiz',
            'baodautu_main': 'B√°o ƒê·∫ßu t∆∞', 'vneconomy_main': 'VnEconomy', 'vneconomy_chungkhoan': 'VnEconomy CK',
            'vnexpress_kinhdoanh': 'VnExpress KD', 'vnexpress_chungkhoan': 'VnExpress CK',
            'thanhnien_kinhtevimo': 'Thanh Ni√™n VM', 'thanhnien_chungkhoan': 'Thanh Ni√™n CK',
            'nhandanonline_tc': 'Nh√¢n D√¢n TC'
        }
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'üì∞')
            title = news['title'][:60] + "..." if len(news['title']) > 60 else news['title']
            source_display = source_names.get(news['source'], news['source'])
            
            embed.add_field(
                name=f"{i}. {emoji} {title}",
                value=f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üì∞ {source_display}\nüîó [ƒê·ªçc b√†i vi·∫øt]({news['link']})",
                inline=False
            )
        
        save_user_news_enhanced(ctx.author.id, page_news, f"in_page_{page}")
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"üöÄ Stealth ‚Ä¢ 9 ngu·ªìn VN ‚Ä¢ Trang {page}/{total_pages} ‚Ä¢ !chitiet [s·ªë] xem chi ti·∫øt")
        
        await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='out')
async def get_international_news_enhanced(ctx, page=1):
    """üöÄ Enhanced tin t·ª©c qu·ªëc t·∫ø t·ª´ 8 ngu·ªìn v·ªõi auto-translate"""
    try:
        page = max(1, int(page))
        loading_msg = await ctx.send(f"‚è≥ ƒêang t·∫£i tin t·ª©c qu·ªëc t·∫ø t·ª´ 8 ngu·ªìn - Enhanced...")
        
        news_list = await collect_news_stealth_enhanced(RSS_FEEDS['international'], 6)
        await loading_msg.delete()
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        embed = discord.Embed(
            title=f"üåç Tin kinh t·∫ø qu·ªëc t·∫ø Enhanced (Trang {page})",
            description=f"üöÄ Enhanced t·ª´ 8 ngu·ªìn h√†ng ƒë·∫ßu ‚Ä¢ Auto-extract ‚Ä¢ Auto-translate",
            color=0x0066ff
        )
        
        embed.add_field(
            name="üìä Enhanced International Info",
            value=f"üì∞ T·ªïng tin c√≥ s·∫µn: {len(news_list)} tin\nüöÄ Ngu·ªìn: Yahoo Finance, Reuters, Bloomberg, MarketWatch, Forbes, FT, Business Insider, The Economist\nüåê Auto-translate: Ti·∫øng Anh ‚Üí Ti·∫øng Vi·ªát\nüìÖ C·∫≠p nh·∫≠t: {get_current_datetime_str()}",
            inline=False
        )
        
        emoji_map = {
            'yahoo_finance': 'üí∞', 'reuters_business': 'üåç', 'bloomberg_markets': 'üíπ', 'marketwatch_latest': 'üìà',
            'forbes_money': 'üíé', 'financial_times': 'üíº', 'business_insider': 'üì∞', 'the_economist': 'üéì'
        }
        
        source_names = {
            'yahoo_finance': 'Yahoo Finance', 'reuters_business': 'Reuters', 'bloomberg_markets': 'Bloomberg', 
            'marketwatch_latest': 'MarketWatch', 'forbes_money': 'Forbes', 'financial_times': 'Financial Times', 
            'business_insider': 'Business Insider', 'the_economist': 'The Economist'
        }
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'üåç')
            title = news['title'][:60] + "..." if len(news['title']) > 60 else news['title']
            source_display = source_names.get(news['source'], news['source'])
            
            embed.add_field(
                name=f"{i}. {emoji} {title}",
                value=f"üï∞Ô∏è {news['published_str']} ‚Ä¢ üì∞ {source_display}\nüîó [ƒê·ªçc b√†i vi·∫øt]({news['link']})",
                inline=False
            )
        
        save_user_news_enhanced(ctx.author.id, page_news, f"out_page_{page}")
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"üöÄ Stealth ‚Ä¢ 8 ngu·ªìn QT ‚Ä¢ Trang {page}/{total_pages} ‚Ä¢ !chitiet [s·ªë] (auto-translate)")
        
        await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

# üöÄ ENHANCED ARTICLE DETAILS COMMAND
@bot.command(name='chitiet')
async def get_news_detail_enhanced(ctx, news_number: int):
    """üöÄ Enhanced chi ti·∫øt b√†i vi·∫øt v·ªõi content extraction ƒë∆∞·ª£c s·ª≠a l·ªói"""
    try:
        user_id = ctx.author.id
        
        if user_id not in user_news_cache:
            await ctx.send("‚ùå B·∫°n ch∆∞a xem tin t·ª©c! D√πng `!all`, `!in`, ho·∫∑c `!out` tr∆∞·ªõc.")
            return
        
        user_data = user_news_cache[user_id]
        news_list = user_data['news']
        
        if news_number < 1 or news_number > len(news_list):
            await ctx.send(f"‚ùå S·ªë kh√¥ng h·ª£p l·ªá! Ch·ªçn t·ª´ 1 ƒë·∫øn {len(news_list)}")
            return
        
        news = news_list[news_number - 1]
        
        loading_msg = await ctx.send(f"üöÄ ƒêang tr√≠ch xu·∫•t n·ªôi dung Stealth Enhanced (bypass 403/406)...")
        
        # Stealth content extraction (fixed)
        full_content = await fetch_content_stealth_enhanced(news['link'])
        
        # Enhanced auto-translate
        source_name = extract_source_name(news['link'])
        translated_content, is_translated = await detect_and_translate_content_enhanced(full_content, source_name)
        
        await loading_msg.delete()
        
        # Create optimized embeds for Discord
        title_suffix = " üåê (ƒê√£ d·ªãch)" if is_translated else ""
        main_title = f"üìñ Chi ti·∫øt b√†i vi·∫øt Enhanced{title_suffix}"
        
        # Create content with metadata
        content_with_meta = f"**üì∞ Ti√™u ƒë·ªÅ:** {news['title']}\n"
        content_with_meta += f"**üï∞Ô∏è Th·ªùi gian:** {news['published_str']} ({get_current_date_str()})\n"
        content_with_meta += f"**üì∞ Ngu·ªìn:** {source_name}{'üåê' if is_translated else ''}\n"
        
        extraction_methods = []
        if TRAFILATURA_AVAILABLE:
            extraction_methods.append("üöÄ Trafilatura")
        if NEWSPAPER_AVAILABLE:
            extraction_methods.append("üì∞ Newspaper3k")
        extraction_methods.append("üîÑ Legacy")
        
        content_with_meta += f"**üöÄ Enhanced Extract:** {' ‚Üí '.join(extraction_methods)}\n\n"
        
        if is_translated:
            content_with_meta += f"**üîÑ Enhanced Auto-Translate:** Groq AI ƒë√£ d·ªãch t·ª´ ti·∫øng Anh\n\n"
        
        content_with_meta += f"**üìÑ N·ªôi dung chi ti·∫øt:**\n{translated_content}"
        
        # Create optimized embeds
        optimized_embeds = create_optimized_embeds(main_title, content_with_meta, 0x9932cc)
        
        # Add link to last embed
        if optimized_embeds:
            optimized_embeds[-1].add_field(
                name="üîó ƒê·ªçc b√†i vi·∫øt ƒë·∫ßy ƒë·ªß",
                value=f"[Nh·∫•n ƒë·ªÉ ƒë·ªçc to√†n b·ªô b√†i vi·∫øt{'g·ªëc' if is_translated else ''}]({news['link']})",
                inline=False
            )
            
            optimized_embeds[-1].set_footer(text=f"üöÄ Stealth Content Extraction ‚Ä¢ Tin s·ªë {news_number} ‚Ä¢ !hoi [question]")
        
        # Send optimized embeds
        for embed in optimized_embeds:
            await ctx.send(embed=embed)
        
        print(f"‚úÖ Enhanced content extraction completed for: {news['title'][:50]}...")
        
    except ValueError:
        await ctx.send("‚ùå Vui l√≤ng nh·∫≠p s·ªë! V√≠ d·ª•: `!chitiet 5`")
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")
        print(f"‚ùå Enhanced content extraction error: {e}")

@bot.command(name='cuthe')
async def get_news_detail_alias_stealth(ctx, news_number: int):
    """üöÄ Alias cho l·ªánh !chitiet Stealth Enhanced"""
    await get_news_detail_enhanced(ctx, news_number)

@bot.command(name='menu')
async def help_command_enhanced(ctx):
    """üöÄ Enhanced menu guide v·ªõi full features"""
    current_datetime_str = get_current_datetime_str()
    
    embed = discord.Embed(
        title="üöÄ Stealth Multi-AI Discord News Bot - Anti-Detection Edition",
        description=f"Bot tin t·ª©c AI v·ªõi Stealth Tech bypass 403/406 - {current_datetime_str}",
        color=0xff9900
    )
    
    ai_count = len(debate_engine.available_engines)
    if ai_count >= 1:
        ai_status = f"üöÄ **{ai_count} Enhanced AI Engines**\n"
        for ai_provider in debate_engine.available_engines:
            ai_info = debate_engine.ai_engines[ai_provider]
            ai_status += f"{ai_info['emoji']} **{ai_info['name']}** - {ai_info['strength']} ({ai_info['free_limit']}) ‚úÖ\n"
    else:
        ai_status = "‚ö†Ô∏è C·∫ßn √≠t nh·∫•t 1 AI engine ƒë·ªÉ ho·∫°t ƒë·ªông"
    
    embed.add_field(name="üöÄ Enhanced AI Status", value=ai_status, inline=False)
    
    embed.add_field(
        name="ü•ä Enhanced AI Commands",
        value=f"**!hoi [c√¢u h·ªèi]** - Gemini AI v·ªõi d·ªØ li·ªáu th·ªùi gian th·ª±c {get_current_date_str()}\n*VD: !hoi gi√° v√†ng h√¥m nay*\n*VD: !hoi ch·ª©ng kho√°n vi·ªát nam*\n*VD: !hoi l·∫°m ph√°t l√† g√¨*",
        inline=False
    )
    
    embed.add_field(
        name="üì∞ Enhanced News Commands",
        value="**!all [trang]** - Tin t·ª´ 17 ngu·ªìn (12 tin/trang)\n**!in [trang]** - Tin trong n∆∞·ªõc (9 ngu·ªìn)\n**!out [trang]** - Tin qu·ªëc t·∫ø (8 ngu·ªìn + auto-translate)\n**!chitiet [s·ªë]** - Chi ti·∫øt (üöÄ Enhanced extraction + auto-translate)",
        inline=False
    )
    
    embed.add_field(
        name="üöÄ Stealth Features - Anti-Detection",
        value=f"‚úÖ **Stealth Headers**: 10+ User-Agents rotation\n‚úÖ **Session Management**: Cookies & anti-detection\n‚úÖ **Random Delays**: Bypass rate limiting\n‚úÖ **403/406 Bypass**: Bloomberg, Reuters accessible\n‚úÖ **Graceful Fallback**: Summary khi kh√¥ng extract ƒë∆∞·ª£c\n‚úÖ **Discord Optimized**: T·ª± ƒë·ªông ph√¢n t√°ch n·ªôi dung AI\n‚úÖ **Auto-translate**: Groq AI cho tin qu·ªëc t·∫ø",
        inline=False
    )
    
    embed.add_field(
        name="üéØ Stealth Examples",
        value=f"**!hoi gi√° v√†ng h√¥m nay** - AI t√¨m gi√° v√†ng {get_current_date_str()}\n**!hoi t·ª∑ gi√° usd vnd** - AI t√¨m t·ª∑ gi√° hi·ªán t·∫°i\n**!hoi l·∫°m ph√°t vi·ªát nam** - AI gi·∫£i th√≠ch l·∫°m ph√°t\n**!all** - Xem tin t·ª´ 17 ngu·ªìn (stealth)\n**!chitiet 1** - Xem chi ti·∫øt v·ªõi Stealth extraction (bypass 403)",
        inline=False
    )
    
    # Enhanced status
    search_status = "‚úÖ Enhanced search"
    if GOOGLE_API_KEY and GOOGLE_CSE_ID:
        search_status += " + Google API"
    
    embed.add_field(name="üîç Enhanced Search", value=search_status, inline=True)
    embed.add_field(name="üì∞ News Sources", value=f"üáªüá≥ **Trong n∆∞·ªõc**: 9 ngu·ªìn\nüåç **Qu·ªëc t·∫ø**: 8 ngu·ªìn\nüìä **T·ªïng**: 17 ngu·ªìn\nüöÄ **Status**: Enhanced & Fixed", inline=True)
    
    embed.set_footer(text=f"üöÄ Stealth Multi-AI ‚Ä¢ Anti-Detection ‚Ä¢ {current_datetime_str}")
    await ctx.send(embed=embed)

# Cleanup function
async def cleanup_stealth():
    """Stealth cleanup"""
    if debate_engine:
        await debate_engine.close_session()
    
    global user_news_cache
    if len(user_news_cache) > MAX_CACHE_ENTRIES:
        user_news_cache.clear()
        print("üßπ Stealth memory cleanup completed")

# Main execution
if __name__ == "__main__":
    try:
        keep_alive()
        print("üöÄ Starting Stealth Multi-AI Discord News Bot - Anti-Detection Edition...")
        print("üèóÔ∏è Stealth Edition v·ªõi anti-detection techniques ƒë·ªÉ bypass 403/406")
        
        ai_count = len(debate_engine.available_engines)
        print(f"ü§ñ Stealth Multi-AI System: {ai_count} FREE engines initialized")
        
        current_datetime_str = get_current_datetime_str()
        print(f"üîß Current Vietnam time: {current_datetime_str}")
        
        if ai_count >= 1:
            ai_names = [debate_engine.ai_engines[ai]['name'] for ai in debate_engine.available_engines]
            print(f"ü•ä Stealth debate ready with: {', '.join(ai_names)}")
            print("üí∞ Cost: $0/month (FREE AI tiers only)")
            print("üöÄ Features: 17 News sources + Stealth extraction + Anti-detection + Auto-translate + Multi-AI + Discord optimized")
        else:
            print("‚ö†Ô∏è Warning: Need at least 1 FREE AI engine")
        
        # Stealth status
        if GOOGLE_API_KEY and GOOGLE_CSE_ID:
            print("üîç Google Search API: Available with Stealth optimization")
        else:
            print("üîß Google Search API: Using Stealth fallback")
        
        if WIKIPEDIA_AVAILABLE:
            print("üìö Wikipedia Knowledge Base: Available")
        else:
            print("‚ö†Ô∏è Wikipedia Knowledge Base: Not available")
        
        total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
        print(f"üìä {total_sources} RSS sources loaded with STEALTH TECHNIQUES")
        
        # Stealth extraction capabilities
        print("\nüöÄ STEALTH CONTENT EXTRACTION (ANTI-DETECTION):")
        extraction_tiers = []
        if TRAFILATURA_AVAILABLE:
            extraction_tiers.append("Tier 1: Stealth Trafilatura (10+ User-Agents)")
        else:
            print("‚ùå Trafilatura: Not available")
        
        if NEWSPAPER_AVAILABLE:
            extraction_tiers.append("Tier 2: Stealth Newspaper3k (Session Management)")
        else:
            print("‚ùå Newspaper3k: Not available")
        
        extraction_tiers.append("Tier 3: Stealth Legacy (Always works)")
        
        for tier in extraction_tiers:
            print(f"‚úÖ {tier}")
        
        print("\nüöÄ STEALTH OPTIMIZATIONS:")
        print("‚úÖ User-Agent rotation: 10+ realistic browsers")
        print("‚úÖ Headers spoofing: Complete browser fingerprint")
        print("‚úÖ Random delays: 1-5 seconds anti-rate-limit")
        print("‚úÖ Session management: Cookies & state persistence")
        print("‚úÖ 403/406 bypass: Multiple retry strategies")
        print("‚úÖ Graceful fallback: Summary when extraction fails")
        print("‚úÖ Discord optimization: Auto-split for embed limits")
        
        print(f"\n‚úÖ Stealth Multi-AI Discord News Bot ready!")
        print(f"üí° Use !hoi [question] to get stealth Gemini answers")
        print("üí° Use !all, !in, !out for stealth news from 17 sources")
        print("üí° Use !chitiet [number] for stealth details (bypass 403/406)")
        print(f"üí° Date auto-updates: {current_datetime_str}")
        print("üí° Content extraction: Stealth techniques ‚Üí Bypass all blocks")
        print("üí° Anti-detection: Multiple User-Agents, delays, sessions")
        
        print("\n" + "="*70)
        print("üöÄ STEALTH MULTI-AI DISCORD NEWS BOT - ANTI-DETECTION EDITION")
        print("üí∞ COST: $0/month (100% FREE AI tiers)")
        print("üì∞ SOURCES: 17 RSS feeds (9 VN + 8 International) - STEALTH ACCESS")
        print("ü§ñ AI: Gemini (Primary) + Groq (Translation)")
        print("üöÄ STEALTH: User-Agent rotation, Session management, Anti-detection")
        print("üõ°Ô∏è BYPASS: 403 Forbidden, 406 Not Acceptable, Rate limits")
        print("üéØ USAGE: !menu for complete guide")
        print("="*70)
        
        bot.run(TOKEN)
        
    except discord.LoginFailure:
        print("‚ùå Discord login error!")
        print("üîß Token may be invalid or reset")
        print("üîß Check DISCORD_TOKEN in Environment Variables")
        
    except Exception as e:
        print(f"‚ùå Bot startup error: {e}")
        print("üîß Check internet connection and Environment Variables")
        
    finally:
        try:
            asyncio.run(cleanup_stealth())
        except:
            pass
