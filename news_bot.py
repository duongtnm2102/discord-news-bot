import discord
from discord.ext import commands
import feedparser
import requests
import asyncio
import os
import re
from datetime import datetime
import time
import calendar
from urllib.parse import urljoin
import html
import chardet
import pytz
import json
import aiohttp
from keep_alive import keep_alive
import google.generativeai as genai
from enum import Enum

# üÜï TH√äM C√ÅC TH·ª¨ VI·ªÜN N√ÇNG CAO (OPTIONAL)
try:
    import trafilatura
    TRAFILATURA_AVAILABLE = True
    print("‚úÖ Trafilatura ƒë√£ ƒë∆∞·ª£c t√≠ch h·ª£p - Tr√≠ch xu·∫•t n·ªôi dung c·∫£i ti·∫øn!")
except ImportError:
    TRAFILATURA_AVAILABLE = False
    print("‚ö†Ô∏è Trafilatura kh√¥ng c√≥ s·∫µn - S·∫Ω d√πng ph∆∞∆°ng ph√°p c∆° b·∫£n")

try:
    import newspaper
    from newspaper import Article
    NEWSPAPER_AVAILABLE = True
    print("‚úÖ Newspaper3k ƒë√£ ƒë∆∞·ª£c t√≠ch h·ª£p - Fallback extraction!")
except ImportError:
    NEWSPAPER_AVAILABLE = False
    print("‚ö†Ô∏è Newspaper3k kh√¥ng c√≥ s·∫µn - S·∫Ω d√πng ph∆∞∆°ng ph√°p c∆° b·∫£n")

# üÜï MULTI-AI ENGINE ARCHITECTURE
class AIProvider(Enum):
    GEMINI = "gemini"
    DEEPSEEK = "deepseek"
    CLAUDE = "claude"
    GROQ = "groq"  # Fallback

# ü§ñ AI CONFIGURATION
AI_CONFIGS = {
    AIProvider.GEMINI: {
        'api_key_env': 'GEMINI_API_KEY',
        'model': 'gemini-2.0-flash-exp',
        'endpoint': 'google_ai_studio',
        'free_tier': 'unlimited',
        'strengths': ['search_integration', 'instruction_following', 'multimodal']
    },
    AIProvider.DEEPSEEK: {
        'api_key_env': 'DEEPSEEK_API_KEY',
        'model': 'deepseek-v3',
        'endpoint': 'https://api.deepseek.com/v1/chat/completions',
        'free_tier': 'generous',
        'strengths': ['reasoning', 'cost_effective', 'math']
    },
    AIProvider.CLAUDE: {
        'api_key_env': 'ANTHROPIC_API_KEY',
        'model': 'claude-3-5-sonnet-20241022',
        'endpoint': 'https://api.anthropic.com/v1/messages',
        'free_tier': 'limited',
        'strengths': ['safety', 'analysis', 'structured_output']
    },
    AIProvider.GROQ: {
        'api_key_env': 'GROQ_API_KEY',
        'model': 'llama-3.3-70b-versatile',
        'endpoint': 'https://api.groq.com/openai/v1/chat/completions',
        'free_tier': 'rate_limited',
        'strengths': ['speed', 'compatibility']
    }
}

# C·∫•u h√¨nh bot
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix='!', intents=intents)

# üîí B·∫¢O M·∫¨T: Environment Variables
TOKEN = os.getenv('DISCORD_TOKEN')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
GOOGLE_CSE_ID = os.getenv('GOOGLE_CSE_ID')

# AI API Keys
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')
ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')
GROQ_API_KEY = os.getenv('GROQ_API_KEY')

if not TOKEN:
    print("‚ùå C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y DISCORD_TOKEN trong environment variables!")
    print("üîß Vui l√≤ng th√™m DISCORD_TOKEN v√†o Render Environment Variables")
    exit(1)

# üáªüá≥ TIMEZONE VI·ªÜT NAM - ƒê√É S·ª¨A L·ªñI M√öI GI·ªú
VN_TIMEZONE = pytz.timezone('Asia/Ho_Chi_Minh')
UTC_TIMEZONE = pytz.UTC

# L∆∞u tr·ªØ tin t·ª©c theo t·ª´ng user
user_news_cache = {}

# RSS feeds ƒë√£ ƒë∆∞·ª£c ki·ªÉm tra v√† x√°c nh·∫≠n ho·∫°t ƒë·ªông
RSS_FEEDS = {
    # === KINH T·∫æ TRONG N∆Ø·ªöC - ƒê√É KI·ªÇM TRA ===
    'domestic': {
        # CafeF - RSS ch√≠nh ho·∫°t ƒë·ªông t·ªët
        'cafef_main': 'https://cafef.vn/index.rss',
        'cafef_chungkhoan': 'https://cafef.vn/thi-truong-chung-khoan.rss',
        'cafef_batdongsan': 'https://cafef.vn/bat-dong-san.rss',
        'cafef_taichinh': 'https://cafef.vn/tai-chinh-ngan-hang.rss',
        'cafef_vimo': 'https://cafef.vn/vi-mo-dau-tu.rss',
        
        # CafeBiz - RSS t·ªïng h·ª£p
        'cafebiz_main': 'https://cafebiz.vn/index.rss',
        
        # B√°o ƒê·∫ßu t∆∞ - RSS ho·∫°t ƒë·ªông
        'baodautu_main': 'https://baodautu.vn/rss.xml',
        
        # VnEconomy - RSS tin t·ª©c ch√≠nh
        'vneconomy_main': 'https://vneconomy.vn/rss/home.rss',
        'vneconomy_chungkhoan': 'https://vneconomy.vn/rss/chung-khoan.rss',
        
        # VnExpress Kinh doanh 
        'vnexpress_kinhdoanh': 'https://vnexpress.net/rss/kinh-doanh.rss',
        'vnexpress_chungkhoan': 'https://vnexpress.net/rss/kinh-doanh/chung-khoan.rss',
        
        # Thanh Ni√™n - RSS kinh t·∫ø
        'thanhnien_kinhtevimo': 'https://thanhnien.vn/rss/kinh-te/vi-mo.rss',
        'thanhnien_chungkhoan': 'https://thanhnien.vn/rss/kinh-te/chung-khoan.rss',
        
        # Nh√¢n D√¢n - RSS t√†i ch√≠nh ch·ª©ng kho√°n
        'nhandanonline_tc': 'https://nhandan.vn/rss/tai-chinh-chung-khoan.rss'
    },
    
    # === KINH T·∫æ QU·ªêC T·∫æ ===
    'international': {
        'yahoo_finance': 'https://feeds.finance.yahoo.com/rss/2.0/headline',
        'reuters_business': 'https://feeds.reuters.com/reuters/businessNews',
        'bloomberg_markets': 'https://feeds.bloomberg.com/markets/news.rss',
        'marketwatch_latest': 'https://feeds.marketwatch.com/marketwatch/realtimeheadlines/',
        'forbes_money': 'https://www.forbes.com/money/feed/',
        'financial_times': 'https://www.ft.com/rss/home',
        'business_insider': 'https://feeds.businessinsider.com/custom/all',
        'the_economist': 'https://www.economist.com/rss'
    }
}

def convert_utc_to_vietnam_time(utc_time_tuple):
    """üîß S·ª¨A L·ªñI M√öI GI·ªú: Chuy·ªÉn ƒë·ªïi UTC sang gi·ªù Vi·ªát Nam ch√≠nh x√°c"""
    try:
        # S·ª≠ d·ª•ng calendar.timegm() thay v√¨ time.mktime() ƒë·ªÉ x·ª≠ l√Ω UTC ƒë√∫ng c√°ch
        utc_timestamp = calendar.timegm(utc_time_tuple)
        
        # T·∫°o datetime object UTC
        utc_dt = datetime.fromtimestamp(utc_timestamp, tz=UTC_TIMEZONE)
        
        # Chuy·ªÉn sang m√∫i gi·ªù Vi·ªát Nam
        vn_dt = utc_dt.astimezone(VN_TIMEZONE)
        
        return vn_dt
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói chuy·ªÉn ƒë·ªïi m√∫i gi·ªù: {e}")
        # Fallback: s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
        return datetime.now(VN_TIMEZONE)

# üÜï AI ENGINE MANAGER
class AIEngineManager:
    def __init__(self):
        self.primary_ai = None
        self.fallback_ais = []
        self.initialize_engines()
    
    def initialize_engines(self):
        """Kh·ªüi t·∫°o c√°c AI engines theo th·ª© t·ª± ∆∞u ti√™n"""
        available_engines = []
        
        # Gemini - Highest priority
        if GEMINI_API_KEY:
            try:
                genai.configure(api_key=GEMINI_API_KEY)
                available_engines.append(AIProvider.GEMINI)
                print("‚úÖ Gemini AI initialized - PRIMARY ENGINE")
            except Exception as e:
                print(f"‚ö†Ô∏è Gemini initialization failed: {e}")
        
        # DeepSeek - Second priority  
        if DEEPSEEK_API_KEY:
            available_engines.append(AIProvider.DEEPSEEK)
            print("‚úÖ DeepSeek AI available - FALLBACK 1")
            
        # Claude - Third priority
        if ANTHROPIC_API_KEY:
            available_engines.append(AIProvider.CLAUDE)
            print("‚úÖ Claude AI available - FALLBACK 2")
            
        # Groq - Last fallback
        if GROQ_API_KEY:
            available_engines.append(AIProvider.GROQ)
            print("‚úÖ Groq AI available - LAST FALLBACK")
        
        if available_engines:
            self.primary_ai = available_engines[0]
            self.fallback_ais = available_engines[1:]
            print(f"üöÄ Primary AI: {self.primary_ai.value}")
            print(f"üõ°Ô∏è Fallback AIs: {[ai.value for ai in self.fallback_ais]}")
        else:
            print("‚ùå No AI engines available!")
            self.primary_ai = None

    async def call_ai_with_fallback(self, prompt, context="", require_specific_data=True):
        """G·ªçi AI v·ªõi fallback automatic"""
        
        # Th·ª≠ primary AI tr∆∞·ªõc
        if self.primary_ai:
            try:
                response = await self._call_specific_ai(self.primary_ai, prompt, context, require_specific_data)
                if self._validate_response(response, require_specific_data):
                    return response, self.primary_ai.value
            except Exception as e:
                print(f"‚ö†Ô∏è Primary AI {self.primary_ai.value} failed: {e}")
        
        # Th·ª≠ fallback AIs
        for fallback_ai in self.fallback_ais:
            try:
                response = await self._call_specific_ai(fallback_ai, prompt, context, require_specific_data)
                if self._validate_response(response, require_specific_data):
                    print(f"‚úÖ Fallback to {fallback_ai.value} successful")
                    return response, fallback_ai.value
            except Exception as e:
                print(f"‚ö†Ô∏è Fallback AI {fallback_ai.value} failed: {e}")
                continue
        
        # N·∫øu t·∫•t c·∫£ fail
        return "‚ùå T·∫•t c·∫£ AI engines ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng th·ª≠ l·∫°i sau.", "error"

    async def _call_specific_ai(self, ai_provider, prompt, context, require_specific_data):
        """G·ªçi AI engine c·ª• th·ªÉ"""
        
        if ai_provider == AIProvider.GEMINI:
            return await self._call_gemini(prompt, context, require_specific_data)
        elif ai_provider == AIProvider.DEEPSEEK:
            return await self._call_deepseek(prompt, context, require_specific_data)
        elif ai_provider == AIProvider.CLAUDE:
            return await self._call_claude(prompt, context, require_specific_data)
        elif ai_provider == AIProvider.GROQ:
            return await self._call_groq(prompt, context, require_specific_data)
        
        raise Exception(f"Unknown AI provider: {ai_provider}")

    async def _call_gemini(self, prompt, context, require_specific_data):
        """üöÄ Gemini 2.5 Flash - RECOMMENDED"""
        
        # T·∫°o prompt si√™u nghi√™m kh·∫Øc cho Gemini
        system_prompt = """B·∫†N L√Ä CHUY√äN GIA T√ÄI CH√çNH VI·ªÜT NAM. QUY T·∫ÆC NGHI√äM NG·∫∂T:

üî• B·∫ÆT BU·ªòC (VI PH·∫†M = TH·∫§T B·∫†I HO√ÄN TO√ÄN):
1. S·ª¨ D·ª§NG S·ªê LI·ªÜU C·ª§ TH·ªÇ t·ª´ n·ªôi dung tin t·ª©c ƒë∆∞·ª£c cung c·∫•p
2. N√äU TH·ªúI GIAN C·ª§ TH·ªÇ (ng√†y/th√°ng/nƒÉm, gi·ªù n·∫øu c√≥)  
3. TR√çCH D·∫™N CH√çNH X√ÅC t·ª´ ngu·ªìn tin
4. GI·∫¢I TH√çCH L√ù DO d·ª±a tr√™n s·ª± ki·ªán th·ª±c t·∫ø

‚ùå NGHI√äM C·∫§M:
- N√≥i chung chung: "th∆∞·ªùng", "c√≥ th·ªÉ", "n√≥i chung"
- D√πng d·ªØ li·ªáu c≈© kh√¥ng c√≥ trong tin t·ª©c
- ƒê∆∞a ra √Ω ki·∫øn c√° nh√¢n kh√¥ng d·ª±a tr√™n facts

‚úÖ ƒê·ªäNH D·∫†NG B·∫ÆT BU·ªòC:
[S·ªê LI·ªÜU HI·ªÜN T·∫†I] - [TH·ªúI GIAN] - [L√ù DO C·ª§ TH·ªÇ] - [NGU·ªíN]

üéØ N·∫æU KH√îNG C√ì ƒê·ª¶ TH√îNG TIN: Tr·∫£ l·ªùi "Kh√¥ng ƒë·ªß d·ªØ li·ªáu c·ª• th·ªÉ trong c√°c ngu·ªìn tin hi·ªán t·∫°i"""

        full_prompt = f"{system_prompt}\n\nüì∞ TH√îNG TIN T·ª™ NGU·ªíN TIN:\n{context}\n\n‚ùì C√ÇU H·ªéI: {prompt}\n\nüî• TH·ª∞C HI·ªÜN NGAY - TU√ÇN TH·ª¶ NGHI√äM NG·∫∂T:"
        
        model = genai.GenerativeModel('gemini-2.0-flash-exp')
        
        # Configure generation v·ªõi settings strict
        generation_config = genai.types.GenerationConfig(
            temperature=0.1,  # Th·∫•p ƒë·ªÉ factual
            top_p=0.8,
            top_k=20,
            max_output_tokens=1000,
        )
        
        response = model.generate_content(
            full_prompt,
            generation_config=generation_config
        )
        
        return response.text.strip()

    async def _call_deepseek(self, prompt, context, require_specific_data):
        """üí∞ DeepSeek V3 - Cost Effective"""
        
        headers = {
            'Authorization': f'Bearer {DEEPSEEK_API_KEY}',
            'Content-Type': 'application/json'
        }
        
        system_message = """B·∫°n l√† chuy√™n gia t√†i ch√≠nh. PH·∫¢I tu√¢n th·ªß nghi√™m ng·∫∑t:
1. S·ª≠ d·ª•ng ch√≠nh x√°c s·ªë li·ªáu t·ª´ tin t·ª©c ƒë∆∞·ª£c cung c·∫•p
2. N√™u th·ªùi gian c·ª• th·ªÉ  
3. Gi·∫£i th√≠ch l√Ω do d·ª±a tr√™n s·ª± ki·ªán th·ª±c t·∫ø
4. KH√îNG ƒë∆∞·ª£c n√≥i chung chung ho·∫∑c d√πng d·ªØ li·ªáu c≈©"""

        data = {
            'model': 'deepseek-v3',
            'messages': [
                {'role': 'system', 'content': system_message},
                {'role': 'user', 'content': f"TH√îNG TIN TIN T·ª®C:\n{context}\n\nC√ÇU H·ªéI: {prompt}"}
            ],
            'temperature': 0.1,
            'max_tokens': 1000
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post('https://api.deepseek.com/v1/chat/completions', 
                                  headers=headers, json=data) as response:
                result = await response.json()
                return result['choices'][0]['message']['content'].strip()

    async def _call_claude(self, prompt, context, require_specific_data):
        """üß† Claude 3.5 Sonnet - Reliable"""
        
        headers = {
            'x-api-key': ANTHROPIC_API_KEY,
            'Content-Type': 'application/json',
            'anthropic-version': '2023-06-01'
        }
        
        data = {
            'model': 'claude-3-5-sonnet-20241022',
            'max_tokens': 1000,
            'temperature': 0.1,
            'messages': [
                {
                    'role': 'user', 
                    'content': f"""B·∫°n l√† chuy√™n gia t√†i ch√≠nh. QUY T·∫ÆC B·∫ÆT BU·ªòC:
- S·ª≠ d·ª•ng s·ªë li·ªáu c·ª• th·ªÉ t·ª´ tin t·ª©c
- N√™u th·ªùi gian ch√≠nh x√°c  
- Gi·∫£i th√≠ch l√Ω do d·ª±a tr√™n facts
- Kh√¥ng n√≥i chung chung

TH√îNG TIN TIN T·ª®C:
{context}

C√ÇU H·ªéI: {prompt}"""
                }
            ]
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post('https://api.anthropic.com/v1/messages',
                                  headers=headers, json=data) as response:
                result = await response.json()
                return result['content'][0]['text'].strip()

    async def _call_groq(self, prompt, context, require_specific_data):
        """‚ö° Groq - Fast Fallback"""
        
        headers = {
            'Authorization': f'Bearer {GROQ_API_KEY}',
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': 'llama-3.3-70b-versatile',
            'messages': [
                {'role': 'system', 'content': 'B·∫°n l√† chuy√™n gia t√†i ch√≠nh. Ph·∫£i s·ª≠ d·ª•ng s·ªë li·ªáu c·ª• th·ªÉ t·ª´ tin t·ª©c v√† n√™u th·ªùi gian ch√≠nh x√°c. Kh√¥ng ƒë∆∞·ª£c n√≥i chung chung.'},
                {'role': 'user', 'content': f"TH√îNG TIN TIN T·ª®C:\n{context}\n\nC√ÇU H·ªéI: {prompt}"}
            ],
            'temperature': 0.1,
            'max_tokens': 1000
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post('https://api.groq.com/openai/v1/chat/completions',
                                  headers=headers, json=data) as response:
                result = await response.json()
                return result['choices'][0]['message']['content'].strip()

    def _validate_response(self, response, require_specific_data):
        """Validate AI response quality"""
        if not require_specific_data:
            return len(response.strip()) > 50
        
        # Check for specific data requirements
        has_numbers = re.search(r'\d+[.,]?\d*\s*%|\d+[.,]?\d*\s*(tri·ªáu|t·ª∑|USD|VND|ƒë·ªìng)', response)
        has_time = re.search(r'\d{1,2}[/\-\.]\d{1,2}[/\-\.]\d{4}|\d{1,2}\s*(th√°ng|thg)\s*\d{1,2}', response)
        
        # Check for forbidden generic terms
        forbidden_terms = ['th∆∞·ªùng', 'c√≥ th·ªÉ', 'n√≥i chung', 'th√¥ng th∆∞·ªùng', 'th·ªãnh n·ªôp']
        has_forbidden = any(term in response.lower() for term in forbidden_terms)
        
        if require_specific_data:
            return has_numbers and has_time and not has_forbidden
        
        return not has_forbidden and len(response.strip()) > 100

# Initialize AI Manager
ai_manager = AIEngineManager()

# üîç IMPROVED GOOGLE SEARCH v·ªõi Generic Query
async def search_reliable_sources_improved(query, max_results=5):
    """üÜï T√¨m ki·∫øm th√¥ng minh v·ªõi Generic Query + Time Context"""
    
    if not GOOGLE_API_KEY or not GOOGLE_CSE_ID:
        print("‚ö†Ô∏è Google Search API not configured")
        return []
    
    try:
        # Th√™m time context cho query
        current_date = datetime.now(VN_TIMEZONE).strftime("%Y")
        current_month = datetime.now(VN_TIMEZONE).strftime("%m/%Y")
        
        # Generic query v·ªõi time context - KH√îNG C·∫¶N specific keywords
        enhanced_query = f'{query} {current_date} m·ªõi nh·∫•t tin t·ª©c site:cafef.vn OR site:vneconomy.vn OR site:vnexpress.net OR site:tuoitre.vn OR site:thanhnien.vn OR site:baodautu.vn OR site:dantri.com.vn OR site:investing.com OR site:bloomberg.com OR site:reuters.com'
        
        print(f"üîç Enhanced search query: {enhanced_query}")
        
        from googleapiclient.discovery import build
        service = build("customsearch", "v1", developerKey=GOOGLE_API_KEY)
        
        result = service.cse().list(
            q=enhanced_query,
            cx=GOOGLE_CSE_ID,
            num=max_results,
            lr='lang_vi|lang_en',
            safe='active',
            sort='date'  # S·∫Øp x·∫øp theo ng√†y m·ªõi nh·∫•t
        ).execute()
        
        sources = []
        if 'items' in result:
            for item in result['items']:
                source = {
                    'title': item.get('title', ''),
                    'link': item.get('link', ''),
                    'snippet': item.get('snippet', ''),
                    'source_name': extract_source_name(item.get('link', '')),
                    'publishedDate': item.get('pagemap', {}).get('metatags', [{}])[0].get('article:published_time', '')
                }
                sources.append(source)
        
        print(f"‚úÖ Found {len(sources)} reliable sources")
        return sources
        
    except Exception as e:
        print(f"‚ùå Google Search error: {e}")
        return []

def extract_source_name(url):
    """Extract readable source name from URL"""
    domain_mapping = {
        'cafef.vn': 'CafeF',
        'vneconomy.vn': 'VnEconomy', 
        'vnexpress.net': 'VnExpress',
        'tuoitre.vn': 'Tu·ªïi Tr·∫ª',
        'thanhnien.vn': 'Thanh Ni√™n',
        'baodautu.vn': 'B√°o ƒê·∫ßu t∆∞',
        'dantri.com.vn': 'D√¢n tr√≠',
        'investing.com': 'Investing.com',
        'bloomberg.com': 'Bloomberg',
        'reuters.com': 'Reuters',
        'bbc.com': 'BBC'
    }
    
    for domain, name in domain_mapping.items():
        if domain in url:
            return name
    
    try:
        from urllib.parse import urlparse
        domain = urlparse(url).netloc.replace('www.', '')
        return domain.title()
    except:
        return 'Unknown Source'

# üÜï CONTENT EXTRACTION FUNCTIONS (FROM ORIGINAL CODE)
async def fetch_content_with_trafilatura(url):
    """üÜï TR√çCH XU·∫§T N·ªòI DUNG B·∫∞NG TRAFILATURA - T·ªêT NH·∫§T 2024"""
    try:
        if not TRAFILATURA_AVAILABLE:
            return None
        
        print(f"üöÄ S·ª≠ d·ª•ng Trafilatura cho: {url}")
        
        # T·∫£i n·ªôi dung
        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            return None
        
        # Tr√≠ch xu·∫•t v·ªõi metadata
        result = trafilatura.bare_extraction(
            downloaded,
            include_comments=False,
            include_tables=True,
            include_links=False,
            with_metadata=True
        )
        
        if result and result.get('text'):
            content = result['text']
            
            # Gi·ªõi h·∫°n ƒë·ªô d√†i v√† l√†m s·∫°ch
            if len(content) > 2000:
                content = content[:2000] + "..."
            
            return content.strip()
        
        return None
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói Trafilatura cho {url}: {e}")
        return None

async def fetch_content_with_newspaper(url):
    """üì∞ TR√çCH XU·∫§T B·∫∞NG NEWSPAPER3K - FALLBACK"""
    try:
        if not NEWSPAPER_AVAILABLE:
            return None
        
        print(f"üì∞ S·ª≠ d·ª•ng Newspaper3k cho: {url}")
        
        # T·∫°o article object
        article = Article(url)
        article.download()
        article.parse()
        
        if article.text:
            content = article.text
            
            # Gi·ªõi h·∫°n ƒë·ªô d√†i
            if len(content) > 2000:
                content = content[:2000] + "..."
            
            return content.strip()
        
        return None
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói Newspaper3k cho {url}: {e}")
        return None

async def fetch_content_legacy(url):
    """üîÑ PH∆Ø∆†NG PH√ÅP C≈® - CU·ªêI C√ôNG FALLBACK"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        response = requests.get(url, headers=headers, timeout=8, stream=True)
        response.raise_for_status()
        
        # X·ª≠ l√Ω encoding
        raw_content = response.content
        detected = chardet.detect(raw_content)
        encoding = detected['encoding'] or 'utf-8'
        
        try:
            content = raw_content.decode(encoding)
        except:
            content = raw_content.decode('utf-8', errors='ignore')
        
        # Lo·∫°i b·ªè HTML tags c∆° b·∫£n
        clean_content = re.sub(r'<script[^>]*>.*?</script>', '', content, flags=re.DOTALL | re.IGNORECASE)
        clean_content = re.sub(r'<style[^>]*>.*?</style>', '', clean_content, flags=re.DOTALL | re.IGNORECASE)
        clean_content = re.sub(r'<[^>]+>', ' ', clean_content)
        clean_content = html.unescape(clean_content)
        clean_content = re.sub(r'\s+', ' ', clean_content).strip()
        
        # L·∫•y ph·∫ßn ƒë·∫ßu c√≥ √Ω nghƒ©a
        sentences = clean_content.split('. ')
        meaningful_content = []
        
        for sentence in sentences[:8]:
            if len(sentence.strip()) > 20:
                meaningful_content.append(sentence.strip())
                
        result = '. '.join(meaningful_content)
        
        if len(result) > 1800:
            result = result[:1800] + "..."
            
        return result if result else "Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ b√†i vi·∫øt n√†y."
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói legacy extraction t·ª´ {url}: {e}")
        return f"Kh√¥ng th·ªÉ l·∫•y n·ªôi dung chi ti·∫øt. L·ªói: {str(e)}"

async def fetch_full_content_improved(url):
    """üÜï TR√çCH XU·∫§T N·ªòI DUNG C·∫¢I TI·∫æN - S·ª¨ D·ª§NG 3 PH∆Ø∆†NG PH√ÅP"""
    # Th·ª≠ ph∆∞∆°ng ph√°p 1: Trafilatura (t·ªët nh·∫•t)
    content = await fetch_content_with_trafilatura(url)
    if content and len(content) > 50:
        print("‚úÖ Th√†nh c√¥ng v·ªõi Trafilatura")
        return content
    
    # Th·ª≠ ph∆∞∆°ng ph√°p 2: Newspaper3k (fallback)
    content = await fetch_content_with_newspaper(url)
    if content and len(content) > 50:
        print("‚úÖ Th√†nh c√¥ng v·ªõi Newspaper3k")
        return content
    
    # Ph∆∞∆°ng ph√°p 3: Legacy method (cu·ªëi c√πng)
    content = await fetch_content_legacy(url)
    print("‚ö†Ô∏è S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p legacy")
    return content

# üÜï IMPROVED CONTENT EXTRACTION
async def get_full_content_from_sources_improved(sources):
    """L·∫•y n·ªôi dung ƒë·∫ßy ƒë·ªß v·ªõi fallback strategy"""
    
    full_contexts = []
    
    for i, source in enumerate(sources[:3], 1):  # Top 3 sources
        try:
            print(f"üìÑ Extracting content from source {i}: {source['source_name']}")
            
            # Try multiple extraction methods
            content = await fetch_full_content_improved(source['link'])
            
            if content and len(content) > 200:
                # L·∫•y 800 k√Ω t·ª± ƒë·∫ßu - ch·ª©a info quan tr·ªçng nh·∫•t
                summary_content = content[:800]
                
                full_contexts.append(f"""
üì∞ NGU·ªíN {i}: {source['source_name']}
üìÖ Th·ªùi gian: {source.get('publishedDate', 'Kh√¥ng x√°c ƒë·ªãnh')}
üîó Link: {source['link']}
üìÑ N·ªôi dung: {summary_content}
""")
            else:
                # Fallback to snippet
                full_contexts.append(f"""
üì∞ NGU·ªíN {i}: {source['source_name']} 
üìÑ T√≥m t·∫Øt: {source['snippet']}
üîó Link: {source['link']}
""")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Content extraction failed for {source['source_name']}: {e}")
            # Fallback to snippet
            full_contexts.append(f"""
üì∞ NGU·ªíN {i}: {source['source_name']}
üìÑ T√≥m t·∫Øt: {source['snippet']}
üîó Link: {source['link']}
""")
    
    return "\n".join(full_contexts)

# RSS COLLECTION FUNCTIONS (FROM ORIGINAL CODE)
async def collect_news_from_sources(sources_dict, limit_per_source=8):
    """Thu th·∫≠p tin t·ª©c v·ªõi x·ª≠ l√Ω m√∫i gi·ªù ch√≠nh x√°c"""
    all_news = []
    
    for source_name, rss_url in sources_dict.items():
        try:
            print(f"üîÑ ƒêang l·∫•y tin t·ª´ {source_name}...")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/rss+xml, application/xml, text/xml',
                'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8'
            }
            
            try:
                response = requests.get(rss_url, headers=headers, timeout=10)
                response.raise_for_status()
                feed = feedparser.parse(response.content)
            except Exception as req_error:
                print(f"‚ö†Ô∏è L·ªói request t·ª´ {source_name}: {req_error}")
                feed = feedparser.parse(rss_url)
            
            if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                print(f"‚ö†Ô∏è Kh√¥ng c√≥ tin t·ª´ {source_name}")
                continue
                
            entries_processed = 0
            for entry in feed.entries[:limit_per_source]:
                try:
                    # üîß X·ª¨ L√ù TH·ªúI GIAN CH√çNH X√ÅC
                    vn_time = datetime.now(VN_TIMEZONE)  # Default fallback
                    
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        vn_time = convert_utc_to_vietnam_time(entry.published_parsed)
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        vn_time = convert_utc_to_vietnam_time(entry.updated_parsed)
                    
                    # L·∫•y m√¥ t·∫£
                    description = ""
                    if hasattr(entry, 'summary'):
                        description = entry.summary[:500] + "..." if len(entry.summary) > 500 else entry.summary
                    elif hasattr(entry, 'description'):
                        description = entry.description[:500] + "..." if len(entry.description) > 500 else entry.description
                    
                    if not hasattr(entry, 'title') or not hasattr(entry, 'link'):
                        continue
                    
                    title = html.unescape(entry.title.strip())
                    
                    news_item = {
                        'title': title,
                        'link': entry.link,
                        'source': source_name,
                        'published': vn_time,
                        'published_str': vn_time.strftime("%H:%M %d/%m"),
                        'description': html.unescape(description) if description else ""
                    }
                    all_news.append(news_item)
                    entries_processed += 1
                    
                except Exception as entry_error:
                    print(f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω tin t·ª´ {source_name}: {entry_error}")
                    continue
                    
            print(f"‚úÖ L·∫•y ƒë∆∞·ª£c {entries_processed} tin t·ª´ {source_name}")
            
        except Exception as e:
            print(f"‚ùå L·ªói khi l·∫•y tin t·ª´ {source_name}: {e}")
            continue
    
    print(f"üìä T·ªïng c·ªông l·∫•y ƒë∆∞·ª£c {len(all_news)} tin t·ª´ t·∫•t c·∫£ ngu·ªìn")
    
    # Lo·∫°i b·ªè tin tr√πng l·∫∑p
    unique_news = remove_duplicate_news(all_news)
    print(f"üîÑ Sau khi lo·∫°i tr√πng c√≤n {len(unique_news)} tin")
    
    # S·∫Øp x·∫øp theo th·ªùi gian m·ªõi nh·∫•t
    unique_news.sort(key=lambda x: x['published'], reverse=True)
    return unique_news

def remove_duplicate_news(news_list):
    """Lo·∫°i b·ªè tin t·ª©c tr√πng l·∫∑p"""
    seen_links = set()
    seen_titles = set()
    unique_news = []
    
    for news in news_list:
        normalized_title = normalize_title(news['title'])
        
        is_duplicate = False
        
        if news['link'] in seen_links:
            is_duplicate = True
        else:
            for existing_title in seen_titles:
                similarity = calculate_title_similarity(normalized_title, existing_title)
                if similarity > 0.75:
                    is_duplicate = True
                    break
        
        if not is_duplicate:
            seen_links.add(news['link'])
            seen_titles.add(normalized_title)
            unique_news.append(news)
    
    return unique_news

def calculate_title_similarity(title1, title2):
    """T√≠nh ƒë·ªô t∆∞∆°ng t·ª± gi·ªØa 2 ti√™u ƒë·ªÅ"""
    words1 = set(title1.split())
    words2 = set(title2.split())
    
    if not words1 or not words2:
        return 0
    
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    return len(intersection) / len(union) if union else 0

def normalize_title(title):
    """Chu·∫©n h√≥a ti√™u ƒë·ªÅ ƒë·ªÉ so s√°nh tr√πng l·∫∑p"""
    import re
    title = title.lower()
    title = re.sub(r'[^\w\s]', '', title)
    title = ' '.join(title.split())
    
    words = title.split()[:10]
    return ' '.join(words)

def save_user_news(user_id, news_list, command_type):
    """L∆∞u tin t·ª©c c·ªßa user ƒë·ªÉ s·ª≠ d·ª•ng cho l·ªánh !detail"""
    user_news_cache[user_id] = {
        'news': news_list,
        'command': command_type,
        'timestamp': datetime.now(VN_TIMEZONE)
    }

# BOT EVENT HANDLERS
@bot.event
async def on_ready():
    print(f'‚úÖ {bot.user} ƒë√£ online!')
    print(f'üìä K·∫øt n·ªëi v·ªõi {len(bot.guilds)} server(s)')
    
    # AI Engine status
    if ai_manager.primary_ai:
        print(f'ü§ñ Primary AI: {ai_manager.primary_ai.value.upper()}')
        print(f'üõ°Ô∏è Fallback AIs: {[ai.value.upper() for ai in ai_manager.fallback_ais]}')
    else:
        print('‚ö†Ô∏è No AI engines configured')
    
    total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
    print(f'üì∞ S·∫µn s√†ng cung c·∫•p tin t·ª´ {total_sources} ngu·ªìn ƒê√É KI·ªÇM TRA')
    print(f'üáªüá≥ Trong n∆∞·ªõc: {len(RSS_FEEDS["domestic"])} ngu·ªìn')
    print(f'üåç Qu·ªëc t·∫ø: {len(RSS_FEEDS["international"])} ngu·ªìn')
    print('üéØ Lƒ©nh v·ª±c: Kinh t·∫ø, Ch·ª©ng kho√°n, Vƒ© m√¥, B·∫•t ƒë·ªông s·∫£n')
    
    # Ki·ªÉm tra th∆∞ vi·ªán ƒë√£ c√†i ƒë·∫∑t
    if TRAFILATURA_AVAILABLE:
        print('üöÄ Trafilatura: Tr√≠ch xu·∫•t n·ªôi dung c·∫£i ti·∫øn (94.5% ƒë·ªô ch√≠nh x√°c)')
    if NEWSPAPER_AVAILABLE:
        print('üì∞ Newspaper3k: Fallback extraction cho tin t·ª©c')
    
    print('üéØ G√µ !menu ƒë·ªÉ xem h∆∞·ªõng d·∫´n')
    
    # Set bot status
    status_text = f"Multi-AI Engine ‚Ä¢ {ai_manager.primary_ai.value.upper() if ai_manager.primary_ai else 'No AI'} ‚Ä¢ !menu"
    await bot.change_presence(
        activity=discord.Activity(
            type=discord.ActivityType.watching,
            name=status_text
        )
    )

# DISCORD COMMANDS (FROM ORIGINAL CODE)
@bot.command(name='all')
async def get_all_news(ctx, page=1):
    """L·∫•y tin t·ª©c t·ª´ t·∫•t c·∫£ ngu·ªìn v·ªõi m√∫i gi·ªù ch√≠nh x√°c"""
    try:
        page = max(1, int(page))
        
        loading_msg = await ctx.send("‚è≥ ƒêang t·∫£i tin t·ª©c t·ª´ t·∫•t c·∫£ ngu·ªìn...")
        
        domestic_news = await collect_news_from_sources(RSS_FEEDS['domestic'], 8)
        international_news = await collect_news_from_sources(RSS_FEEDS['international'], 6)
        
        await loading_msg.delete()
        
        all_news = domestic_news + international_news
        all_news.sort(key=lambda x: x['published'], reverse=True)
        
        # Ph√¢n trang
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = all_news[start_index:end_index]
        
        if not page_news:
            total_pages = (len(all_news) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        # T·∫°o embed v·ªõi th√¥ng tin m√∫i gi·ªù
        embed = discord.Embed(
            title=f"üì∞ Tin t·ª©c kinh t·∫ø t·ªïng h·ª£p (Trang {page})",
            description=f"üï∞Ô∏è Gi·ªù Vi·ªát Nam ch√≠nh x√°c ‚Ä¢ üöÄ Multi-AI Engine ‚Ä¢ üì∞ T·ª´ {len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])} ngu·ªìn",
            color=0x00ff88,
            timestamp=ctx.message.created_at
        )
        
        # Emoji map
        emoji_map = {
            'cafef_main': '‚òï', 'cafef_chungkhoan': 'üìà', 'cafef_batdongsan': 'üè¢', 'cafef_taichinh': 'üí∞', 'cafef_vimo': 'üìä',
            'cafebiz_main': 'üíº', 'baodautu_main': 'üéØ', 'vneconomy_main': 'üì∞', 'vneconomy_chungkhoan': 'üìà',
            'vnexpress_kinhdoanh': '‚ö°', 'vnexpress_chungkhoan': 'üìà', 'thanhnien_kinhtevimo': 'üìä', 'thanhnien_chungkhoan': 'üìà',
            'nhandanonline_tc': 'üèõÔ∏è', 'yahoo_finance': 'üí∞', 'reuters_business': 'üåç', 'bloomberg_markets': 'üíπ', 
            'marketwatch_latest': 'üìà', 'forbes_money': 'üíé', 'financial_times': 'üíº', 'business_insider': 'üì∞', 'the_economist': 'üéì'
        }
        
        # Th·ªëng k√™
        domestic_count = sum(1 for news in page_news if news['source'] in RSS_FEEDS['domestic'])
        international_count = len(page_news) - domestic_count
        
        embed.add_field(
            name="üìä Th·ªëng k√™ trang n√†y",
            value=f"üáªüá≥ Trong n∆∞·ªõc: {domestic_count} tin\nüåç Qu·ªëc t·∫ø: {international_count} tin\nüìä T·ªïng c√≥ s·∫µn: {len(all_news)} tin",
            inline=False
        )
        
        # Hi·ªÉn th·ªã tin t·ª©c v·ªõi th·ªùi gian ch√≠nh x√°c
        source_names = {
            'cafef_main': 'CafeF', 'cafef_chungkhoan': 'CafeF CK', 'cafef_batdongsan': 'CafeF BƒêS',
            'cafef_taichinh': 'CafeF TC', 'cafef_vimo': 'CafeF VM', 'cafebiz_main': 'CafeBiz',
            'baodautu_main': 'B√°o ƒê·∫ßu t∆∞', 'vneconomy_main': 'VnEconomy', 'vneconomy_chungkhoan': 'VnEconomy CK',
            'vnexpress_kinhdoanh': 'VnExpress KD', 'vnexpress_chungkhoan': 'VnExpress CK',
            'thanhnien_kinhtevimo': 'Thanh Ni√™n VM', 'thanhnien_chungkhoan': 'Thanh Ni√™n CK',
            'nhandanonline_tc': 'Nh√¢n D√¢n TC', 'yahoo_finance': 'Yahoo Finance', 'reuters_business': 'Reuters',
            'bloomberg_markets': 'Bloomberg', 'marketwatch_latest': 'MarketWatch', 'forbes_money': 'Forbes',
            'financial_times': 'Financial Times', 'business_insider': 'Business Insider', 'the_economist': 'The Economist'
        }
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'üì∞')
            title = news['title'][:70] + "..." if len(news['title']) > 70 else news['title']
            source_display = source_names.get(news['source'], news['source'])
            
            embed.add_field(
                name=f"{i}. {emoji} {title}",
                value=f"üï∞Ô∏è {news['published_str']} (VN) ‚Ä¢ üì∞ {source_display}\nüîó [ƒê·ªçc b√†i vi·∫øt]({news['link']})",
                inline=False
            )
        
        save_user_news(ctx.author.id, page_news, f"all_page_{page}")
        
        total_pages = (len(all_news) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"üöÄ Multi-AI Engine ‚Ä¢ Trang {page}/{total_pages} ‚Ä¢ !all {page+1} ti·∫øp ‚Ä¢ !chitiet [s·ªë] xem chi ti·∫øt")
        
        await ctx.send(embed=embed)
        
    except ValueError:
        await ctx.send("‚ùå S·ªë trang kh√¥ng h·ª£p l·ªá! S·ª≠ d·ª•ng: `!all [s·ªë]`")
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='in')
async def get_domestic_news(ctx, page=1):
    """L·∫•y tin t·ª©c t·ª´ c√°c ngu·ªìn trong n∆∞·ªõc v·ªõi m√∫i gi·ªù ch√≠nh x√°c"""
    try:
        page = max(1, int(page))
        
        loading_msg = await ctx.send("‚è≥ ƒêang t·∫£i tin t·ª©c trong n∆∞·ªõc...")
        
        news_list = await collect_news_from_sources(RSS_FEEDS['domestic'], 10)
        
        await loading_msg.delete()
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        embed = discord.Embed(
            title=f"üáªüá≥ Tin kinh t·∫ø trong n∆∞·ªõc (Trang {page})",
            description=f"üï∞Ô∏è Gi·ªù Vi·ªát Nam ch√≠nh x√°c ‚Ä¢ üöÄ Multi-AI Engine ‚Ä¢ T·ª´ {len(RSS_FEEDS['domestic'])} ngu·ªìn chuy√™n ng√†nh",
            color=0xff0000,
            timestamp=ctx.message.created_at
        )
        
        embed.add_field(
            name="üìä Th√¥ng tin",
            value=f"üì∞ T·ªïng tin c√≥ s·∫µn: {len(news_list)} tin\nüéØ Lƒ©nh v·ª±c: Kinh t·∫ø, Ch·ª©ng kho√°n, B·∫•t ƒë·ªông s·∫£n, Vƒ© m√¥",
            inline=False
        )
        
        # Hi·ªÉn th·ªã tin t·ª©c trong n∆∞·ªõc
        emoji_map = {
            'cafef_main': '‚òï', 'cafef_chungkhoan': 'üìà', 'cafef_batdongsan': 'üè¢', 'cafef_taichinh': 'üí∞', 'cafef_vimo': 'üìä',
            'cafebiz_main': 'üíº', 'baodautu_main': 'üéØ', 'vneconomy_main': 'üì∞', 'vneconomy_chungkhoan': 'üìà',
            'vnexpress_kinhdoanh': '‚ö°', 'vnexpress_chungkhoan': 'üìà', 'thanhnien_kinhtevimo': 'üìä', 'thanhnien_chungkhoan': 'üìà',
            'nhandanonline_tc': 'üèõÔ∏è'
        }
        
        source_names = {
            'cafef_main': 'CafeF', 'cafef_chungkhoan': 'CafeF CK', 'cafef_batdongsan': 'CafeF BƒêS',
            'cafef_taichinh': 'CafeF TC', 'cafef_vimo': 'CafeF VM', 'cafebiz_main': 'CafeBiz',
            'baodautu_main': 'B√°o ƒê·∫ßu t∆∞', 'vneconomy_main': 'VnEconomy', 'vneconomy_chungkhoan': 'VnEconomy CK',
            'vnexpress_kinhdoanh': 'VnExpress KD', 'vnexpress_chungkhoan': 'VnExpress CK',
            'thanhnien_kinhtevimo': 'Thanh Ni√™n VM', 'thanhnien_chungkhoan': 'Thanh Ni√™n CK',
            'nhandanonline_tc': 'Nh√¢n D√¢n TC'
        }
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'üì∞')
            title = news['title'][:70] + "..." if len(news['title']) > 70 else news['title']
            source_display = source_names.get(news['source'], news['source'])
            
            embed.add_field(
                name=f"{i}. {emoji} {title}",
                value=f"üï∞Ô∏è {news['published_str']} (VN) ‚Ä¢ üì∞ {source_display}\nüîó [ƒê·ªçc b√†i vi·∫øt]({news['link']})",
                inline=False
            )
        
        save_user_news(ctx.author.id, page_news, f"in_page_{page}")
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"üöÄ Multi-AI Engine ‚Ä¢ Trang {page}/{total_pages} ‚Ä¢ !in {page+1} ti·∫øp ‚Ä¢ !chitiet [s·ªë] xem chi ti·∫øt")
        
        await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='out')
async def get_international_news(ctx, page=1):
    """L·∫•y tin t·ª©c t·ª´ c√°c ngu·ªìn qu·ªëc t·∫ø v·ªõi m√∫i gi·ªù ch√≠nh x√°c"""
    try:
        page = max(1, int(page))
        
        loading_msg = await ctx.send("‚è≥ ƒêang t·∫£i tin t·ª©c qu·ªëc t·∫ø...")
        
        news_list = await collect_news_from_sources(RSS_FEEDS['international'], 8)
        
        await loading_msg.delete()
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        embed = discord.Embed(
            title=f"üåç Tin kinh t·∫ø qu·ªëc t·∫ø (Trang {page})",
            description=f"üï∞Ô∏è Gi·ªù Vi·ªát Nam ch√≠nh x√°c ‚Ä¢ üöÄ Multi-AI Engine ‚Ä¢ T·ª´ {len(RSS_FEEDS['international'])} ngu·ªìn h√†ng ƒë·∫ßu",
            color=0x0066ff,
            timestamp=ctx.message.created_at
        )
        
        embed.add_field(
            name="üìä Th√¥ng tin",
            value=f"üì∞ T·ªïng tin c√≥ s·∫µn: {len(news_list)} tin",
            inline=False
        )
        
        emoji_map = {
            'yahoo_finance': 'üí∞', 'reuters_business': 'üåç', 'bloomberg_markets': 'üíπ', 'marketwatch_latest': 'üìà',
            'forbes_money': 'üíé', 'financial_times': 'üíº', 'business_insider': 'üì∞', 'the_economist': 'üéì'
        }
        
        source_names = {
            'yahoo_finance': 'Yahoo Finance', 'reuters_business': 'Reuters', 'bloomberg_markets': 'Bloomberg', 
            'marketwatch_latest': 'MarketWatch', 'forbes_money': 'Forbes', 'financial_times': 'Financial Times', 
            'business_insider': 'Business Insider', 'the_economist': 'The Economist'
        }
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'üåç')
            title = news['title'][:70] + "..." if len(news['title']) > 70 else news['title']
            source_display = source_names.get(news['source'], news['source'])
            
            embed.add_field(
                name=f"{i}. {emoji} {title}",
                value=f"üï∞Ô∏è {news['published_str']} (VN) ‚Ä¢ üì∞ {source_display}\nüîó [ƒê·ªçc b√†i vi·∫øt]({news['link']})",
                inline=False
            )
        
        save_user_news(ctx.author.id, page_news, f"out_page_{page}")
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"üöÄ Multi-AI Engine ‚Ä¢ Trang {page}/{total_pages} ‚Ä¢ !out {page+1} ti·∫øp ‚Ä¢ !chitiet [s·ªë] xem chi ti·∫øt")
        
        await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='chitiet')
async def get_news_detail(ctx, news_number: int):
    """üÜï XEM CHI TI·∫æT B·∫∞NG MULTI-AI ENGINE + T·ª∞ ƒê·ªòNG D·ªäCH"""
    try:
        user_id = ctx.author.id
        
        if user_id not in user_news_cache:
            await ctx.send("‚ùå B·∫°n ch∆∞a xem tin t·ª©c n√†o! H√£y d√πng `!all`, `!in`, ho·∫∑c `!out` tr∆∞·ªõc.")
            return
        
        user_data = user_news_cache[user_id]
        news_list = user_data['news']
        
        if news_number < 1 or news_number > len(news_list):
            await ctx.send(f"‚ùå S·ªë kh√¥ng h·ª£p l·ªá! Ch·ªçn t·ª´ 1 ƒë·∫øn {len(news_list)}")
            return
        
        news = news_list[news_number - 1]
        
        # Th√¥ng b√°o ƒëang t·∫£i v·ªõi th√¥ng tin c√¥ng ngh·ªá
        loading_msg = await ctx.send("üöÄ ƒêang tr√≠ch xu·∫•t n·ªôi dung v·ªõi Multi-AI Engine...")
        
        # S·ª≠ d·ª•ng function c·∫£i ti·∫øn
        full_content = await fetch_full_content_improved(news['link'])
        
        # üåê T√çNH NƒÇNG M·ªöI: T·ª± ƒë·ªông d·ªãch n·∫øu l√† tin n∆∞·ªõc ngo√†i
        international_sources = {
            'yahoo_finance', 'reuters_business', 'bloomberg_markets', 'marketwatch_latest',
            'forbes_money', 'financial_times', 'business_insider', 'the_economist'
        }
        
        translated_content = full_content
        is_translated = False
        
        if news['source'] in international_sources and ai_manager.primary_ai:
            try:
                # Detect English content
                english_indicators = ['the', 'and', 'is', 'are', 'was', 'were', 'have', 'has']
                content_lower = full_content.lower()
                english_word_count = sum(1 for word in english_indicators if word in content_lower)
                
                if english_word_count >= 3:
                    print(f"üåê ƒêang d·ªãch n·ªôi dung t·ª´ {news['source']} sang ti·∫øng Vi·ªát...")
                    
                    translation_prompt = f"""D·ªãch ƒëo·∫°n vƒÉn ti·∫øng Anh sau sang ti·∫øng Vi·ªát ch√≠nh x√°c, t·ª± nhi√™n:

{full_content}

Y√™u c·∫ßu:
- Gi·ªØ nguy√™n s·ªë li·ªáu, t·ª∑ l·ªá ph·∫ßn trƒÉm
- D·ªãch t·ª± nhi√™n, kh√¥ng m√°y m√≥c
- S·ª≠ d·ª•ng thu·∫≠t ng·ªØ kinh t·∫ø ti·∫øng Vi·ªát chu·∫©n

B·∫£n d·ªãch:"""

                    translated_content, used_engine = await ai_manager.call_ai_with_fallback(
                        prompt=translation_prompt,
                        context="",
                        require_specific_data=False
                    )
                    
                    if translated_content and "‚ùå" not in translated_content:
                        is_translated = True
                        print("‚úÖ D·ªãch thu·∫≠t th√†nh c√¥ng")
                    else:
                        translated_content = full_content
                        
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói d·ªãch thu·∫≠t: {e}")
                translated_content = full_content
        
        await loading_msg.delete()
        
        # T·∫°o embed ƒë·∫πp h∆°n
        embed = discord.Embed(
            title="üìñ Chi ti·∫øt b√†i vi·∫øt",
            color=0x9932cc,
            timestamp=ctx.message.created_at
        )
        
        # Emoji cho ngu·ªìn
        emoji_map = {
            'cafef_main': '‚òï', 'cafef_chungkhoan': 'üìà', 'cafef_batdongsan': 'üè¢', 'cafef_taichinh': 'üí∞', 'cafef_vimo': 'üìä',
            'cafebiz_main': 'üíº', 'baodautu_main': 'üéØ', 'vneconomy_main': 'üì∞', 'vneconomy_chungkhoan': 'üìà',
            'vnexpress_kinhdoanh': '‚ö°', 'vnexpress_chungkhoan': 'üìà', 'thanhnien_kinhtevimo': 'üìä', 'thanhnien_chungkhoan': 'üìà',
            'nhandanonline_tc': 'üèõÔ∏è', 'yahoo_finance': 'üí∞', 'reuters_business': 'üåç', 'bloomberg_markets': 'üíπ', 
            'marketwatch_latest': 'üìà', 'forbes_money': 'üíé', 'financial_times': 'üíº', 'business_insider': 'üì∞', 'the_economist': 'üéì'
        }
        
        source_names = {
            'cafef_main': 'CafeF', 'cafef_chungkhoan': 'CafeF Ch·ª©ng kho√°n', 'cafef_batdongsan': 'CafeF B·∫•t ƒë·ªông s·∫£n',
            'cafef_taichinh': 'CafeF T√†i ch√≠nh', 'cafef_vimo': 'CafeF Vƒ© m√¥', 'cafebiz_main': 'CafeBiz',
            'baodautu_main': 'B√°o ƒê·∫ßu t∆∞', 'vneconomy_main': 'VnEconomy', 'vneconomy_chungkhoan': 'VnEconomy Ch·ª©ng kho√°n',
            'vnexpress_kinhdoanh': 'VnExpress Kinh doanh', 'vnexpress_chungkhoan': 'VnExpress Ch·ª©ng kho√°n',
            'thanhnien_kinhtevimo': 'Thanh Ni√™n Vƒ© m√¥', 'thanhnien_chungkhoan': 'Thanh Ni√™n Ch·ª©ng kho√°n',
            'nhandanonline_tc': 'Nh√¢n D√¢n T√†i ch√≠nh', 'yahoo_finance': 'Yahoo Finance', 'reuters_business': 'Reuters Business',
            'bloomberg_markets': 'Bloomberg Markets', 'marketwatch_latest': 'MarketWatch', 'forbes_money': 'Forbes Money',
            'financial_times': 'Financial Times', 'business_insider': 'Business Insider', 'the_economist': 'The Economist'
        }
        
        emoji = emoji_map.get(news['source'], 'üì∞')
        source_display = source_names.get(news['source'], news['source'])
        
        # Th√™m indicator d·ªãch thu·∫≠t v√†o ti√™u ƒë·ªÅ
        title_suffix = " üåê (ƒê√£ d·ªãch)" if is_translated else ""
        embed.add_field(
            name=f"{emoji} Ti√™u ƒë·ªÅ{title_suffix}",
            value=news['title'],
            inline=False
        )
        
        embed.add_field(
            name="üï∞Ô∏è Th·ªùi gian (VN)",
            value=news['published_str'],
            inline=True
        )
        
        embed.add_field(
            name="üì∞ Ngu·ªìn",
            value=source_display + (" üåê" if is_translated else ""),
            inline=True
        )
        
        # S·ª≠ d·ª•ng n·ªôi dung ƒë√£ d·ªãch (n·∫øu c√≥)
        content_to_display = translated_content
        
        # Hi·ªÉn th·ªã n·ªôi dung ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        if len(content_to_display) > 1000:
            # Chia n·ªôi dung th√†nh 2 ph·∫ßn
            content_title = "üìÑ N·ªôi dung chi ti·∫øt üåê (ƒê√£ d·ªãch sang ti·∫øng Vi·ªát)" if is_translated else "üìÑ N·ªôi dung chi ti·∫øt"
            
            embed.add_field(
                name=f"{content_title} (Ph·∫ßn 1)",
                value=content_to_display[:1000] + "...",
                inline=False
            )
            
            await ctx.send(embed=embed)
            
            # T·∫°o embed th·ª© 2
            embed2 = discord.Embed(
                title=f"üìñ Chi ti·∫øt b√†i vi·∫øt (ti·∫øp theo){'üåê' if is_translated else ''}",
                color=0x9932cc
            )
            
            embed2.add_field(
                name=f"{content_title} (Ph·∫ßn 2)",
                value=content_to_display[1000:2000],
                inline=False
            )
            
            # Th√™m th√¥ng tin v·ªÅ b·∫£n g·ªëc n·∫øu ƒë√£ d·ªãch
            if is_translated:
                embed2.add_field(
                    name="üîÑ Th√¥ng tin d·ªãch thu·∫≠t",
                    value="üìù N·ªôi dung g·ªëc b·∫±ng ti·∫øng Anh ƒë√£ ƒë∆∞·ª£c d·ªãch sang ti·∫øng Vi·ªát b·∫±ng Multi-AI Engine\nüí° ƒê·ªÉ xem b·∫£n g·ªëc, vui l√≤ng truy c·∫≠p link b√†i vi·∫øt",
                    inline=False
                )
            
            embed2.add_field(
                name="üîó ƒê·ªçc b√†i vi·∫øt ƒë·∫ßy ƒë·ªß",
                value=f"[Nh·∫•n ƒë·ªÉ ƒë·ªçc to√†n b·ªô b√†i vi·∫øt g·ªëc]({news['link']})",
                inline=False
            )
            
            # Th√¥ng tin c√¥ng ngh·ªá s·ª≠ d·ª•ng
            tech_info = "üöÄ Multi-AI Engine"
            if TRAFILATURA_AVAILABLE:
                tech_info += " + Trafilatura"
            if NEWSPAPER_AVAILABLE:
                tech_info += " + Newspaper3k"
            if is_translated:
                tech_info += " + AI Translation"
            
            embed2.set_footer(text=f"{tech_info} ‚Ä¢ T·ª´ l·ªánh: {user_data['command']} ‚Ä¢ Tin s·ªë {news_number}")
            
            await ctx.send(embed=embed2)
            return
        else:
            content_title = "üìÑ N·ªôi dung chi ti·∫øt üåê (ƒê√£ d·ªãch sang ti·∫øng Vi·ªát)" if is_translated else "üìÑ N·ªôi dung chi ti·∫øt"
            embed.add_field(
                name=content_title,
                value=content_to_display,
                inline=False
            )
        
        # Th√™m th√¥ng tin v·ªÅ d·ªãch thu·∫≠t n·∫øu c√≥
        if is_translated:
            embed.add_field(
                name="üîÑ Th√¥ng tin d·ªãch thu·∫≠t",
                value="üìù B√†i vi·∫øt g·ªëc b·∫±ng ti·∫øng Anh ƒë√£ ƒë∆∞·ª£c d·ªãch sang ti·∫øng Vi·ªát b·∫±ng Multi-AI Engine",
                inline=False
            )
        
        embed.add_field(
            name="üîó ƒê·ªçc b√†i vi·∫øt ƒë·∫ßy ƒë·ªß",
            value=f"[Nh·∫•n ƒë·ªÉ ƒë·ªçc to√†n b·ªô b√†i vi·∫øt{'g·ªëc' if is_translated else ''}]({news['link']})",
            inline=False
        )
        
        # Th√¥ng tin c√¥ng ngh·ªá s·ª≠ d·ª•ng
        tech_info = "üöÄ Multi-AI Engine"
        if TRAFILATURA_AVAILABLE:
            tech_info += " + Trafilatura"
        if NEWSPAPER_AVAILABLE:
            tech_info += " + Newspaper3k"
        if is_translated:
            tech_info += " + AI Translation"
        
        embed.set_footer(text=f"{tech_info} ‚Ä¢ T·ª´ l·ªánh: {user_data['command']} ‚Ä¢ Tin s·ªë {news_number} ‚Ä¢ !menu ƒë·ªÉ xem th√™m l·ªánh")
        
        await ctx.send(embed=embed)
        
    except ValueError:
        await ctx.send("‚ùå Vui l√≤ng nh·∫≠p s·ªë! V√≠ d·ª•: `!chitiet 5`")
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

# Alias cho l·ªánh chitiet
@bot.command(name='cuthe')
async def get_news_detail_alias(ctx, news_number: int):
    """Alias cho l·ªánh !chitiet"""
    await get_news_detail(ctx, news_number)

# üÜï MAIN AI COMMAND - Completely Rewritten
@bot.command(name='hoi')
async def ask_economic_question_improved(ctx, *, question):
    """üÜï AI Q&A v·ªõi Multi-Engine Support v√† Validation"""
    
    try:
        if not ai_manager.primary_ai:
            embed = discord.Embed(
                title="‚ö†Ô∏è AI Services kh√¥ng kh·∫£ d·ª•ng",
                description="Ch∆∞a c·∫•u h√¨nh AI API keys. C·∫ßn √≠t nh·∫•t m·ªôt trong: GEMINI_API_KEY, DEEPSEEK_API_KEY, ANTHROPIC_API_KEY, GROQ_API_KEY",
                color=0xff6b6b
            )
            await ctx.send(embed=embed)
            return
        
        # Th√¥ng b√°o ƒëang x·ª≠ l√Ω
        processing_msg = await ctx.send("üîç ƒêang t√¨m ki·∫øm th√¥ng tin t·ª´ c√°c ngu·ªìn tin ƒë√°ng tin c·∫≠y...")
        
        # üîç Step 1: Generic Google Search (No specific keywords needed)
        sources = await search_reliable_sources_improved(question, max_results=5)
        
        if not sources:
            await processing_msg.edit(content="‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y ngu·ªìn tin. ƒêang s·ª≠ d·ª•ng ki·∫øn th·ª©c t·ªïng qu√°t...")
        
        # üìÑ Step 2: Extract full content 
        await processing_msg.edit(content="üìÑ ƒêang ph√¢n t√≠ch n·ªôi dung t·ª´ c√°c ngu·ªìn tin...")
        full_context = await get_full_content_from_sources_improved(sources)
        
        # ü§ñ Step 3: AI Analysis v·ªõi Multi-Engine Fallback
        await processing_msg.edit(content="ü§ñ Multi-AI Engine ƒëang ph√¢n t√≠ch v√† t·∫°o c√¢u tr·∫£ l·ªùi...")
        
        # Detect if question requires specific financial data
        requires_specific_data = any(keyword in question.lower() for keyword in 
                                   ['gi√°', 'bao nhi√™u', 'tƒÉng gi·∫£m', 'thay ƒë·ªïi', 'hi·ªán t·∫°i', 'h√¥m nay'])
        
        ai_response, used_engine = await ai_manager.call_ai_with_fallback(
            prompt=question,
            context=full_context,
            require_specific_data=requires_specific_data
        )
        
        # X√≥a th√¥ng b√°o processing
        await processing_msg.delete()
        
        # üìä Create beautiful embed response
        embed = discord.Embed(
            title=f"ü§ñ AI Tr·∫£ l·ªùi: {question.title()[:100]}...",
            description=ai_response,
            color=0x9932cc,
            timestamp=ctx.message.created_at
        )
        
        # Add AI engine info
        engine_emoji = {
            'gemini': 'üíé',
            'deepseek': 'üí∞', 
            'claude': 'üß†',
            'groq': '‚ö°'
        }
        
        embed.add_field(
            name="ü§ñ AI Engine s·ª≠ d·ª•ng",
            value=f"{engine_emoji.get(used_engine, 'ü§ñ')} {used_engine.upper()}",
            inline=True
        )
        
        if sources:
            embed.add_field(
                name="üìä S·ªë ngu·ªìn tin",
                value=f"üì∞ {len(sources)} ngu·ªìn ƒë√°ng tin c·∫≠y",
                inline=True
            )
        
        # Add source references
        if sources:
            sources_text = ""
            for i, source in enumerate(sources[:3], 1):
                sources_text += f"{i}. **{source['source_name']}**: [{source['title'][:50]}...]({source['link']})\n"
            
            embed.add_field(
                name="üì∞ Ngu·ªìn tin tham kh·∫£o",
                value=sources_text,
                inline=False
            )
        
        # Footer
        embed.set_footer(
            text=f"üöÄ Multi-AI Engine ‚Ä¢ D·ªØ li·ªáu th·ªùi gian th·ª±c ‚Ä¢ !menu ƒë·ªÉ xem th√™m l·ªánh",
            icon_url=ctx.bot.user.avatar.url if ctx.bot.user.avatar else None
        )
        
        await ctx.send(embed=embed)
        
        # Log cho debug
        print(f"‚úÖ Question answered: '{question}' using {used_engine}")
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {str(e)}")
        print(f"‚ùå Error in !hoi command: {e}")

# üìä Updated Menu Command
@bot.command(name='menu')
async def help_command_improved(ctx):
    """Menu v·ªõi Multi-AI Engine info"""
    
    embed = discord.Embed(
        title="ü§ñüöÄ Menu News Bot - Multi-AI Engine",
        description="Bot tin t·ª©c kinh t·∫ø v·ªõi AI th√¥ng minh ƒëa engine",
        color=0xff9900
    )
    
    # AI Engine status
    ai_status = ""
    if ai_manager.primary_ai:
        engine_name = ai_manager.primary_ai.value.upper()
        ai_status += f"üöÄ **Primary**: {engine_name} ‚úÖ\n"
        
        for fallback in ai_manager.fallback_ais:
            ai_status += f"üõ°Ô∏è **Fallback**: {fallback.value.upper()} ‚úÖ\n"
    else:
        ai_status = "‚ùå Ch∆∞a c·∫•u h√¨nh AI engines"
    
    embed.add_field(
        name="ü§ñ AI Engines ho·∫°t ƒë·ªông",
        value=ai_status,
        inline=False
    )
    
    embed.add_field(
        name="üì∞ L·ªánh tin t·ª©c",
        value="""
**!all [trang]** - Tin t·ª´ t·∫•t c·∫£ ngu·ªìn (12 tin/trang)
**!in [trang]** - Tin trong n∆∞·ªõc (12 tin/trang)  
**!out [trang]** - Tin qu·ªëc t·∫ø (12 tin/trang)
**!chitiet [s·ªë]** - Xem n·ªôi dung chi ti·∫øt + üåê T·ª± ƒë·ªông d·ªãch
        """,
        inline=True
    )
    
    embed.add_field(
        name="ü§ñ L·ªánh AI th√¥ng minh",
        value="""
**!hoi [c√¢u h·ªèi]** - AI tr·∫£ l·ªùi v·ªõi Multi-Engine
*V√≠ d·ª•: !hoi gi√° v√†ng h√¥m nay nh∆∞ th·∫ø n√†o*
        """,
        inline=True
    )
    
    embed.add_field(
        name="üáªüá≥ Ngu·ªìn trong n∆∞·ªõc (13 ngu·ªìn)",
        value="CafeF (5 chuy√™n m·ª•c), CafeBiz, B√°o ƒê·∫ßu t∆∞, VnEconomy (2), VnExpress (2), Thanh Ni√™n (2), Nh√¢n D√¢n",
        inline=True
    )
    
    embed.add_field(
        name="üåç Ngu·ªìn qu·ªëc t·∫ø (8 ngu·ªìn)",
        value="Yahoo Finance, Reuters, Bloomberg, MarketWatch, Forbes, Financial Times, Business Insider, The Economist",
        inline=True
    )
    
    embed.add_field(
        name="üéØ T√≠nh nƒÉng m·ªõi",
        value="""
‚úÖ **Multi-AI Engine** - T·ª± ƒë·ªông fallback khi AI fail
‚úÖ **Generic Search** - Kh√¥ng c·∫ßn config t·ª´ng keyword  
‚úÖ **Real-time Data** - D·ªØ li·ªáu c·∫≠p nh·∫≠t li√™n t·ª•c
‚úÖ **Response Validation** - ƒê·∫£m b·∫£o ch·∫•t l∆∞·ª£ng
‚úÖ **Full Content Extract** - Ph√¢n t√≠ch s√¢u
‚úÖ **Auto Translation** - T·ª± ƒë·ªông d·ªãch tin n∆∞·ªõc ngo√†i
        """,
        inline=False
    )
    
    embed.add_field(
        name="üí° V√≠ d·ª• s·ª≠ d·ª•ng AI",
        value="""
`!hoi gi√° v√†ng h√¥m nay nh∆∞ th·∫ø n√†o` - H·ªèi v·ªÅ gi√° v√†ng hi·ªán t·∫°i
`!hoi t·∫°i sao t·ª∑ gi√° USD tƒÉng` - Ph√¢n t√≠ch t·ª∑ gi√°
`!hoi gi√° nh√† ƒë·∫•t TPHCM c√≥ ƒë·∫Øt kh√¥ng` - H·ªèi v·ªÅ b·∫•t ƒë·ªông s·∫£n
`!hoi ch·ª©ng kho√°n VN-Index h√¥m nay` - Th√¥ng tin ch·ª©ng kho√°n
        """,
        inline=False
    )
    
    if not ai_manager.primary_ai:
        embed.add_field(
            name="‚öôÔ∏è C·∫•u h√¨nh AI (ƒë·ªÉ b·∫≠t th√™m t√≠nh nƒÉng)",
            value="""
Bot ƒë√£ ho·∫°t ƒë·ªông ƒë·∫ßy ƒë·ªß ·ªü ch·∫ø ƒë·ªô c∆° b·∫£n.
ƒê·ªÉ k√≠ch ho·∫°t AI features, th√™m v√†o Environment Variables:
‚Ä¢ **GEMINI_API_KEY** - Mi·ªÖn ph√≠ t·∫°i aistudio.google.com (KHUY·∫æN NGH·ªä)
‚Ä¢ **DEEPSEEK_API_KEY** - Si√™u r·∫ª t·∫°i platform.deepseek.com
‚Ä¢ **ANTHROPIC_API_KEY** - Claude t·∫°i console.anthropic.com  
‚Ä¢ **GROQ_API_KEY** - Nhanh nh·∫•t t·∫°i console.groq.com
            """,
            inline=False
        )
    
    embed.set_footer(text="üöÄ Multi-AI Engine ‚Ä¢ Generic Search ‚Ä¢ Real-time Analysis ‚Ä¢ Auto Translation")
    await ctx.send(embed=embed)

# Main execution
if __name__ == "__main__":
    try:
        keep_alive()  # B·∫≠t web server ƒë·ªÉ keep alive
        print("üöÄ Starting Multi-AI Discord News Bot...")
        print("üîë ƒêang ki·ªÉm tra token t·ª´ Environment Variables...")
        
        if TOKEN:
            print("‚úÖ Token ƒë√£ ƒë∆∞·ª£c t·∫£i t·ª´ Environment Variables")
        
        total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
        print(f"üìä ƒê√£ load {total_sources} ngu·ªìn RSS ƒê√É KI·ªÇM TRA")
        print(f"üáªüá≥ Trong n∆∞·ªõc: {len(RSS_FEEDS['domestic'])} ngu·ªìn")
        print(f"üåç Qu·ªëc t·∫ø: {len(RSS_FEEDS['international'])} ngu·ªìn")
        print("üéØ Lƒ©nh v·ª±c: Kinh t·∫ø, Ch·ª©ng kho√°n, Vƒ© m√¥, B·∫•t ƒë·ªông s·∫£n")
        print("üï∞Ô∏è M√∫i gi·ªù: ƒê√£ s·ª≠a l·ªói - Hi·ªÉn th·ªã ch√≠nh x√°c gi·ªù Vi·ªát Nam")
        
        print("‚úÖ Bot ready with Multi-AI Engine support!")
        bot.run(TOKEN)
        
    except Exception as e:
        print(f"‚ùå Bot startup error: {e}")
        input("Press Enter to exit...")
