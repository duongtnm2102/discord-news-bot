import discord
from discord.ext import commands
import feedparser
import requests
import asyncio
import os
import re
from datetime import datetime
import time
from urllib.parse import urljoin
import html
import chardet

# C·∫•u h√¨nh bot
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix='!', intents=intents)

# üîí B·∫¢O M·∫¨T: L·∫•y token t·ª´ environment variable
# KH√îNG BAO GI·ªú hardcode token trong code n·ªØa!
TOKEN = os.getenv('DISCORD_TOKEN')

if not TOKEN:
    print("‚ùå C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y DISCORD_TOKEN trong environment variables!")
    print("üîß Vui l√≤ng th√™m DISCORD_TOKEN v√†o Render Environment Variables")
    exit(1)

# L∆∞u tr·ªØ tin t·ª©c theo t·ª´ng user ƒë·ªÉ xem chi ti·∫øt
user_news_cache = {}

# RSS feeds ƒë√£ ƒë∆∞·ª£c ki·ªÉm tra v√† x√°c nh·∫≠n ho·∫°t ƒë·ªông
RSS_FEEDS = {
    # === KINH T·∫æ TRONG N∆Ø·ªöC - ƒê√É KI·ªÇM TRA ===
    'domestic': {
        # CafeF - RSS ch√≠nh ho·∫°t ƒë·ªông t·ªët
        'cafef_main': 'https://cafef.vn/index.rss',
        'cafef_chungkhoan': 'https://cafef.vn/thi-truong-chung-khoan.rss',
        'cafef_batdongsan': 'https://cafef.vn/bat-dong-san.rss',
        'cafef_taichinh': 'https://cafef.vn/tai-chinh-ngan-hang.rss',
        'cafef_vimo': 'https://cafef.vn/vi-mo-dau-tu.rss',
        
        # CafeBiz - RSS t·ªïng h·ª£p
        'cafebiz_main': 'https://cafebiz.vn/index.rss',
        
        # B√°o ƒê·∫ßu t∆∞ - RSS ho·∫°t ƒë·ªông
        'baodautu_main': 'https://baodautu.vn/rss.xml',
        
        # VnEconomy - RSS tin t·ª©c ch√≠nh
        'vneconomy_main': 'https://vneconomy.vn/rss/home.rss',
        'vneconomy_chungkhoan': 'https://vneconomy.vn/rss/chung-khoan.rss',
        
        # VnExpress Kinh doanh 
        'vnexpress_kinhdoanh': 'https://vnexpress.net/rss/kinh-doanh.rss',
        'vnexpress_chungkhoan': 'https://vnexpress.net/rss/kinh-doanh/chung-khoan.rss',
        
        # Thanh Ni√™n - RSS kinh t·∫ø
        'thanhnien_kinhtevimo': 'https://thanhnien.vn/rss/kinh-te/vi-mo.rss',
        'thanhnien_chungkhoan': 'https://thanhnien.vn/rss/kinh-te/chung-khoan.rss',
        
        # Nh√¢n D√¢n - RSS t√†i ch√≠nh ch·ª©ng kho√°n
        'nhandanonline_tc': 'https://nhandan.vn/rss/tai-chinh-chung-khoan.rss'
    },
    
    # === KINH T·∫æ QU·ªêC T·∫æ ===
    'international': {
        'yahoo_finance': 'https://feeds.finance.yahoo.com/rss/2.0/headline',
        'reuters_business': 'https://feeds.reuters.com/reuters/businessNews',
        'bloomberg_markets': 'https://feeds.bloomberg.com/markets/news.rss',
        'marketwatch_latest': 'https://feeds.marketwatch.com/marketwatch/realtimeheadlines/',
        'forbes_money': 'https://www.forbes.com/money/feed/',
        'financial_times': 'https://www.ft.com/rss/home',
        'business_insider': 'https://feeds.businessinsider.com/custom/all',
        'the_economist': 'https://www.economist.com/rss'
    }
}

@bot.event
async def on_ready():
    print(f'‚úÖ {bot.user} ƒë√£ online!')
    print(f'üìä K·∫øt n·ªëi v·ªõi {len(bot.guilds)} server(s)')
    total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
    print(f'üì∞ S·∫µn s√†ng cung c·∫•p tin t·ª´ {total_sources} ngu·ªìn ƒê√É KI·ªÇM TRA')
    print(f'üáªüá≥ Trong n∆∞·ªõc: {len(RSS_FEEDS["domestic"])} ngu·ªìn')
    print(f'üåç Qu·ªëc t·∫ø: {len(RSS_FEEDS["international"])} ngu·ªìn')
    print('üéØ Lƒ©nh v·ª±c: Kinh t·∫ø, Ch·ª©ng kho√°n, Vƒ© m√¥, B·∫•t ƒë·ªông s·∫£n')
    print('üéØ G√µ !menu ƒë·ªÉ xem h∆∞·ªõng d·∫´n')
    
    # Set bot status
    await bot.change_presence(
        activity=discord.Activity(
            type=discord.ActivityType.watching, 
            name="tin kinh t·∫ø b·∫£o m·∫≠t | !menu"
        )
    )

async def fetch_full_content(url):
    """L·∫•y n·ªôi dung ƒë·∫ßy ƒë·ªß t·ª´ URL b√†i vi·∫øt v·ªõi x·ª≠ l√Ω encoding t·ªët h∆°n"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # L·∫•y n·ªôi dung v·ªõi timeout ng·∫Øn h∆°n
        response = requests.get(url, headers=headers, timeout=8, stream=True)
        response.raise_for_status()
        
        # X·ª≠ l√Ω encoding
        raw_content = response.content
        
        # T·ª± ƒë·ªông detect encoding
        detected = chardet.detect(raw_content)
        encoding = detected['encoding'] or 'utf-8'
        
        try:
            content = raw_content.decode(encoding)
        except:
            # Fallback encoding
            content = raw_content.decode('utf-8', errors='ignore')
        
        # Lo·∫°i b·ªè HTML tags m·ªôt c√°ch th√¥ng minh h∆°n
        # T√¨m n·ªôi dung ch√≠nh
        content_patterns = [
            r'<article[^>]*>(.*?)</article>',
            r'<div[^>]*class="[^"]*content[^"]*"[^>]*>(.*?)</div>',
            r'<div[^>]*class="[^"]*article[^"]*"[^>]*>(.*?)</div>',
            r'<div[^>]*id="[^"]*content[^"]*"[^>]*>(.*?)</div>',
            r'<main[^>]*>(.*?)</main>',
            r'<section[^>]*class="[^"]*content[^"]*"[^>]*>(.*?)</section>'
        ]
        
        main_content = ""
        for pattern in content_patterns:
            matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)
            if matches:
                main_content = matches[0]
                break
        
        # N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c content ch√≠nh, l·∫•y to√†n b·ªô body
        if not main_content:
            body_match = re.search(r'<body[^>]*>(.*?)</body>', content, re.DOTALL | re.IGNORECASE)
            if body_match:
                main_content = body_match.group(1)
            else:
                main_content = content
        
        # Lo·∫°i b·ªè scripts, styles, v√† c√°c th·∫ª kh√¥ng c·∫ßn thi·∫øt
        main_content = re.sub(r'<script[^>]*>.*?</script>', '', main_content, flags=re.DOTALL | re.IGNORECASE)
        main_content = re.sub(r'<style[^>]*>.*?</style>', '', main_content, flags=re.DOTALL | re.IGNORECASE)
        main_content = re.sub(r'<nav[^>]*>.*?</nav>', '', main_content, flags=re.DOTALL | re.IGNORECASE)
        main_content = re.sub(r'<header[^>]*>.*?</header>', '', main_content, flags=re.DOTALL | re.IGNORECASE)
        main_content = re.sub(r'<footer[^>]*>.*?</footer>', '', main_content, flags=re.DOTALL | re.IGNORECASE)
        main_content = re.sub(r'<aside[^>]*>.*?</aside>', '', main_content, flags=re.DOTALL | re.IGNORECASE)
        
        # Lo·∫°i b·ªè HTML tags
        clean_content = re.sub(r'<[^>]+>', ' ', main_content)
        
        # Decode HTML entities
        clean_content = html.unescape(clean_content)
        
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a v√† normalize
        clean_content = re.sub(r'\s+', ' ', clean_content).strip()
        
        # L·∫•y ph·∫ßn ƒë·∫ßu c√≥ √Ω nghƒ©a
        sentences = clean_content.split('. ')
        meaningful_content = []
        
        for sentence in sentences[:10]:  # L·∫•y 10 c√¢u ƒë·∫ßu
            if len(sentence.strip()) > 20:  # Ch·ªâ l·∫•y c√¢u c√≥ ƒë·ªô d√†i h·ª£p l√Ω
                meaningful_content.append(sentence.strip())
                
        result = '. '.join(meaningful_content)
        
        # Gi·ªõi h·∫°n ƒë·ªô d√†i
        if len(result) > 1800:
            result = result[:1800] + "..."
            
        return result if result else "Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ b√†i vi·∫øt n√†y."
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói l·∫•y n·ªôi dung t·ª´ {url}: {e}")
        return f"Kh√¥ng th·ªÉ l·∫•y n·ªôi dung chi ti·∫øt. L·ªói: {str(e)}"

async def collect_news_from_sources(sources_dict, limit_per_source=8):
    """Thu th·∫≠p tin t·ª©c t·ª´ nhi·ªÅu ngu·ªìn v√† s·∫Øp x·∫øp theo th·ªùi gian"""
    all_news = []
    
    for source_name, rss_url in sources_dict.items():
        try:
            print(f"üîÑ ƒêang l·∫•y tin t·ª´ {source_name}...")
            
            # Headers c·∫£i thi·ªán
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/rss+xml, application/xml, text/xml',
                'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8'
            }
            
            # Parse RSS feed v·ªõi x·ª≠ l√Ω l·ªói t·ªët h∆°n
            try:
                response = requests.get(rss_url, headers=headers, timeout=10)
                response.raise_for_status()
                
                # Parse v·ªõi feedparser
                feed = feedparser.parse(response.content)
                
            except Exception as req_error:
                print(f"‚ö†Ô∏è L·ªói request t·ª´ {source_name}: {req_error}")
                # Th·ª≠ parse tr·ª±c ti·∫øp v·ªõi feedparser
                feed = feedparser.parse(rss_url)
            
            # Ki·ªÉm tra xem feed c√≥ h·ª£p l·ªá kh√¥ng
            if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                print(f"‚ö†Ô∏è Kh√¥ng c√≥ tin t·ª´ {source_name} - RSS c√≥ th·ªÉ kh√¥ng ho·∫°t ƒë·ªông")
                continue
                
            entries_processed = 0
            for entry in feed.entries[:limit_per_source]:
                try:
                    # L·∫•y th·ªùi gian published
                    published_time = datetime.now()
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        try:
                            published_time = datetime.fromtimestamp(time.mktime(entry.published_parsed))
                        except:
                            pass
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        try:
                            published_time = datetime.fromtimestamp(time.mktime(entry.updated_parsed))
                        except:
                            pass
                    
                    # L·∫•y m√¥ t·∫£/t√≥m t·∫Øt n·∫øu c√≥
                    description = ""
                    if hasattr(entry, 'summary'):
                        description = entry.summary[:500] + "..." if len(entry.summary) > 500 else entry.summary
                    elif hasattr(entry, 'description'):
                        description = entry.description[:500] + "..." if len(entry.description) > 500 else entry.description
                    
                    # Ki·ªÉm tra c√°c tr∆∞·ªùng b·∫Øt bu·ªôc
                    if not hasattr(entry, 'title') or not hasattr(entry, 'link'):
                        continue
                    
                    # Clean title
                    title = html.unescape(entry.title.strip())
                    
                    news_item = {
                        'title': title,
                        'link': entry.link,
                        'source': source_name,
                        'published': published_time,
                        'published_str': published_time.strftime("%H:%M %d/%m"),
                        'description': html.unescape(description) if description else ""
                    }
                    all_news.append(news_item)
                    entries_processed += 1
                    
                except Exception as entry_error:
                    print(f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω tin t·ª´ {source_name}: {entry_error}")
                    continue
                    
            print(f"‚úÖ L·∫•y ƒë∆∞·ª£c {entries_processed} tin t·ª´ {source_name}")
            
        except Exception as e:
            print(f"‚ùå L·ªói khi l·∫•y tin t·ª´ {source_name}: {e}")
            continue
    
    print(f"üìä T·ªïng c·ªông l·∫•y ƒë∆∞·ª£c {len(all_news)} tin t·ª´ t·∫•t c·∫£ ngu·ªìn")
    
    # Lo·∫°i b·ªè tin tr√πng l·∫∑p
    unique_news = remove_duplicate_news(all_news)
    print(f"üîÑ Sau khi lo·∫°i tr√πng c√≤n {len(unique_news)} tin")
    
    # S·∫Øp x·∫øp theo th·ªùi gian m·ªõi nh·∫•t
    unique_news.sort(key=lambda x: x['published'], reverse=True)
    return unique_news

def remove_duplicate_news(news_list):
    """Lo·∫°i b·ªè tin t·ª©c tr√πng l·∫∑p d·ª±a tr√™n ti√™u ƒë·ªÅ v√† link"""
    seen_links = set()
    seen_titles = set()
    unique_news = []
    
    for news in news_list:
        # Chu·∫©n h√≥a ti√™u ƒë·ªÅ ƒë·ªÉ so s√°nh
        normalized_title = normalize_title(news['title'])
        
        # Ki·ªÉm tra tr√πng l·∫∑p
        is_duplicate = False
        
        if news['link'] in seen_links:
            is_duplicate = True
        else:
            # Ki·ªÉm tra ti√™u ƒë·ªÅ t∆∞∆°ng t·ª±
            for existing_title in seen_titles:
                similarity = calculate_title_similarity(normalized_title, existing_title)
                if similarity > 0.75:  # 75% t∆∞∆°ng t·ª± th√¨ coi l√† tr√πng
                    is_duplicate = True
                    break
        
        if not is_duplicate:
            seen_links.add(news['link'])
            seen_titles.add(normalized_title)
            unique_news.append(news)
    
    return unique_news

def calculate_title_similarity(title1, title2):
    """T√≠nh ƒë·ªô t∆∞∆°ng t·ª± gi·ªØa 2 ti√™u ƒë·ªÅ"""
    words1 = set(title1.split())
    words2 = set(title2.split())
    
    if not words1 or not words2:
        return 0
    
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    return len(intersection) / len(union) if union else 0

def normalize_title(title):
    """Chu·∫©n h√≥a ti√™u ƒë·ªÅ ƒë·ªÉ so s√°nh tr√πng l·∫∑p"""
    import re
    # Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    title = title.lower()
    # Lo·∫°i b·ªè d·∫•u c√¢u v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
    title = re.sub(r'[^\w\s]', '', title)
    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    title = ' '.join(title.split())
    
    # Ch·ªâ l·∫•y 10 t·ª´ ƒë·∫ßu ƒë·ªÉ so s√°nh
    words = title.split()[:10]
    return ' '.join(words)

def save_user_news(user_id, news_list, command_type):
    """L∆∞u tin t·ª©c c·ªßa user ƒë·ªÉ s·ª≠ d·ª•ng cho l·ªánh !detail"""
    user_news_cache[user_id] = {
        'news': news_list,
        'command': command_type,
        'timestamp': datetime.now()
    }

@bot.command(name='all')
async def get_all_news(ctx, page=1):
    """L·∫•y tin t·ª©c t·ª´ t·∫•t c·∫£ ngu·ªìn (trong n∆∞·ªõc + qu·ªëc t·∫ø)"""
    try:
        page = max(1, int(page))
        
        # G·ª≠i th√¥ng b√°o ƒëang t·∫£i
        loading_msg = await ctx.send("‚è≥ ƒêang t·∫£i tin t·ª©c t·ª´ t·∫•t c·∫£ ngu·ªìn...")
        
        # Thu th·∫≠p tin t·ª´ c·∫£ hai ngu·ªìn
        domestic_news = await collect_news_from_sources(RSS_FEEDS['domestic'], 8)
        international_news = await collect_news_from_sources(RSS_FEEDS['international'], 6)
        
        # X√≥a th√¥ng b√°o loading
        await loading_msg.delete()
        
        # K·∫øt h·ª£p v√† s·∫Øp x·∫øp
        all_news = domestic_news + international_news
        all_news.sort(key=lambda x: x['published'], reverse=True)
        
        # Ph√¢n trang
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = all_news[start_index:end_index]
        
        if not page_news:
            total_pages = (len(all_news) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        # T·∫°o embed v·ªõi thi·∫øt k·∫ø t·ªët h∆°n
        embed = discord.Embed(
            title=f"üì∞ Tin t·ª©c kinh t·∫ø t·ªïng h·ª£p (Trang {page})",
            description=f"üîí Bot b·∫£o m·∫≠t ‚Ä¢ C·∫≠p nh·∫≠t t·ª´ {len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])} ngu·ªìn tin uy t√≠n",
            color=0x00ff88,
            timestamp=ctx.message.created_at
        )
        
        # Emoji map cho t·ª´ng ngu·ªìn
        emoji_map = {
            # Ngu·ªìn trong n∆∞·ªõc
            'cafef_main': '‚òï', 'cafef_chungkhoan': 'üìà', 'cafef_batdongsan': 'üè¢', 'cafef_taichinh': 'üí∞', 'cafef_vimo': 'üìä',
            'cafebiz_main': 'üíº', 'baodautu_main': 'üéØ', 'vneconomy_main': 'üì∞', 'vneconomy_chungkhoan': 'üìà',
            'vnexpress_kinhdoanh': '‚ö°', 'vnexpress_chungkhoan': 'üìà', 'thanhnien_kinhtevimo': 'üìä', 'thanhnien_chungkhoan': 'üìà',
            'nhandanonline_tc': 'üèõÔ∏è',
            # Ngu·ªìn qu·ªëc t·∫ø
            'yahoo_finance': 'üí∞', 'reuters_business': 'üåç', 'bloomberg_markets': 'üíπ', 'marketwatch_latest': 'üìà',
            'forbes_money': 'üíé', 'financial_times': 'üíº', 'business_insider': 'üì∞', 'the_economist': 'üéì'
        }
        
        # Th·ªëng k√™
        domestic_count = sum(1 for news in page_news if news['source'] in RSS_FEEDS['domestic'])
        international_count = len(page_news) - domestic_count
        
        embed.add_field(
            name="üìä Th·ªëng k√™ trang n√†y",
            value=f"üáªüá≥ Trong n∆∞·ªõc: {domestic_count} tin\nüåç Qu·ªëc t·∫ø: {international_count} tin\nüìä T·ªïng c√≥ s·∫µn: {len(all_news)} tin",
            inline=False
        )
        
        # Hi·ªÉn th·ªã tin t·ª©c
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'üì∞')
            title = news['title'][:70] + "..." if len(news['title']) > 70 else news['title']
            
            # T√™n ngu·ªìn hi·ªÉn th·ªã
            source_names = {
                'cafef_main': 'CafeF', 'cafef_chungkhoan': 'CafeF CK', 'cafef_batdongsan': 'CafeF BƒêS',
                'cafef_taichinh': 'CafeF TC', 'cafef_vimo': 'CafeF VM', 'cafebiz_main': 'CafeBiz',
                'baodautu_main': 'B√°o ƒê·∫ßu t∆∞', 'vneconomy_main': 'VnEconomy', 'vneconomy_chungkhoan': 'VnEconomy CK',
                'vnexpress_kinhdoanh': 'VnExpress KD', 'vnexpress_chungkhoan': 'VnExpress CK',
                'thanhnien_kinhtevimo': 'Thanh Ni√™n VM', 'thanhnien_chungkhoan': 'Thanh Ni√™n CK',
                'nhandanonline_tc': 'Nh√¢n D√¢n TC', 'yahoo_finance': 'Yahoo Finance', 'reuters_business': 'Reuters',
                'bloomberg_markets': 'Bloomberg', 'marketwatch_latest': 'MarketWatch', 'forbes_money': 'Forbes',
                'financial_times': 'Financial Times', 'business_insider': 'Business Insider', 'the_economist': 'The Economist'
            }
            source_display = source_names.get(news['source'], news['source'])
            
            embed.add_field(
                name=f"{i}. {emoji} {title}",
                value=f"üìÖ {news['published_str']} ‚Ä¢ üì∞ {source_display}\nüîó [ƒê·ªçc b√†i vi·∫øt]({news['link']})",
                inline=False
            )
        
        # L∆∞u tin t·ª©c
        save_user_news(ctx.author.id, page_news, f"all_page_{page}")
        
        # Footer
        total_pages = (len(all_news) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"üîí Bot b·∫£o m·∫≠t ‚Ä¢ Trang {page}/{total_pages} ‚Ä¢ !all {page+1} ti·∫øp ‚Ä¢ !chitiet [s·ªë] xem chi ti·∫øt")
        
        await ctx.send(embed=embed)
        
    except ValueError:
        await ctx.send("‚ùå S·ªë trang kh√¥ng h·ª£p l·ªá! S·ª≠ d·ª•ng: `!all [s·ªë]`")
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='in')
async def get_domestic_news(ctx, page=1):
    """L·∫•y tin t·ª©c t·ª´ c√°c ngu·ªìn trong n∆∞·ªõc"""
    try:
        page = max(1, int(page))
        
        loading_msg = await ctx.send("‚è≥ ƒêang t·∫£i tin t·ª©c trong n∆∞·ªõc...")
        
        news_list = await collect_news_from_sources(RSS_FEEDS['domestic'], 10)
        
        await loading_msg.delete()
        
        # Ph√¢n trang
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        embed = discord.Embed(
            title=f"üáªüá≥ Tin kinh t·∫ø trong n∆∞·ªõc (Trang {page})",
            description=f"üîí Bot b·∫£o m·∫≠t ‚Ä¢ T·ª´ {len(RSS_FEEDS['domestic'])} ngu·ªìn tin chuy√™n ng√†nh Vi·ªát Nam",
            color=0xff0000,
            timestamp=ctx.message.created_at
        )
        
        embed.add_field(
            name="üìä Th√¥ng tin",
            value=f"üì∞ T·ªïng tin c√≥ s·∫µn: {len(news_list)} tin\nüéØ Lƒ©nh v·ª±c: Kinh t·∫ø, Ch·ª©ng kho√°n, B·∫•t ƒë·ªông s·∫£n, Vƒ© m√¥",
            inline=False
        )
        
        # Hi·ªÉn th·ªã tin t·ª©c trong n∆∞·ªõc
        emoji_map = {
            'cafef_main': '‚òï', 'cafef_chungkhoan': 'üìà', 'cafef_batdongsan': 'üè¢', 'cafef_taichinh': 'üí∞', 'cafef_vimo': 'üìä',
            'cafebiz_main': 'üíº', 'baodautu_main': 'üéØ', 'vneconomy_main': 'üì∞', 'vneconomy_chungkhoan': 'üìà',
            'vnexpress_kinhdoanh': '‚ö°', 'vnexpress_chungkhoan': 'üìà', 'thanhnien_kinhtevimo': 'üìä', 'thanhnien_chungkhoan': 'üìà',
            'nhandanonline_tc': 'üèõÔ∏è'
        }
        
        source_names = {
            'cafef_main': 'CafeF', 'cafef_chungkhoan': 'CafeF CK', 'cafef_batdongsan': 'CafeF BƒêS',
            'cafef_taichinh': 'CafeF TC', 'cafef_vimo': 'CafeF VM', 'cafebiz_main': 'CafeBiz',
            'baodautu_main': 'B√°o ƒê·∫ßu t∆∞', 'vneconomy_main': 'VnEconomy', 'vneconomy_chungkhoan': 'VnEconomy CK',
            'vnexpress_kinhdoanh': 'VnExpress KD', 'vnexpress_chungkhoan': 'VnExpress CK',
            'thanhnien_kinhtevimo': 'Thanh Ni√™n VM', 'thanhnien_chungkhoan': 'Thanh Ni√™n CK',
            'nhandanonline_tc': 'Nh√¢n D√¢n TC'
        }
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'üì∞')
            title = news['title'][:70] + "..." if len(news['title']) > 70 else news['title']
            source_display = source_names.get(news['source'], news['source'])
            
            embed.add_field(
                name=f"{i}. {emoji} {title}",
                value=f"üìÖ {news['published_str']} ‚Ä¢ üì∞ {source_display}\nüîó [ƒê·ªçc b√†i vi·∫øt]({news['link']})",
                inline=False
            )
        
        save_user_news(ctx.author.id, page_news, f"in_page_{page}")
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"üîí Bot b·∫£o m·∫≠t ‚Ä¢ Trang {page}/{total_pages} ‚Ä¢ !in {page+1} ti·∫øp ‚Ä¢ !chitiet [s·ªë] xem chi ti·∫øt")
        
        await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='out')
async def get_international_news(ctx, page=1):
    """L·∫•y tin t·ª©c t·ª´ c√°c ngu·ªìn qu·ªëc t·∫ø"""
    try:
        page = max(1, int(page))
        
        loading_msg = await ctx.send("‚è≥ ƒêang t·∫£i tin t·ª©c qu·ªëc t·∫ø...")
        
        news_list = await collect_news_from_sources(RSS_FEEDS['international'], 8)
        
        await loading_msg.delete()
        
        # Ph√¢n trang
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = news_list[start_index:end_index]
        
        if not page_news:
            total_pages = (len(news_list) + items_per_page - 1) // items_per_page
            await ctx.send(f"‚ùå Kh√¥ng c√≥ tin t·ª©c ·ªü trang {page}! T·ªïng c·ªông c√≥ {total_pages} trang.")
            return
        
        embed = discord.Embed(
            title=f"üåç Tin kinh t·∫ø qu·ªëc t·∫ø (Trang {page})",
            description=f"üîí Bot b·∫£o m·∫≠t ‚Ä¢ T·ª´ {len(RSS_FEEDS['international'])} ngu·ªìn tin h√†ng ƒë·∫ßu th·∫ø gi·ªõi",
            color=0x0066ff,
            timestamp=ctx.message.created_at
        )
        
        embed.add_field(
            name="üìä Th√¥ng tin",
            value=f"üì∞ T·ªïng tin c√≥ s·∫µn: {len(news_list)} tin",
            inline=False
        )
        
        emoji_map = {
            'yahoo_finance': 'üí∞', 'reuters_business': 'üåç', 'bloomberg_markets': 'üíπ', 'marketwatch_latest': 'üìà',
            'forbes_money': 'üíé', 'financial_times': 'üíº', 'business_insider': 'üì∞', 'the_economist': 'üéì'
        }
        
        source_names = {
            'yahoo_finance': 'Yahoo Finance', 'reuters_business': 'Reuters', 'bloomberg_markets': 'Bloomberg', 
            'marketwatch_latest': 'MarketWatch', 'forbes_money': 'Forbes', 'financial_times': 'Financial Times', 
            'business_insider': 'Business Insider', 'the_economist': 'The Economist'
        }
        
        for i, news in enumerate(page_news, 1):
            emoji = emoji_map.get(news['source'], 'üåç')
            title = news['title'][:70] + "..." if len(news['title']) > 70 else news['title']
            source_display = source_names.get(news['source'], news['source'])
            
            embed.add_field(
                name=f"{i}. {emoji} {title}",
                value=f"üìÖ {news['published_str']} ‚Ä¢ üì∞ {source_display}\nüîó [ƒê·ªçc b√†i vi·∫øt]({news['link']})",
                inline=False
            )
        
        save_user_news(ctx.author.id, page_news, f"out_page_{page}")
        
        total_pages = (len(news_list) + items_per_page - 1) // items_per_page
        embed.set_footer(text=f"üîí Bot b·∫£o m·∫≠t ‚Ä¢ Trang {page}/{total_pages} ‚Ä¢ !out {page+1} ti·∫øp ‚Ä¢ !chitiet [s·ªë] xem chi ti·∫øt")
        
        await ctx.send(embed=embed)
        
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

@bot.command(name='chitiet')
async def get_news_detail(ctx, news_number: int):
    """Xem chi ti·∫øt tin t·ª©c theo s·ªë th·ª© t·ª± - ƒê√É S·ª¨A L·ªñI ENCODING"""
    try:
        user_id = ctx.author.id
        
        if user_id not in user_news_cache:
            await ctx.send("‚ùå B·∫°n ch∆∞a xem tin t·ª©c n√†o! H√£y d√πng `!all`, `!in`, ho·∫∑c `!out` tr∆∞·ªõc.")
            return
        
        user_data = user_news_cache[user_id]
        news_list = user_data['news']
        
        if news_number < 1 or news_number > len(news_list):
            await ctx.send(f"‚ùå S·ªë kh√¥ng h·ª£p l·ªá! Ch·ªçn t·ª´ 1 ƒë·∫øn {len(news_list)}")
            return
        
        news = news_list[news_number - 1]
        
        # Th√¥ng b√°o ƒëang t·∫£i v·ªõi progress
        loading_msg = await ctx.send("‚è≥ ƒêang ph√¢n t√≠ch v√† tr√≠ch xu·∫•t n·ªôi dung b√†i vi·∫øt...")
        
        # L·∫•y n·ªôi dung v·ªõi function ƒë√£ c·∫£i ti·∫øn
        full_content = await fetch_full_content(news['link'])
        
        await loading_msg.delete()
        
        # T·∫°o embed ƒë·∫πp h∆°n
        embed = discord.Embed(
            title="üìñ Chi ti·∫øt b√†i vi·∫øt",
            color=0x9932cc,
            timestamp=ctx.message.created_at
        )
        
        # Emoji cho ngu·ªìn
        emoji_map = {
            'cafef_main': '‚òï', 'cafef_chungkhoan': 'üìà', 'cafef_batdongsan': 'üè¢', 'cafef_taichinh': 'üí∞', 'cafef_vimo': 'üìä',
            'cafebiz_main': 'üíº', 'baodautu_main': 'üéØ', 'vneconomy_main': 'üì∞', 'vneconomy_chungkhoan': 'üìà',
            'vnexpress_kinhdoanh': '‚ö°', 'vnexpress_chungkhoan': 'üìà', 'thanhnien_kinhtevimo': 'üìä', 'thanhnien_chungkhoan': 'üìà',
            'nhandanonline_tc': 'üèõÔ∏è', 'yahoo_finance': 'üí∞', 'reuters_business': 'üåç', 'bloomberg_markets': 'üíπ', 
            'marketwatch_latest': 'üìà', 'forbes_money': 'üíé', 'financial_times': 'üíº', 'business_insider': 'üì∞', 'the_economist': 'üéì'
        }
        
        source_names = {
            'cafef_main': 'CafeF', 'cafef_chungkhoan': 'CafeF Ch·ª©ng kho√°n', 'cafef_batdongsan': 'CafeF B·∫•t ƒë·ªông s·∫£n',
            'cafef_taichinh': 'CafeF T√†i ch√≠nh', 'cafef_vimo': 'CafeF Vƒ© m√¥', 'cafebiz_main': 'CafeBiz',
            'baodautu_main': 'B√°o ƒê·∫ßu t∆∞', 'vneconomy_main': 'VnEconomy', 'vneconomy_chungkhoan': 'VnEconomy Ch·ª©ng kho√°n',
            'vnexpress_kinhdoanh': 'VnExpress Kinh doanh', 'vnexpress_chungkhoan': 'VnExpress Ch·ª©ng kho√°n',
            'thanhnien_kinhtevimo': 'Thanh Ni√™n Vƒ© m√¥', 'thanhnien_chungkhoan': 'Thanh Ni√™n Ch·ª©ng kho√°n',
            'nhandanonline_tc': 'Nh√¢n D√¢n T√†i ch√≠nh', 'yahoo_finance': 'Yahoo Finance', 'reuters_business': 'Reuters Business',
            'bloomberg_markets': 'Bloomberg Markets', 'marketwatch_latest': 'MarketWatch', 'forbes_money': 'Forbes Money',
            'financial_times': 'Financial Times', 'business_insider': 'Business Insider', 'the_economist': 'The Economist'
        }
        
        emoji = emoji_map.get(news['source'], 'üì∞')
        source_display = source_names.get(news['source'], news['source'])
        
        embed.add_field(
            name=f"{emoji} Ti√™u ƒë·ªÅ",
            value=news['title'],
            inline=False
        )
        
        embed.add_field(
            name="üìÖ Th·ªùi gian",
            value=news['published_str'],
            inline=True
        )
        
        embed.add_field(
            name="üì∞ Ngu·ªìn",
            value=source_display,
            inline=True
        )
        
        # Hi·ªÉn th·ªã n·ªôi dung ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        if len(full_content) > 1000:
            # Chia n·ªôi dung th√†nh 2 ph·∫ßn
            embed.add_field(
                name="üìÑ N·ªôi dung chi ti·∫øt (Ph·∫ßn 1)",
                value=full_content[:1000] + "...",
                inline=False
            )
            
            await ctx.send(embed=embed)
            
            # T·∫°o embed th·ª© 2
            embed2 = discord.Embed(
                title=f"üìñ Chi ti·∫øt b√†i vi·∫øt (ti·∫øp theo)",
                color=0x9932cc
            )
            
            embed2.add_field(
                name="üìÑ N·ªôi dung chi ti·∫øt (Ph·∫ßn 2)",
                value=full_content[1000:2000],
                inline=False
            )
            
            embed2.add_field(
                name="üîó ƒê·ªçc b√†i vi·∫øt ƒë·∫ßy ƒë·ªß",
                value=f"[Nh·∫•n ƒë·ªÉ ƒë·ªçc to√†n b·ªô b√†i vi·∫øt]({news['link']})",
                inline=False
            )
            
            embed2.set_footer(text=f"üîí Bot b·∫£o m·∫≠t ‚Ä¢ T·ª´ l·ªánh: {user_data['command']} ‚Ä¢ Tin s·ªë {news_number}")
            
            await ctx.send(embed=embed2)
            return
        else:
            embed.add_field(
                name="üìÑ N·ªôi dung chi ti·∫øt",
                value=full_content,
                inline=False
            )
        
        embed.add_field(
            name="üîó ƒê·ªçc b√†i vi·∫øt ƒë·∫ßy ƒë·ªß",
            value=f"[Nh·∫•n ƒë·ªÉ ƒë·ªçc to√†n b·ªô b√†i vi·∫øt]({news['link']})",
            inline=False
        )
        
        embed.set_footer(text=f"üîí Bot b·∫£o m·∫≠t ‚Ä¢ T·ª´ l·ªánh: {user_data['command']} ‚Ä¢ Tin s·ªë {news_number} ‚Ä¢ !menu ƒë·ªÉ xem th√™m l·ªánh")
        
        await ctx.send(embed=embed)
        
    except ValueError:
        await ctx.send("‚ùå Vui l√≤ng nh·∫≠p s·ªë! V√≠ d·ª•: `!chitiet 5`")
    except Exception as e:
        await ctx.send(f"‚ùå L·ªói: {str(e)}")

# Alias cho l·ªánh chitiet
@bot.command(name='cuthe')
async def get_news_detail_alias(ctx, news_number: int):
    """Alias cho l·ªánh !chitiet"""
    await get_news_detail(ctx, news_number)

@bot.command(name='menu')
async def help_command(ctx):
    """Hi·ªÉn th·ªã menu l·ªánh - ƒê√É C·∫¨P NH·∫¨T B·∫¢O M·∫¨T"""
    embed = discord.Embed(
        title="ü§ñüîí Menu News Bot - B·∫£o m·∫≠t & ·ªîn ƒë·ªãnh",
        description="Bot tin t·ª©c kinh t·∫ø ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u v√† b·∫£o m·∫≠t token",
        color=0xff9900
    )
    
    embed.add_field(
        name="üì∞ L·ªánh ch√≠nh",
        value="""
**!all [trang]** - Tin t·ª´ t·∫•t c·∫£ ngu·ªìn (12 tin/trang)
**!in [trang]** - Tin trong n∆∞·ªõc (12 tin/trang)  
**!out [trang]** - Tin qu·ªëc t·∫ø (12 tin/trang)
**!chitiet [s·ªë]** - Xem n·ªôi dung chi ti·∫øt
        """,
        inline=False
    )
    
    embed.add_field(
        name="üáªüá≥ Ngu·ªìn trong n∆∞·ªõc (9 ngu·ªìn)",
        value="CafeF, CafeBiz, B√°o ƒê·∫ßu t∆∞, VnEconomy, VnExpress KD, Thanh Ni√™n, Nh√¢n D√¢n",
        inline=True
    )
    
    embed.add_field(
        name="üåç Ngu·ªìn qu·ªëc t·∫ø (8 ngu·ªìn)",
        value="Yahoo Finance, Reuters, Bloomberg, MarketWatch, Forbes, Financial Times, Business Insider, The Economist",
        inline=True
    )
    
    embed.add_field(
        name="üîí B·∫£o m·∫≠t m·ªõi",
        value="""
‚úÖ **Token ƒë∆∞·ª£c b·∫£o v·ªá** - S·ª≠ d·ª•ng Environment Variables
‚úÖ **Kh√¥ng hardcode** - Token kh√¥ng c√≤n trong source code
‚úÖ **Deploy an to√†n** - Kh√¥ng b·ªã Discord reset token
‚úÖ **Monitoring t√≠ch h·ª£p** - Ph√°t hi·ªán l·ªói nhanh ch√≥ng
        """,
        inline=False
    )
    
    embed.add_field(
        name="üìã H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng",
        value="""
1Ô∏è‚É£ G√µ **!all** ƒë·ªÉ xem tin m·ªõi nh·∫•t
2Ô∏è‚É£ Ch·ªçn s·ªë tin mu·ªën ƒë·ªçc chi ti·∫øt (1-12)
3Ô∏è‚É£ G√µ **!chitiet [s·ªë]** ƒë·ªÉ xem n·ªôi dung ƒë·∫ßy ƒë·ªß
4Ô∏è‚É£ D√πng **!all 2**, **!all 3** ƒë·ªÉ xem trang ti·∫øp theo
        """,
        inline=False
    )
    
    embed.set_footer(text="üîí Bot ƒë√£ ƒë∆∞·ª£c b·∫£o m·∫≠t ‚Ä¢ Token an to√†n ‚Ä¢ RSS feeds ·ªïn ƒë·ªãnh")
    await ctx.send(embed=embed)

# Ch·∫°y bot v·ªõi error handling t·ªët h∆°n
if __name__ == "__main__":
    try:
        print("üöÄ ƒêang kh·ªüi ƒë·ªông News Bot b·∫£o m·∫≠t...")
        print("üîë ƒêang ki·ªÉm tra token t·ª´ Environment Variables...")
        
        if TOKEN:
            print("‚úÖ Token ƒë√£ ƒë∆∞·ª£c t·∫£i t·ª´ Environment Variables")
        
        total_sources = len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international'])
        print(f"üìä ƒê√£ load {total_sources} ngu·ªìn RSS ƒê√É KI·ªÇM TRA")
        print(f"üáªüá≥ Trong n∆∞·ªõc: {len(RSS_FEEDS['domestic'])} ngu·ªìn")
        print(f"üåç Qu·ªëc t·∫ø: {len(RSS_FEEDS['international'])} ngu·ªìn")
        print("üéØ Lƒ©nh v·ª±c: Kinh t·∫ø, Ch·ª©ng kho√°n, Vƒ© m√¥, B·∫•t ƒë·ªông s·∫£n")
        print("üîí Bot ƒë√£ ƒë∆∞·ª£c b·∫£o m·∫≠t token")
        print("‚úÖ Bot s·∫µn s√†ng nh·∫≠n l·ªánh!")
        
        bot.run(TOKEN)
        
    except discord.LoginFailure:
        print("‚ùå L·ªói ƒëƒÉng nh·∫≠p Discord!")
        print("üîß Token c√≥ th·ªÉ kh√¥ng h·ª£p l·ªá ho·∫∑c ƒë√£ b·ªã reset")
        print("üîß Ki·ªÉm tra DISCORD_TOKEN trong Environment Variables")
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ch·∫°y bot: {e}")
        print("üîß Ki·ªÉm tra k·∫øt n·ªëi internet v√† Environment Variables")
        
    input("Nh·∫•n Enter ƒë·ªÉ tho√°t...")
